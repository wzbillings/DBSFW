[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Doing Bayesian Statistics the Fun Way ‚Äì Solutions",
    "section": "",
    "text": "I‚Äôve been working through this book, and I decided to type my solutions so it will not be a problem when I inevitably lose the papers.\n\n\n\n\n\n\n\n\n\n\nChapter 8: Priors, posteriors, and likelihoods of Bayes‚Äô Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 7: Bayes‚Äôs Theorem with LEGO\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 6: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 5: the Beta Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 4\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 3\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 2\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/cp1/index.html",
    "href": "posts/cp1/index.html",
    "title": "Chapter 1",
    "section": "",
    "text": "‚ÄúBayesian Thinking and Everyday Reasoning.‚Äù"
  },
  {
    "objectID": "posts/cp1/index.html#q1",
    "href": "posts/cp1/index.html#q1",
    "title": "Chapter 1",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nRewrite the following statements as equations using the mathematical notation you learned in the chapter:\n\nThe probability of rain is low\nThe probabiliy of rain given that it is cloudy is high\nThe probability of you having an umbrella given it is raining is much greater than the probability of you having an umbrella in general.\n\n\n\n\nNothing too out of the ordinary here!\n\n\\(\\displaystyle P\\left( \\text{rain} \\right) = \\text{low}.\\)\n\\(P \\left( \\text{rain} \\mid \\text{cloudy} \\right) = \\text{high}.\\)\n\\(P \\left( \\text{umbrella} \\mid \\text{raining} \\right) \\gg P \\left(\\text{umbrella}\\right).\\)"
  },
  {
    "objectID": "posts/cp1/index.html#q2",
    "href": "posts/cp1/index.html#q2",
    "title": "Chapter 1",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nOrganize the data you observe in the following scenario into a mathematical notation, using the techniques we‚Äôve covered in this chapter. Then come up with a hypothesis to explain this data:\n\nYou come home from work and notice that your front door is open and the side window is broken. As you walk inside, you immediately notice that your laptop is missing.\n\n\n\n\nLet \\(D\\) represent the data we‚Äôve observed. In order words, \\[D = \\{\\text{door open, window broken, laptop missing}\\}.\\]\nOur hypothesis might be \\[H_1: \\text{My house was robbed and they took my laptop!}\\]\nSo we might conjecture that \\[P(D \\mid H_1) = \\mathrm{high}.\\]"
  },
  {
    "objectID": "posts/cp1/index.html#q3",
    "href": "posts/cp1/index.html#q3",
    "title": "Chapter 1",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nThe following scenario adds data to the previous one. Demonstrate how this new information changes your beliefs and come up with a second hypothesis to explain the data, using the notation you‚Äôve learned in this chapter.\n\nA neighborhood child runs up to you and apologizes profusely for accidentally throwing a rock through your window. They claim that they saw the laptop and didn‚Äôt want it stolen so they opened the front door to grab it, and your laptop is safe at their house.\n\n\n\n\nNow we‚Äôll let \\(D_U\\) represent our updated data (augmented with the new information that we learned from this child). Then, \\[D_U = \\{D, \\text{ child's explanation}\\}\\]\nand our new hypothesis could be \\[H_2: \\text{the child has my laptop.}\\]\nIf the child appears to be telling the truth, we could then conjecture that \\[P(D_U \\mid H_2) \\gg P(D_U \\mid H_1),\\] although we shouldn‚Äôt rule out the possibility that the child is being coerced or is a part of the robbery."
  },
  {
    "objectID": "posts/cp2/index.html",
    "href": "posts/cp2/index.html",
    "title": "Chapter 2",
    "section": "",
    "text": "What is the probability of rolling two six-sided dice and getting a value greater than 7?\n\n\n\nIn general, I think it‚Äôs worth solving this problem via simulation, because solving via enumeration scales much worse. (We could also solve this problem analytically, but that scales even worse than the enumerative solution.) So first I‚Äôll show how to solve via simulation.\n\n# The number of simulations we will do. As this number gets larger, our\n# simulated probability will approach the theoretical probability.\nN_rolls <- 10000\n\n# Simulate the two die rolls. You could do this with one call of sample but\n# for expository purposes this is easier.\ndie_1 <- sample(1:6, N_rolls, replace = TRUE)\ndie_2 <- sample(1:6, N_rolls, replace = TRUE)\n\n# Calculate the proportion of sums that are greater than 7\nsum <- die_1 + die_2\nmean(sum > 7)\n\n[1] 0.4238\n\n\nWe can also use R to solve this problem by enumeration.\n\npossible_rolls <- expand.grid(die_1 = 1:6, die_2 = 1:6)\npossible_rolls$sum <- possible_rolls$die_1 + possible_rolls$die_2\n\nmean(possible_rolls$sum > 7)\n\n[1] 0.4166667\n\n\nAs you can see, our simulated solution is extremely close to the ‚Äútrue‚Äù answer that we get by enumerating the sample space. If you prefer the answer as a fraction, we can also just count.\n\n# Number of possible combinations that sum to > 7\nsum(possible_rolls$sum  > 7)\n\n[1] 15\n\n# Number of possible combinations\nnrow(possible_rolls)\n\n[1] 36\n\n\nSo 15 out of the 36 possible combinations have a sum greater than 7, giving us the observed probability \\(0.41\\bar{6}.\\)\n\n\n\n\n\n\n\n\n\nWhat is the probability of rolling three six-sided dice and getting a value greater than seven?\n\n\n\nWe can solve this in the same way. We‚Äôll do it just via enumeration this time, since this book is about probability, not about simulations.\n\npossible_rolls <- expand.grid(die_1 = 1:6, die_2 = 1:6, die_3 = 1:6)\npossible_rolls$sum <- possible_rolls$die_1 + possible_rolls$die_2 +\n    possible_rolls$die_3\n\nsum(possible_rolls$sum > 7)\n\n[1] 181\n\nnrow(possible_rolls)\n\n[1] 216\n\nmean(possible_rolls$sum > 7)\n\n[1] 0.837963\n\n\nSo our answer is \\(181 / 216 \\approx 0.84.\\)\n\n\n\n\n\n\n\n\n\nThe Yankees are playing the Red Sox. You‚Äôre a diehard Sox fan and bet your friend they‚Äôll win the game. You‚Äôll pay your friend $30 if the Sox lose and your friend will have to pay you only $5 if the Sox win. What is the probability you have intuitively assigned to the belief that the Red Sox will win?\n\n\n\nTo solve this problem, we just use the formula that relates the odds to the probability of an event.\n\\[\\begin{align*}\n\\frac{P(x)}{P(\\lnot x)} &= \\frac{30}{5} \\\\\n\\frac{P(x)}{1 - P(x)} &= \\frac{30}{5} \\\\\nP(x) &= \\frac{30}{5} \\left( 1 - P(x) \\right) \\\\\nP(x) &= \\frac{30}{5} - \\frac{30}{5} P(x) \\\\\nP(x) + \\frac{30}{5} P(x) &= \\frac{30}{5} \\\\\n\\frac{35}{5} P(x) &= \\frac{30}{5} \\\\\n7 \\cdot P(x) &= 6 \\\\\nP(x) &= \\frac{6}{7}.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cp3/index.html",
    "href": "posts/cp3/index.html",
    "title": "Chapter 3",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 16)"
  },
  {
    "objectID": "posts/cp3/index.html#q1",
    "href": "posts/cp3/index.html#q1",
    "title": "Chapter 3",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat is the probability of rolling a 20 three times in a row on a 20-sided die?\n\n\n\nOf course, we could again solve by simulation üòÅ.\n\nset.seed(375)\nn_sims <- 1000000\nrolls <- tibble::tibble(\n    roll1 = sample(1:20, n_sims, replace = TRUE),\n    roll2 = sample(1:20, n_sims, replace = TRUE),\n    roll3 = sample(1:20, n_sims, replace = TRUE)\n)\n\nwith(rolls, mean(roll1 == 20 & roll2 == 20 & roll3 == 20))\n\n[1] 0.000124\n\n\nBut I don‚Äôt want anyone to take away my math degree, so we can show the analytical solution as well. (Besides, this event is quite rare, so the simulation will need a lot of samples, as you can see, to converge.)\nSince the probability of rolling a 20 on one die is \\(\\frac{1}{20}\\), and the rolls are (assumed) independent, we can calculate \\[P(\\text{three 20's}) = \\left( \\frac{1}{20} \\right)^3 = \\frac{1}{8000} = 0.000125.\\]"
  },
  {
    "objectID": "posts/cp3/index.html#q2",
    "href": "posts/cp3/index.html#q2",
    "title": "Chapter 3",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nThe weather report says there‚Äôs a 10 percent chance of rain tomorrow, and you forget your umbrella half the time you go out. What is the probability that you‚Äôll be caught in the rain without an umbrella tomorrow?\n\n\n\nIf these two events are independent (which is kind of weird but seems to be what the question wants), we just multiply the two probabilities to get\n\\[P(\\text{no umbrella} \\cap \\text{rain}) = (10\\%) (50\\%) = 5\\%.\\]"
  },
  {
    "objectID": "posts/cp3/index.html#q3",
    "href": "posts/cp3/index.html#q3",
    "title": "Chapter 3",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nRaw eggs have a \\(1/20000\\) probability of having salmonella. If you eat two raw eggs, what is the probability you ate a raw egg with salmonella?\n\n\n\nWe have to use the inclusion-exclusion rule here to avoid double-counting, since the two eggs having salmonella are not mutually exclusive (they could both have salmonella). So we get \\[\\begin{align*}\nP(\\text{ate a salmonella egg}) &= P(\\text{egg one had salmonella}) + P(\\text{egg two had salmonella})\n\\\\ &\\quad\\quad -P(\\text{both eggs had salmonella}) \\\\\n&= \\frac{1}{20000} + \\frac{1}{20000} - \\left(\\frac{1}{20000}\\right) ^2 \\\\\n&= \\frac{39,999}{400,000,000} \\approx 0.009\\%.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cp3/index.html#q4",
    "href": "posts/cp3/index.html#q4",
    "title": "Chapter 3",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nNote\n\n\n\nWhat is the probability of either flipping two heads in two coin tosses or rolling three 6s in three six-sided dice rolls?\n\n\nI hate how vaguely worded this question is, so I‚Äôll list the assumptions that I think we are making.\nWe are doing one experiment that involves us flipping a coin twice (or flipping two coins) and also rolling a d6 three times (or rolling 3d6 one time). These two things are all mutually independent from each other, both coin flips are independent, and all three die rolls are independent. The die rolls and coin flips do not affect each other. Once we have done all of these things during the experiment, we want to know the probability that we flipped two heads OR (inclusize) we rolled three sixes.\nSo, we use inclusion-exclusion again. We get that\n\\[P(\\text{two heads}) = \\frac{1}{2}\\cdot \\frac{1}{2} = \\frac{1}{4},\\] \\[P(\\text{three sixes}) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216},\\] and thus \\[P(\\text{two heads} \\cup \\text{three sixes}) = \\frac{1}{4} + \\frac{1}{216} - \\frac{1}{4 \\cdot 216} = \\frac{219}{864} \\approx 25.35\\%.\\]"
  },
  {
    "objectID": "posts/cp4/index.html",
    "href": "posts/cp4/index.html",
    "title": "Chapter 4",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "posts/cp4/index.html#q1",
    "href": "posts/cp4/index.html#q1",
    "title": "Chapter 4",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat are the parameters of the binomial distribution for the probability of rolling either a 1 or a 20 on a 20-sided die, if we roll the die 12 times?\n\n\n\nThe parameters are \\(n = 12\\) and \\(p = 2/20 = 1/10 = 0.1\\)."
  },
  {
    "objectID": "posts/cp4/index.html#q2",
    "href": "posts/cp4/index.html#q2",
    "title": "Chapter 4",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nThere are four aces in a deck of 52 cards. If you pull a card, return the card, then reshuffle, and pull a card again, how many ways can you pull just one ace in five pulls?\n\n\n\nSince we‚Äôre drawing with replacement, we are doing \\(5\\) Bernoulli trials, each with a probability of \\(4 / 52 = 1 / 13\\). So we can use the Binomial distribution to get this probability. We can get that\n\\[P(1 \\text{ ace in } 5 \\text{ pulls}) = B\\left( x = 1 \\mid n = 5, \\ p = \\frac{1}{13} \\right),\\]\nwhich we would then calculate as\n\\[P(1 \\text{ ace in } 5 \\text{ pulls}) = \\left( 5\\atop{1} \\right) \\left(\\frac{1}{13}\\right)^1\\left(\\frac{12}{13}\\right)^4.\\]\nTo get the answer, we can certainly write out the entire formula in R.\n\nchoose(5, 1) * (1 / 13) ^ 1 * (12 / 13) ^ 4\n\n[1] 0.2792\n\n\nBut here‚Äôs a sneaky trick. Since R was designed for statistical computing, a fast version of the binomial distribution is already built in.\n\ndbinom(x = 1, size = 5, prob = 1 / 13)\n\n[1] 0.2792"
  },
  {
    "objectID": "posts/cp4/index.html#q3",
    "href": "posts/cp4/index.html#q3",
    "title": "Chapter 4",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nFor the example in question 2, what is the probability of pulling 5 aces in 10 pulls (remember the card is shuffled back in the deck when it is pulled)?\n\n\n\nThis time I‚Äôll just use the R calculation, since this problem is worked out exactly the same way. The probability we want to estimate is \\[P(5 \\text{ aces in } 10 \\text{ pulls}) = B\\left( x = 5 \\mid n = 10, \\ p = \\frac{1}{13} \\right),\\] which we can approximate in R.\n\ndbinom(x = 5, size = 10, prob = 1 / 13)\n\n[1] 0.0004549\n\n\nNote that this probability is quite small ‚Äì that is because both drawing this many aces is difficult, and because this is the probability of drawing EXACTLY five aces."
  },
  {
    "objectID": "posts/cp4/index.html#q4",
    "href": "posts/cp4/index.html#q4",
    "title": "Chapter 4",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nWhen you‚Äôre searching for a new job, it‚Äôs always helpful to have more than one offer on the table so you can use it in negotiations. If you have a 1/5 probability of receiving a job offer when you interview, and you interview with seven companies in a month, what is the probability you‚Äôll have at least two competing offers by the end of that month.\n\n\n\nThis time we have to consider more than just one of these probabilities, because we want the probability of at least two offers. Let \\(O\\) be the number of competing offers we receive. Then we can write \\[P(O \\geq 2) = B\\left(O = 2 \\mid n = 7, p = \\frac{1}{5}\\right) + \\ldots + B\\left(O = 7 \\mid n = 7, p = \\frac{1}{5}\\right),\\] which we could also write more compactly as \\[P(O \\geq 2) = \\sum_{o = 2}^7 B\\left(O = o \\mid n = 7, p = \\frac{1}{5}\\right).\\] Since dbinom() is vectorized, we can calculate this pretty easily in R.\n\nsum( dbinom(x = 2:7, size = 7, prob = 1 / 5) )\n\n[1] 0.4233\n\n\nOf course, as you might have guessed, R has a shortcut for this: the pbinom function. This way is a little bit tricky though, because the way we have to specify the probability we want is a little more complicated than we would hope.\nThe lower.tail parameter specifies whether we want the cumulative probability above (FALSE) or below (TRUE) the argument(s) for the q parameter. For above, the probability is \\(P(X > x)\\), but for below, it is \\(P(X \\leq x)\\). Since we have a positive probability at \\(P(X = x)\\), we can‚Äôt just throw in 2 as the boundary, it will not give the correct answer. Instead we can use this function to calculate \\[P(X \\geq 2) = 1 - P(X < 2) = 1 - P(X \\leq 1) = P(X > 1).\\] This last two forms are the two that we can get with pbinom, as I‚Äôll calculate below.\n\n# P(O > 1)\npbinom(q = 1, size = 7, prob =  1 / 5, lower.tail = FALSE)\n\n[1] 0.4233\n\n# 1 - P(O <= 1)\n1 - pbinom(q = 1, size = 7, prob = 1 / 5)\n\n[1] 0.4233\n\n\nOf course all three of these solutions are equivalent, so no matter which way we do it, we get a probability of about \\(42\\%\\)."
  },
  {
    "objectID": "posts/cp4/index.html#q5",
    "href": "posts/cp4/index.html#q5",
    "title": "Chapter 4",
    "section": "Q5",
    "text": "Q5\n\n\n\n\n\n\nYou get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know that this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you‚Äôre tired. You really don‚Äôt want to go on this many interviews unless you are at least twice as likely to get at least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7?\n\n\n\nSo, what we actually want to calculate is the ratio \\[\\frac{P\\left( O \\geq 2 \\mid n =  7, \\ p = \\frac{1}{ 5} \\right)}\n       {P\\left( O \\geq 2 \\mid n = 25, \\ p = \\frac{1}{10} \\right)}.\\] In the previous problem, we calculated the numerator, so now we need to calculate the denominator. Fortunately, we can do this the same way, using the same practical considerations we did less time. Just to make this issue about the boundary more clear, I‚Äôll write out the R code for doing it all three ways.\n\nsum(dbinom(2:25, size = 25, prob = 1 / 10))\n\n[1] 0.7288\n\npbinom(q = 1, size = 25, prob = 1 / 10, lower.tail = FALSE)\n\n[1] 0.7288\n\n1 - pbinom(q = 1, size = 25, prob = 1 / 10)\n\n[1] 0.7288\n\n\nWe can then approximate the ratio we are interested in as \\[\\frac{42\\%}{73\\%} \\approx 0.58 > 0.50,\\] and I just realized that it probably makes more sense to invert this ratio. So let‚Äôs do that: \\[\\frac{73\\%}{42\\%} \\approx 1.74 < 2,\\] so we will improve our chances of getting at least two competing offers, but we will not be at least twice as likely, so we want to stick with just \\(7\\) interviews instead of \\(25\\). :::"
  },
  {
    "objectID": "posts/cp6/index.html",
    "href": "posts/cp6/index.html",
    "title": "Chapter 6: Conditional Probability",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 6)\nThis chapter introduces the concept of conditional probability, which, in my opinion, is one of the least intuitive parts of probability. I didn‚Äôt take a lot of notes here, but I did at least write down what is covered in this section. For a more detailed explanation of conditional probability, I recommend either Probability for the Enthusiastic Beginner by Morin or Mathematical Statistics with Applications by Wackerly et al.. (I make no money from these links, sales, or promotions.)"
  },
  {
    "objectID": "posts/cp6/index.html#q1",
    "href": "posts/cp6/index.html#q1",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat piece of information would we need in order to use Bayes‚Äô theorem to determine the probability that someone in 2010 who had GBS also had the flu vaccine that year?\n\n\n\nThis is another question that I have beef with. I think it is reasonable would interpret ‚Äúthe probability that someone in 2010 who had GBS also had the flu vaccine that year‚Äù as \\(P(\\text{GBS} \\cap \\text{flu vaccine}).\\) But the book wants us to read this as \\(P(\\text{flu vaccine} \\mid \\text{GBS}),\\) which I would tend to word as (unambiguously) ‚Äúthe probability that someone in 2010 received the flu vaccine, given that they had GBS.‚Äù Since conditional/joint probabilities are often vague like this, I think it is best to be specific instead of just making the word also italicized.\nBut anyways. The formula for this calculation is \\[P(\\text{flu vaccine} \\mid \\text{GBS}) = \\frac{P(\\text{flu vaccine}) \\\nP(\\text{GBS} \\mid \\text{flu vaccine})}{P(\\text{GBS})},\\] so the main thing we are missing is \\(P(\\text{GBS})\\), which would allow us to calculate \\(P(\\text{GBS} \\cap \\text{flu vaccine})\\), the only other part we were not given in the chapter."
  },
  {
    "objectID": "posts/cp6/index.html#q2",
    "href": "posts/cp6/index.html#q2",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nWhat is the probability that a random person picked from the population is female and is not color blind?\n\n\n\nMost of the details for this problem are given in the chapter. Note that for this example, the book is talking about the joint probability, despite what I think is very similar wording to the previous question, but maybe I need to work on my reading comprehension. Anyways, the relevant formula would be\n\\[P(\\text{female} \\cap \\lnot \\text{color blind}) = P(\\text{female}) \\ P(\\lnot\n\\text{color blind} \\mid \\text{female}).\\]\nAssuming male individuals and female individuals are equally likely in the population and are the only two options (neither of which is true, but they are both assumed to be true by the book for this question), we have all of the information that we need to solve this. We get\n\\[P(\\text{female} \\cap \\lnot \\text{color blind}) = (0.5)(1 - 0.005) = 0.4975.\\] So the probability is \\(49.75\\%,\\) since the incidence of color blindness among female members of the population is extremely low."
  },
  {
    "objectID": "posts/cp6/index.html#q3",
    "href": "posts/cp6/index.html#q3",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nWhat is the probability that a male who received the flu vaccine in 2010 is either color blind or has GBS?\n\n\n\nI‚Äôll steal from the notation that the book uses here, and let \\(A\\) be the event that this individual is colorblind, given that they are male, and let \\(B\\) be the event that this individual has GBS, given that they got the flu vaccine. Note that this makes several independence assumptions (being male or color blind are both independent of the flu vaccine and GBS, and vice versa), because if we don‚Äôt make this independence assumptions we would lack the necessary information to solve this problem.\nNow, the probability we want to find is \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B) - P(A) P(B \\mid A).\\]\nIn the chapter, we previously learned the values for \\(P(A)\\) and \\(P(B)\\), but I think this is where the problem starts to break down. The chapter hasn‚Äôt dealt with any really complicated events like this at all, and it doesn‚Äôt really make sense to say that \\[P(B \\mid A) = P( (\\text{GBS} \\mid \\text{flu vaccine}) \\mid (\\text{color\nblind} \\mid \\text{male}) ).\\]\nSo of course the book does not discuss how to address this problem, as it really isn‚Äôt very well-formed, it just says to assume \\(P(B \\mid A) = P(B).\\) This is true if we make all of the independence assumptions above, but again, stating the problem in this way doesn‚Äôt really make sense.\nSo, I‚Äôll detour and work through the problem in a way that I think is a bit more logical and arrives at the same answer.\nWhen I read the question, the probability that I think we want to get is \\[P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine}).\\] This is much more complex than any of the examples in the book, but fortunately for us, we can work it out using the laws of set theory that the book has not discussed at all. Note that, by definition of conditional probability,\n\\[\nP(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine})\n= \\frac{P\\left[(\\text{color blind} \\cup \\text{GBS}) \\cap (\\text{male} \\cap \\text{flu vaccine}) \\right]}{P(\\text{male} \\cap \\text{flu vaccine})}.\n\\] Now, the denominator is pretty easy to sort out. Since we don‚Äôt have any information on how being male affects flu vaccine coverage, we‚Äôll assume they are independent (although in real life, this is not true ‚Äì CDC flu vaccine coverage data for the USA suggests that males under 65 are less likely to get flu vaccines than females of the same age, but this different goes away for the elderly). Under this assumption, we get \\[P(\\text{male} \\cap \\text{flu vaccine}) = P(\\text{male}) \\ P(\\text{flu\nvaccine}),\\] which is easy enough to calculate with the data we have.\nSo it remains to deal with the beast of a numerator. For ease-of-use here, let‚Äôs define the event \\(C \\equiv \\text{color blind} \\cup \\text{GBS}\\) and the event \\(D \\equiv \\text{male} \\cap \\text{flu vaccine}.\\) Then, we get that\n\\[\nP(C \\cap D) = \\left(\\text{color blind} \\cup \\text{GBS}\\right) \\cap D =\n(\\text{color blind} \\cap D) \\cup (\\text{GBS} \\cap D)\n\\] by using the distributive rules of the \\(cap\\) and \\(cup\\) operators. These are not covered in the book, but one potential resource (in terms of set theory) is here. Many books on probability will also briefly cover this material.\nNow, we have a union, and we can use the addition (or inclusion-exclusion) principle to evaluate the probability. We get that\n\\[\\begin{align*}\nP((\\text{color blind} \\cap D) \\cup (\\text{GBS} \\cap D)) &=\nP(\\text{color blind} \\cap D) + P(\\text{GBS} \\cap D) - P(\\text{color blind}\n\\cap \\text{GBS} \\cap D) \\\\\n&=  P(\\text{color blind} \\cap \\text{male} \\cap \\text{flu vaccine}) + P(\\text{GBS} \\cap \\text{male} \\cap \\text{flu vaccine}) \\\\ &\\quad \\quad - P(\\text{color blind}\n\\cap \\text{GBS} \\cap (\\text{male} \\cap \\text{flu vaccine})).\n\\end{align*}\\]\nUnder the same set of independence assumptions before, we can make the following set of simplifications: \\[\\begin{align*}\nP(\\text{color blind} \\cap \\text{male} \\cap \\text{flu vaccine}) &=\n    P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\nP(\\text{GBS} \\cap \\text{male} \\cap \\text{flu vaccine}) &=\n    P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\nP(\\text{color blind} \\cap \\text{GBS} \\cap\n    (\\text{male} \\cap \\text{flu vaccine})) &=\n    P(\\text{GBS} \\cap \\text{flu vaccine} \\cap \\text{male} \\cap\n    \\text{color blind}) \\\\\n    &= P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine}).\n\\end{align*}\\]\nNow, it would be just about impossible to write all that out in text format and still have it fit into the margins of this page. But nonethless we will try. We get that, overall,\n\n\\[\\begin{equation*}P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine}) =\n\\frac{\n\\left(\\begin{split}\n    &P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\n    &\\quad + P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\n    &\\quad - P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine})\n\\end{split}\\right)\n}{\nP(\\text{male}) \\ P(\\text{flu\nvaccine})\n}.\n\\end{equation*}\\]\n\nI can‚Äôt figure out how to make the text of that smaller in this quarto HTML document and it‚Äôs too much work to figure out how to size the LaTeX code in a way that‚Äôs supported by MathJax so that‚Äôs the best that you‚Äôre getting from me.\nAnyways, if we want to solve this with the information given in the book, we have to get rid of all of the \\(P(\\text{flu vaccine})\\), since we don‚Äôt know that. Fortunately for us, they cancel out. First, let‚Äôs plug in the numbers that we do know so this is a bit less cumbersome to type. Let\n\\[\\mathrm{P} = P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine})\\]\nfor convenience (should‚Äôve said this earlier but it‚Äôs too late now). We get \\[\\begin{align*}\n\\mathrm{P} & =\n\\frac{\n\\left(\\begin{split}\n    &P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\n    &\\quad + P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\n    &\\quad - P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine})\n\\end{split}\\right)\n}{\nP(\\text{male}) \\ P(\\text{flu\nvaccine})\n} \\\\ \\\\ &= \\frac{\n0.04 \\ P(\\text{FV}) + 0.5 \\ P(\\text{GBS} \\mid \\text{FV}) \\ P(\\text{FV}) -\n    0.04 \\ P(\\text{GBS} \\mid \\text{FV}) \\ P(\\text{FV})\n}{\n0.5 \\ P(\\text{FV})\n} \\\\ &=\n\\frac{P(\\text{FV})\\left( 0.04 + 0.46 \\ P(\\text{GBS} \\mid \\text{FV}) \\right)}{\n0.5 \\ P(\\text{FV})\n} \\\\\n&= \\frac{0.04 + 0.46 (3 / 100,000)}{0.5} \\\\\n&= 0.0800276 = 8.00276\\%.\n\\end{align*}\\]\nThis is the same answer that the book has, so their shortcut that I don‚Äôt like apparently works (and saves a lot of time). But I had a good time proving that it was true without shortcuts, so I guess it was worth it.\nWell, that‚Äôs all for this chapter!"
  },
  {
    "objectID": "posts/cp5/index.html",
    "href": "posts/cp5/index.html",
    "title": "Chapter 5: the Beta Distribution",
    "section": "",
    "text": "library(latex2exp)\n\nWarning: package 'latex2exp' was built under R version 4.2.2\n\noptions(\"scipen\" = 9999, \"digits\" = 16)"
  },
  {
    "objectID": "posts/cp5/index.html#q1",
    "href": "posts/cp5/index.html#q1",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nYou want to use the beta distribution to determine whether or not a coin you have is a fair coin‚Äâ‚Äî‚Äâmeaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. Using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time?\n\n\n\nThe probability we are interested in is \\[P(p \\geq 0.6) = \\int_0^{0.6} \\mathrm{Beta}(p, 4, 6) \\ \\mathrm{d}p.\\]\nWe could, of course, write out the distribution and integrate it: \\[P(p \\geq 0.6) = \\int_0^{0.6}  \\frac{1}{\\mathrm{B} \\left(4, 6\\right)}p^{4-1}(1-p)^{6-1} \\ \\mathrm{d}p.\\]\nHere, \\(\\mathrm{B}(a, b)\\) is the beta function, not the binomial distribution. In general, we could integrate \\[\\int p^{a - 1}(1-p)^{b - 1} \\ \\mathrm{d}p \\] for real, known constants \\(k, a, b\\), but the general integral in terms of \\(a\\) and \\(b\\) is non-elementary, so it is impractical to do this symbolically. The solution to this integral is called the ‚Äúregularized incomplete beta function‚Äù, \\(I_x(a, b).\\) That means the solution to our original integral is \\[ P(p \\geq 0.6) = I_{0.6}(4, 6) = \\frac{\\mathrm{B}_{0.6}(4, 6)}{\\mathrm{B}(4, 6)}, \\] which we can‚Äôt actually calculate in base R (there is no built-in function for the incomplete beta function, although there could be a math identity that I don‚Äôt know that lets us calculate it with the tools at our disposal). Apparently this does not come up too often (for reasons we will see shortly), but there are a few packages that offer the incomplete beta function. I found: spsh, UCS, and the one I‚Äôll load, zipfR. These all do other things so it seems potentially worthwhile to me to implement a package that ONLY includes the incomplete beta and gamma functions.\n\nzipfR::Ibeta(0.6, 4, 6) / beta(4, 6)\n\n[1] 0.9006474239999999\n\n\nNote that we could get the same thing by numerical integration.\n\nintegrate(\n    f = \\(p) (1 / beta(4, 6)) * p ^ 3 * (1 - p) ^ 5,\n    lower = 0,\n    upper = 0.6\n)\n\n0.9006474239999995 with absolute error < 0.00000000000001\n\n\nThe two values are the same to like 15 digits of precision. So we really don‚Äôt need to go through all of that mess, we can just use numerical integration. As you may have guessed, R has multiple easier ways to do this. First, the beta density is already built into R, so we don‚Äôt have to write out the entire thing.\n\nintegrate(\n    f = \\(p) dbeta(p, 4, 6),\n    lower = 0,\n    upper = 0.6\n)\n\n0.900647424 with absolute error < 0.00000000000001\n\n\nAnd of course, R has a built-in, better way to get this probability than using the standard numerical integrator. (And since the beta distribution is continuous, we don‚Äôt even have to worry about the boundaries this time.)\n\npbeta(0.6, 4, 6, lower.tail = TRUE)\n\n[1] 0.9006474239999999"
  },
  {
    "objectID": "posts/cp5/index.html#q2",
    "href": "posts/cp5/index.html#q2",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nYou flip the coin 10 more times and now have 9 heads and 11 tails total. What is the probability that the coin is fair, using our definition of fair, give or take 5%?\n\n\n\nSo, we can either numerically integrate, which allows us to specify whatever bounds we want (since this integrand is pretty well-behaved for this kind of thing).\n\nintegrate(\n    f = \\(p) dbeta(p, 9, 11),\n    lower = 0.45,\n    upper = 0.55\n)\n\n0.3098800156513043 with absolute error < 0.0000000000000034\n\n\nOr we can just subtract.\n\npbeta(0.55, 9, 11) - pbeta(0.45, 9, 11)\n\n[1] 0.3098800156513036\n\n\nSometimes subtraction can be a problem with numerical computing, but if subtraction will cause a problem, so will numerical integration, so this latter method is what I would default to."
  },
  {
    "objectID": "posts/cp5/index.html#q3",
    "href": "posts/cp5/index.html#q3",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nData is the best way to become more confident in your assertions. You flip the coin 200 more times and end up with 109 heads and 111 tails. Now what is the probability that the coin is fair, give or take 5%?\n\n\n\nAlright, I‚Äôve already talked enough about how to solve these problems, this is just another of the same thing. So let‚Äôs just solve it.\n\npbeta(0.55, 109, 111) - pbeta(0.45, 109, 111)\n\n[1] 0.8589371426532764\n\n\nThat‚Äôs a pretty good chance, I think."
  },
  {
    "objectID": "posts/cp7/index.html",
    "href": "posts/cp7/index.html",
    "title": "Chapter 7: Bayes‚Äôs Theorem with LEGO",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 16)\nAgain, no notes. Mostly because this chapter is super short. Just for good measure, I‚Äôll do the mathematician thing and say. Recall Bayes‚Äô Theorem:\n\\[P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.\\]"
  },
  {
    "objectID": "posts/cp7/index.html#q1",
    "href": "posts/cp7/index.html#q1",
    "title": "Chapter 7: Bayes‚Äôs Theorem with LEGO",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nKansas City, despite its name, sits on the border of two US states: Missouri and Kansas. The Kansas City metropolitan area consists of 15 counties, 9 in Missouri, and 6 in Kansas. The entire state of Kansas has 105 counties and Missouri has 114. Use Bayes‚Äô theorem to calculate the probability that a relative who just moved to a country in the KC metropolitan area also lives in a county in Kansas. Make sure to show \\(P(\\text{Kansas})\\) (assuming your relative lives in either Kansas or Missouri), \\(P(\\text{Kansas City Metropolitan Area}),\\) and \\(P(\\text{Kansas City Metropolitan Area} \\mid \\text{Kansas})\\).\n\n\n\nAssuming that our relative is equally likely to have moved to any of the counties in the KC metropolitan area is an interesting choice, but is necessary for doing this problem. So we‚Äôll assume that. Now, assuming that our relative has moved to either Kansas or Missouri, the overall probability that they are in Kansas is \\[P(\\text{Kansas}) = \\frac{105}{105 + 114} = \\frac{105}{219}.\\] The probability that our relative lives in Kansas City, assuming they live in either Kansas or Missouri (and again, that all counties are equally likely) is \\[P(\\text{KC}) = \\frac{15}{219}.\\] The probability that our relative lives in Kansas City, if we already knew that they lived in Kansas, is \\[P(\\text{KC} \\mid \\text{Kansas}) = \\frac{6/219}{105/219}=\\frac{6}{105}.\\] Then, by Bayes‚Äô Theorem, we get that the probability that our relative lives in Kansas, given that they live in Kansas City, is \\[P(\\text{Kansas} \\mid \\text{KC}) = \\frac{P(\\text{KC} \\mid \\text{Kansas})\n\\ P(\\text{Kansas})}{P(\\text{KC})} = \\frac{\\frac{6}{105} \\frac{105}{219}}{\n\\frac{15}{219}} = \\frac{6/219}{15/219} = \\frac{6}{15} = 40\\%.\\]"
  },
  {
    "objectID": "posts/cp7/index.html#q2",
    "href": "posts/cp7/index.html#q2",
    "title": "Chapter 7: Bayes‚Äôs Theorem with LEGO",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nA deck of cards has 52 cards with suits that are either red or black. There are four aces in a deck of cards: two red and two black. You remove a red ace from the deck and shuffle the cards. Your friend pulls a black card. What is the probability that it is an ace?\n\n\n\nI like this problem more because it‚Äôs like orbs in an urn. We don‚Äôt have to make any other assumptions, this is just good normal probabilities. Anyways.\nThe probability that our friend draws an ace from the remaining 51 cards is \\[P(\\text{ace}) = \\frac{3}{51}.\\] (I am deliberately chosing not to reduce this fraction, because I don‚Äôt have to and because the solution is easier to understand if the fractions aren‚Äôt reduced.) The probability that our friend draws a black card from the remaining 51 cards (26 of which are black and 25 of which are red) is \\[P(\\text{black}) = \\frac{26}{51}.\\] Now, if our friend draws an ace, the probability that it is black (given that we know it is an ace already) is \\[P(\\text{black} \\mid \\text{ace}) = \\frac{P(\\text{black and ace})}{\nP(\\text{ace})} = \\frac{2 / 51}{3 / 51} = \\frac{2}{3}.\\] Now we can use Bayes‚Äô theorem to get that \\[P(\\text{ace} \\mid \\text{black}) = \\frac{P(\\text{black} \\mid \\text{ace})\nP(\\text{black})}{P(\\text{ace})} = \\frac{\\frac{2}{3} \\ \\frac{3}{51}}{\n\\frac{26}{51}} = \\frac{2 / 51}{26 / 51} = \\frac{2}{26} = \\frac{1}{13}.\\] Since all the black cards are still in the deck, once we know that our friend has drawn a black card, the probability that is an ace is the same as if they had drawn from the full deck (if we hadn‚Äôt removed a card). How neat!"
  },
  {
    "objectID": "posts/cp8/index.html",
    "href": "posts/cp8/index.html",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes‚Äô Theorem",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter is mostly about Bayes‚Äô factors without saying that it‚Äôs about Bayes factors, but it also discusses what happens when prior beliefs change (in a particularly simple case). I personally don‚Äôt think this chapter generalizes very well to more complex analyses, but I guess the point of this book is to be beginner-friendly. So maybe I‚Äôm at the point on my Bayes journey where I am confident beyond what I actually know ¬Ø\\(„ÉÑ)/¬Ø."
  },
  {
    "objectID": "posts/cp8/index.html#q1",
    "href": "posts/cp8/index.html#q1",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes‚Äô Theorem",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nAs mentioned, you mighty disagree with the original probability assigned to the likelihood:\n\\[P(\\text{broken window, open front door, missing laptop} \\mid \\text{robbed}) = \\frac{3}{10}.\\]\nIf we change this probability to \\(1/100\\), how does this change our strength in believing \\(H_1\\) over \\(H_2\\)?\n\n\n\n(I bolded the part that was supposed to be in the question but then wasn‚Äôt.) If we do this, it only changes the numerator of our ratio and we get that \\[B = \\frac{3 / 100000}{1 / 21,900,000} = 675. \\] So this decreases our ratio by a factor of 10, the same amount that our beliefs changed."
  },
  {
    "objectID": "posts/cp8/index.html#q2",
    "href": "posts/cp8/index.html#q2",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes‚Äô Theorem",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nHow unlikely would you have to believe being robbed is ‚Äî our prior for \\(H_1\\) ‚Äî in order for the ratio of \\(H_1\\) to \\(H_2\\) to be one?\n\n\n\nCall our unknown prior probability \\(p\\). Then we solve: \\[\\frac{p \\ \\frac{3}{100}}{1 / 21,900,000} \\implies p = \\frac{1}{65,700}.\\] Our prior probability would need to decrease by a factor of 657."
  }
]