[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bookwork Blog",
    "section": "",
    "text": "This is my blog for storing notes and work from books that I read, so I could consolidate them all into one web page.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Rethinking\n\n\n\nStatistics\n\n\nBayesian\n\n\nCausal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8: Conditional Manatees\n\n\n\n\n\n\nZane Billings\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Ulysses’ Compass\n\n\n\n\n\n\nZane Billings\n\n\nNov 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: The Haunted DAG and the Causal Terror\n\n\n\n\n\n\nZane Billings\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: The Many Variables and the Spurious Waffles\n\n\n\n\n\n\nZane Billings\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Geocentric Models\n\n\n\n\n\n\nZane Billings\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Sampling the Imaginary\n\n\n\n\n\n\nZane Billings\n\n\nAug 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Small Worlds and Large Worlds\n\n\n\n\n\n\nZane Billings\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: The Golem of Prague\n\n\n\n\n\n\nZane Billings\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics the Fun Way\n\n\n\nStatistics\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 4\n\n\n\n\n\n\nZane Billings\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 3\n\n\n\n\n\n\nZane Billings\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 2\n\n\n\n\n\n\nZane Billings\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 1\n\n\n\n\n\n\nZane Billings\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sr/cp5.html",
    "href": "sr/cp5.html",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "",
    "text": "This chapter discusses the “causal salad” issue that is really prevelant in epidemiology (and other sciences) right now. When you “adjust” for variables in models, what are you actually doing? What answers can you get from adjusting? How do you decide what variables should go into a model? We get to talk about confounding, which is one of my favorite subjects, and more generally, other types of biases. These are presented in the framework of graphical causal models using directed acyclic graphs (DAGs)."
  },
  {
    "objectID": "sr/cp5.html#chapter-notes",
    "href": "sr/cp5.html#chapter-notes",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nThis chapter is mainly concerned with the issue of confounding, although it doesn’t used the same technical terms that I learned in my epidemiology classes.\nSpecifically, we are concerned with spurious associations (positive confounding) where a third variable causes the relationship between two variables to appear stronger than it is;\nAnd with masked relationships (negative confounding), where a third variable causes the relationship between two variables to appear weaker than it is.\nThis chapter also introduces Directed Acyclic Graphs (DAGs) as heuristic graphical causal models for understanding the causal relationships between variables.\nThe conditional independencies of DAGs are disccused, which are statements of which variables should be associated with each other in the data or not, given that the DAG is an accurate causal model. DAGs with the same variables and implied conditional independencies are called Markov Equivalent.\nThis chapter also discusses three ways to visualize the results of a regression model: predictor residual plots, posterior prediction plots, and counterfactual plots. In this case, the counterfactual plot does not necessarily mean what I am used to it meaning, it just means we are predicting values which may not have been observed using the model (so in some sense they are counterfactual to our observed data).\nFinally, this chapter also discusses categorical variables and index coding, which is used by the rethinking package (and later by Stan) rather than the dummy coding used by most R models."
  },
  {
    "objectID": "sr/cp5.html#exercises",
    "href": "sr/cp5.html#exercises",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "Exercises",
    "text": "Exercises\n\n5E1\nThe linear models\n\\[\\mu_i = \\beta_x x_i + \\beta_z z_i\\] and \\[\\mu_i = \\alpha + \\beta_x x_i + \\beta_z z_i\\]\nare both multiple regression models. The first model \\[\\mu_i = \\alpha + \\beta x_i\\] is a simple linear regression model and while the third model \\[\\left( \\mu_i = \\alpha + \\beta (x_i - z_i) \\right)\\] involves both \\(x\\) and \\(z\\), the model only has one coefficient and treats their difference as a single explanatory variable.\n\n\n5E2\nWe could evaluate the claim animal diversity is linearly related to latitude, but only after controlling for plant diversity using the linear model \\[\\begin{align*}\n\\text{animal diversity}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\mathrm{latitude} \\right) + \\beta_2 \\left( \\text{plant diversity} \\right)\n\\end{align*}\\]\nwhere suitable priors are assigned and other appropriate parameters are given for the likelihood function.\n\n\n5E3\nWe could evaluate the claim neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree using the multiple linear regression \\[\\begin{align*}\n\\text{time to PhD}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\text{amount of funding} \\right) + \\beta_2 \\left( \\text{size of laboratory} \\right)\n\\end{align*}\\]\nwith suitable priors, etc. The slope of both \\(\\beta_j\\) should be positive. Classically, I would probably be inclined to include an interaction term in this model, but we haven’t talked about that yet in the book so I didn’t.\n\n\n5E4\nIf we have a single categorical variable with levels \\(A,\\) \\(B,\\) \\(C,\\) and \\(D,\\) (represented as indicator variables), the following linear models are inferentially equivalent: \\[\\begin{align*}\n\\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i, \\quad \\text{ and }\\\\\n\\mu_i &= \\alpha_A \\left( 1 - B_i - C_i - D_i \\right) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i.\n\\end{align*}\\]\n\n\n5M1\nAn example of a spurious correlation: I am happy on days when it is sunny outside, and when I get to leave work early. Both of these things individually make me happy, but the weather doesn’t determine whether I get to leave work early. (At least not fully anyways. The weather definitely determines how much work I get done, but other external factors control the amount of work I have and the deadlines I need to meet.)\n\n\n5M2\nAn example of a masked relationship that I like: supposed you have multiple measurements of how far the accelerator is pressed in a car and the car’s speed (taken simultaneously) at multiple time points, and you see no correlation. However, you are then given the measurements of the slope of the road at each of those time points, and you see that, when the slope of the road is taken into account, the two variables are correlated. When the car is going uphill, the accelerator is always pressed further and the speed is always lower, but pressing the accelerator still increases the speed.\n\n\n5M3\nI guess a higher divorce rate could cause a higher marriage rate by making more people available to be married. If divorced people tend to get remarried (potentially to non-divorced people), then the overall marriage rate could go up. Addressing this using a multiple linear regression model would be quite difficult, as you would need more data on remarriage and divorce status. It could be difficult to incorporate remarriages into the regression model, maybe an agent-based model would be more intuitive for this.\n\n\n5M4\nI found this list of percent LDS population by state. So if we want to add this as a predictor to the divorce rate model, first I’ll join these data to the WaffleDivorce data from the rethinking package.\n\npct_lds &lt;- readr::read_csv(here::here(\"static/pct_lds.csv\"))\n\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (3): mormonPop, mormonRate, Pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\nThis is cmdstanr version 0.8.1.9000\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n- CmdStan version: 2.34.1\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\nLoading required package: posterior\nThis is posterior version 1.6.0\n\nAttaching package: 'posterior'\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nLoading required package: parallel\nrethinking (Version 2.40)\n\nAttaching package: 'rethinking'\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(\"WaffleDivorce\")\n\ndat_5m4 &lt;-\n    dplyr::left_join(\n        WaffleDivorce,\n        pct_lds,\n        by = c(\"Location\" = \"State\")\n    )\n\ndat_5m4[\n    which(dat_5m4$Location == \"District of Columbia\"),\n    \"mormonRate\"\n] &lt;-0.0038\n\ndat_5m4_l &lt;-\n    dat_5m4 |&gt;\n    dplyr::transmute(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage,\n        L = mormonRate * 100\n    ) |&gt;\n    as.list()\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\npurrr::walk2(dat_5m4_l, names(dat_5m4_l), ~hist(.x, main = .y, breaks = \"FD\"))\n\n\n\n\n\n\n\nlayout(1)\n\nWhen I tried to do this the first time, it turns out that the WaffleDivorce dataset has D.C. in it, but not the state of Nevada? And the table of percent LDS populations I found has Nevada, but not D.C. So I just googled it, and on the Wikipedia page I saw that the percentage was 0.38% in 2014, which is good enough for government work, so I filled it in manually. I didn’t transform any of the predictors, but standardizing and transforming them would probably be a good idea. I think a logit transformation would probably be suitable for the percent LDS population but as long as quap() converges I won’t worry about it too much.\n\\[\\begin{align*}\n\\text{Divorce rate}_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot \\text{Marriage rate}_i + \\beta_2 \\cdot \\text{Median age at marriage}_i \\\\\n&\\quad\\quad + \\beta_3 \\cdot \\text{Percent LDS}_i \\\\\n\\beta_0 &\\sim \\mathrm{Unif}(0, 1000); \\\\\n\\beta_j &\\sim \\mathrm{Normal}(0, 10); \\quad j = \\{0, \\ldots, 3\\} \\\\\n\\sigma &\\sim \\mathrm{Exp}(0.5)\n\\end{align*}\\]\nwhere \\(i\\) indexes the states. Since the outcome is in units of divorces per 1000 people, I decided to let the intercept be anything from 0 to 1000 people. It will probably be quite small but that shouldn’t be too much of a problem I think. Then, I assigned weakly uninformative priors to the slope coefficients and a simple positive prior to the standard deviation. It would be better to do a prior predictive simulation and figure out some better assumptions. Now we’ll fit th model with quap (quadratic approximation).\n\nset.seed(100)\nfit_5m4 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            # We could rewrite this so we didn't have to write out all the identical\n            # priors but this is easier and I am lazy\n            mu &lt;- b0 + bM * M + bA * A + bL * L,\n            b0 ~ dunif(0, 100),\n            bM ~ dnorm(0, 10),\n            bA ~ dnorm(0, 10),\n            bL ~ dnorm(0, 10),\n            sigma ~ dexp(0.5)\n        ),\n        data = dat_5m4_l\n    )\n\nrethinking::precis(fit_5m4)\n\n              mean         sd       5.5%       94.5%\nb0    38.634820524 6.90452240 27.6000602 49.66958086\nbM     0.003608067 0.07550132 -0.1170576  0.12427376\nbA    -1.105494100 0.22397859 -1.4634551 -0.74753306\nbL    -0.066488235 0.02396230 -0.1047846 -0.02819184\nsigma  1.331562503 0.13193156  1.1207104  1.54241461\n\n\nWe can see that for every one percentage increase in the LDS population of a state, the model predicts that the divorce rate will decrease by -0.07 units. Since the divorce rate is in percentage units, this means we would need slightly less than a 15% increase in LDS population for a state’s divorce rate to decrease by 1%.\nThis estimate is probably biased by Utah, which is a strong outlier with 63% LDS population (far more than the second highest state, Idaho, with 24% of the population identifying as LDS).\n\n\n5M5\nFor this exercise, I’ll call the price of gas \\(G\\), obesity \\(O\\), exercise \\(E\\), and eating at restaurants \\(R\\). The dag for this hypothesis looks like this.\n\ndag &lt;- dagitty::dagitty(\n    \"dag {\n        G -&gt; E\n        G -&gt; R\n        G -&gt; O\n        E -&gt; O\n        R -&gt; O\n    }\"\n)\n\nplot(dagitty::graphLayout(dag))\n\n\n\n\n\n\n\n\nTherefore, \\(G\\) confounds the relationship of \\(E\\) on \\(O\\) and also confounds the relationship of \\(R\\) on \\(O\\). (The direct causal effect of \\(G\\) on \\(O\\) represents the total impact of any other pathways that we have not measured.) We could capture both of these pathways simultaneously with the multiple regression \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_R R_i + \\beta_G G_i,\\] where \\(\\mu_i\\) is the conditional mean of \\(O_i\\). This model “controls for” the price of gas, and additionally controls for the effect of \\(E\\) and \\(R\\) on each other. If we are certain that \\(E\\) and \\(R\\) are related only though \\(G\\), we could fit the two regression models \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_G G_i\\] and \\[\\mu_i = \\alpha + \\beta_R R_i + \\beta_G G_i.\\] Using these two models, we control for the effect of gas price and obtain the direct causal effect of \\(E\\) and \\(R\\), assuming that \\(E\\) and \\(R\\) do not affect each other at all.\n\n\n5H1\nAssuming the DAG for the divorce problem is \\(M \\to A \\to D\\). The DAG only has one conditional independency: \\(D\\) and \\(M\\) are uncorrelated if we condition on \\(A\\). We can check this using dagitty as well.\n\ndag2 &lt;- dagitty::dagitty(\"dag {M -&gt; A -&gt; D}\")\ndagitty::impliedConditionalIndependencies(dag2)\n\nD _||_ M | A\n\n\nNow we can check if the data are consistent with this DAG. (We already know that it is, because this is the same conditional independency set as one of the previous example DAGs).\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::select(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage\n    ) |&gt;\n    dplyr::mutate(dplyr::across(tidyselect::everything(), standardize))\n\n# Fit the model without age so we can see the unconditional estimate\nmodel_noage &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age only\nmodel_ageonly &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age to see the estimate after conditioning\nmodel_5h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\ncoeftab_plot(\n    coeftab(model_noage, model_ageonly, model_5h1),\n    par = c(\"bA\", \"bM\")\n)\n\n\n\n\n\n\n\n\nWe can see that before we add age to the model, the effect of marriage is quit large. But then when we condition on age, the effect is close to zero with a large amount of uncertainty. So it appears that our data are consistent with the conditional independencies of the model. The coefficient for age does not change when we condition it on marriage rate, so this supports our conclusion.\n\n\n5H2\nAssuming that this is the true DAG for the divorce example, we want to fit a new model and estimate the counterfactual effect of halving a state’s marriage rate \\(M\\).\n\nset.seed(100)\nm_5h2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            ## M -&gt; A\n            A ~ dnorm(mu_A, sigma_A),\n            mu_A &lt;- b0_A + bM * M,\n            b0_A ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma_A ~ dexp(1),\n            \n            ## A -&gt; D\n            D ~ dnorm(mu_D, sigma_D),\n            mu_D &lt;- b0_D + bA * A,\n            b0_D ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma_D ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_5h2)\n\n                 mean         sd       5.5%      94.5%\nb0_A    -6.922370e-08 0.08684788 -0.1387998  0.1387996\nbM      -6.947376e-01 0.09572699 -0.8477278 -0.5417474\nsigma_A  6.817373e-01 0.06758016  0.5737312  0.7897434\nb0_D    -4.353665e-07 0.09737877 -0.1556305  0.1556296\nbA      -5.684028e-01 0.10999981 -0.7442038 -0.3926019\nsigma_D  7.883257e-01 0.07801134  0.6636485  0.9130029\n\n# Get the halved marriage rate for each state and standardize to model units\nwith(\n    WaffleDivorce,\n    M_seq &lt;&lt;- c(\n        ((Divorce / 2) - mean(Divorce)) / sd(Divorce),\n        d$D\n    )\n)\n\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\nres &lt;-\n    s |&gt;\n    # stack the matrix columns on top of each other into one vector\n    lapply(as.vector) |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::mutate(\n        M = rep(M_seq, each = nrow(s$A))\n    )\n    \nres_diff &lt;-\n    tibble::tibble(\n        A = s$A[, 2] - s$A[, 1],\n        D = s$D[, 2] - s$D[, 1]\n    )\n\ntest &lt;-\n    lapply(s, \\(x) tibble::tibble(\n        mean = colMeans(x[, 51:100] - x[, 1:50]),\n        lwr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[1, ],\n        upr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[2, ]\n    ))\n\ntest2 &lt;- test$D\ntest2 &lt;- cbind(test2, WaffleDivorce)\n\nlibrary(ggplot2)\nggplot(test2, aes(y = forcats::fct_reorder(Location, mean),\n                                    x = mean, xmin = lwr, xmax = upr)) +\n    geom_pointrange() +\n    scale_x_continuous(\n        labels = function(x) scales::label_number()((x * sd(WaffleDivorce$Divorce)))\n    ) +\n    labs(\n        x = \"Counterfactual effect on divorce rate of halving marriage rate (mean, 89% CI)\",\n        y = NULL\n    ) +\n    hgp::theme_ms()\n\n\n\n\n\n\n\n\nFrom the precis, we can derive that for every 1 unit increase in the marriage rate, we expect \\(\\beta_M\\) units of change in the median age of marriage, and thus \\(\\beta_A \\beta_M\\) units of change in the divorce rate, which works out to approximately \\((-0.69)(-0.57) = 0.3933\\). So if the marriage rate increases by 1 standard deviation, we expect the divorce rate to increase by about 0.4 standard deviations.\nWe could also (much more easily, in fact, I just didn’t think about it until after I did this the hard way) compute the counterfactual effect for a range of values, and then look up whatever we wanted. The divorce rate measurements in the original data are only measured to the nearest tenth, so we just need to simulate the counterfactual effect of every one-tenth unit change in divorce rate over the range of observed rates.\n\n# Generate the sequence\nM_seq &lt;-\n    seq(\n        from = 4,\n        to = 16,\n        by = 0.01\n    )\n# Restandardize it using the original values for model units\nM_seq &lt;- (M_seq - mean(WaffleDivorce$Marriage)) /\n    sd(WaffleDivorce$Marriage)\n\n# Do the simulation\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\n\n# Clean up the results\nD_res &lt;-\n    tibble::tibble(\n        M = round(M_seq * sd(WaffleDivorce$Marriage) + mean(WaffleDivorce$Marriage),\n                            digits = 2),\n        mean = colMeans(s$D),\n        lwr = apply(s$D, 2, rethinking::PI)[1, ],\n        upr = apply(s$D, 2, rethinking::PI)[2, ]\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(c(mean, lwr, upr), \\(x) x * sd(WaffleDivorce$Divorce) +\n                                        mean(WaffleDivorce$Divorce) |&gt; round(digits = 2))\n    )\n\nmanipulated &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        Location, Loc, M = Marriage / 2, orig = Marriage, out = Divorce\n    ) |&gt;\n    dplyr::left_join(D_res, by = \"M\")\n\n# Make the plot\nD_res |&gt;\n    ggplot(aes(x = M, y = mean, ymin = lwr, ymax = upr)) +\n    geom_ribbon(fill = \"gray\") +\n    geom_line(size = 0.75) +\n    geom_point(\n        data = manipulated,\n        fill = \"white\",\n        color = \"black\",\n        shape = 21,\n        stroke = 1.5,\n        size = 3\n    ) +\n    labs(\n        x = \"Manipulated marriage rate\",\n        y = \"Counterfactual divorce rate\"\n    ) +\n    hgp::theme_ms()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# manipulated |&gt;\n#   dplyr::mutate(id = factor(dplyr::row_number())) |&gt;\n#   ggplot() +\n#   geom_segment(\n#       aes(x = orig, xend = M, y = out, yend = mean, color = id),\n#       show.legend = FALSE,\n#       alpha = 0.5\n#   ) +\n#   geom_point(\n#       aes(x = M, y = mean, shape = \"Counterfactual\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   geom_point(\n#       aes(x = orig, y = out, shape = \"Observed\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   guides(\n#       fill = guide_none()\n#   ) +\n#   scale_shape_manual(values = c(21, 22)) +\n#   labs(\n#       x = \"Marriage rate\",\n#       y = \"Divorce rate\"\n#   ) +\n#   hgp::theme_ms()\n\nThere we go. The white points here show each of the states if their divorce rate were halved (and the model is true). I didn’t label them because I had already spend too much time on this project. There’s a lot more I could think of to do on this problem, but instead I decided to move on to the next one instead.\n\n\n5H3\nWe are given the following DAG for the milk energy problem.\n\ndag3 &lt;- dagitty::dagitty(\"dag {K &lt;- M -&gt; N -&gt; K}\")\n\nWe want to compute the counterfactual effect on K of doubling M, accounting for both the direct and indirect paths of causation.\n\ndata(\"milk\")\nm &lt;-\n    milk |&gt;\n    dplyr::transmute(\n        N = standardize(neocortex.perc),\n        M = standardize(log(mass)),\n        K = standardize(kcal.per.g)\n    ) |&gt;\n    tidyr::drop_na()\n\nm_5h3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            # M -&gt; N\n            N ~ dnorm(mu_n, sigma_n),\n            mu_n &lt;- a_n + b_m * M,\n            a_n ~ dnorm(0, 0.2),\n            b_m ~ dnorm(0, 0.5),\n            sigma_n ~ dexp(1),\n            # M -&gt; K &lt;- N\n            K ~ dnorm(mu_k, sigma_k),\n            mu_k &lt;- a_k + b_m * M + b_n * N,\n            a_k ~ dnorm(0, 0.2),\n            b_n ~ dnorm(0, 0.5),\n            sigma_k ~ dexp(1)\n        ),\n        data = m\n    )\n\nrethinking::precis(m_5h3)\n\n                mean        sd       5.5%     94.5%\na_n     -0.008405637 0.1274276 -0.2120595 0.1952482\nb_m      0.417649823 0.1672193  0.1504011 0.6848986\nsigma_n  0.681045996 0.1323715  0.4694908 0.8926012\na_k      0.026405034 0.1660113 -0.2389131 0.2917232\nb_n     -0.138437700 0.2789888 -0.5843156 0.3074402\nsigma_k  1.223352397 0.2214232  0.8694753 1.5772295\n\n\nIn this case, the predictor we want the counterfactual effect of (\\(M\\)) is on a log scale, so we should be able to get the effect of halving it (since changes will be proportional on a multiplicative scale). But I’m not sure I formally understand what the “total counterfactual effect” is well enough to do that. So I’ll simulate instead.\n\n# Sequence in log units\nM_seq &lt;- seq(from = -3, to = 5, by = 0.01)\n\n# Standardize with original values to model units\nM_seq &lt;- (M_seq - mean(log(milk$mass), na.rm = TRUE)) /\n    sd(log(milk$mass), na.rm = TRUE)\n\n# Simulate the predictions\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h3, data = sim_dat, vars = c(\"N\", \"K\"))\n\nplot_data &lt;-\n    tibble::tibble(\n        M = exp(sim_dat$M * attr(m$M, \"scaled:scale\") +\n            attr(m$M, \"scaled:center\")),\n        K = colMeans(s$K) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n    )\nplot_PI &lt;- apply(s$K, 2, PI) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n\n# Plot the counterfactual effect\nplot(\n    plot_data$M, plot_data$K,\n    type = \"l\",\n    xlab = \"manipulated mass (kg)\", ylab = \"counterfactual kilocalories per gram of milk\",\n    xlim = exp(c(-3, 5)),\n    ylim = c(0.1, 1.1)\n)\nshade(plot_PI, plot_data$M)\nmtext(\"Total counterfactual effect of M on K\")\n\n\n\n\n\n\n\n\nAgain, the question didn’t say what number to double, but you can get any of them from a simulation like this.\n\n\n5H4\nThis is an open-ended problem where we will consider how to add the indicator of being in the South to the marriage problem. There are a few possibilities for the causal implications of this.\n\nSomething about Southerness directly affects age at marriage, marriage rate, and divorce rate all at the same time. (Or divorce rate, and one of the two predictors) That is, Southerness has both direct and indirect effects on divorce rate.\nSomething about Souttherness directly affects age at marriage and marriage rate, having only indirect effects on divorce rate. Alternatively, Southerness only impacts one of these two variables.\nSomething about Southernness directly affects divorce rate, with no indirect effects.\n\nMy first instinct is to say that age at marriage and marriage rate are primarily influenced by socioeconomic factors. Due to multiple historical factors (including slavery and an enduring legacy of systemic racism), the southern US, on average, has lower education rates and higher poverty rates as a region. Increased socioeconomic status tends to be associated with higher age of marriage, but a higher marriage rate and lower divorce rate (according to what I read while googling this).\nHowever, the southern U.S. also has a unique subculture (which varies widely across regions of the south), which could be a sociological cause of differences in some of these variables. For example, I think it is reasonable to say that traditional Southern culture encourages women to marry young, and also encourages women to get married in general – there is definitely a stereotype about older, unmarried women in traditional Southern culture.\nSo, based on both the socioeconomic reasons and the “culture” argument, there are two models I would like to examine. First, the model that posits a direct effect of Southerness on divorce rate, as well as indirect effects on both age at marriage and marriage rate. Then, I’d also like to examine the model where there is no direct effect of Southerness on divorce rate, and Southerness acts on divorce rate via age at marriage and marriage rate.\nLet’s examine the model with no direct effects first. We’ll consider the conditional independencies of this DAG.\n\ndag_indirect &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D}\")\nplot(dag_indirect)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_indirect)\n\nD _||_ S | A, M\n\n\nSo, if the data are consistent with this DAG, then the divorce rate should be independent of being in the South after we control for age at marriage and marriage rate. So let’s fit a model that does that. Since Southern status is an indicator variable, I wasn’t quite sure how to handle it in this model. In a frequentist framework, I would probably want to include an interaction term – this was briefly mentioned at the beginning of this chapter, but has not been covered in detail so I’ll just use a simple additive-effects-only model for now.\nBut first let’s consider the conditional independencies of the DAG with direct effects.\n\ndag_direct &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D; S -&gt; D}\")\nplot(dag_direct)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_direct)\n\nThere are no conditional independencies for this graph! So we should expect to see a relationship for all three parameters in the model that conditions on all three of our predictor variables.\nFortunately for us, we can use the same model to evaluate both of these DAGs – we just need to see which of the conditional independencies are supported by the result of predicting \\(D\\) using all three of them.\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        D = standardize(Divorce),\n        M = standardize(Marriage),\n        A = standardize(MedianAgeMarriage),\n        S = South\n    )\n\nm_s_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bS * S,\n            a ~ dnorm(0, 0.2),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_m_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_a_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_all &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A + bS * S,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_all)\n\n             mean         sd         5.5%      94.5%\na     -0.07590505 0.10600330 -0.245318792  0.0935087\nbM    -0.04224048 0.14778943 -0.278436533  0.1939556\nbA    -0.56163739 0.15090185 -0.802807694 -0.3204671\nbS     0.34998019 0.21572095  0.005216455  0.6947439\nsigma  0.76290395 0.07580295  0.641756198  0.8840517\n\ncoeftab_plot(\n    coeftab(\n        m_a_only, m_s_only, m_m_only, m_all\n    ),\n    pars = c(\"bA\", \"bS\", \"bM\")\n)\n\n\n\n\n\n\n\n\nSo, interestingly, from the estimated coefficients, we see that the coefficient for being in the South does not change that much when we control for both of the other variables. Of course, the CI crosses zero in the new model, so if we were doing bad statistics we would say that the effect has disappeared, but it was significant in the S-only model, which means that there is no direct effect of S. Since we are not doing bad statistics though, it seems unreasonable to claim that – there appears to be an effect of both \\(A\\) and \\(S\\) in the final model.\nRecall our previous model which was consistent with the idea that marriage rate only impacts divorce rate through the effect of age of marriage. So another potential DAG is like this.\n\ndag_med &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A -&gt; D; M -&gt; A -&gt;; S -&gt; D}\")\nplot(dag_med)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_med)\n\nD _||_ M | A, S\n\n\nSo the only casual independency of this DAG is whether D is independent of M after controlling for A and S, and after fitting our previous model we see that the data are consistent with this DAG. Arguably the data are not consistent with the other DAGs since those DAGs do not have any conditional independencies involving M, but the best model should still be chosen by a domain expert, not based on what the data are consistent with after observing the data."
  },
  {
    "objectID": "sr/cp6.html",
    "href": "sr/cp6.html",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "",
    "text": "This chapter discusses three common pitfalls that can lead our statistical models to misbehave and make our causal interpretations difficult or incorrect. The three major topics are collider bias (selection-distortion), multicollinearity in regression models, and post-treatment bias. The chapter further expands on the idea of DAGs as graphical causal models that was introduced in the previous chapter."
  },
  {
    "objectID": "sr/cp6.html#chapter-notes",
    "href": "sr/cp6.html#chapter-notes",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nThe Selection-distortion effect (AKA Berkson’s/Berksonian bias, generalized to the idea of collider bias) occurs when the selection of a sample changes the relationship between the observed variables. (I.e. there is/isn’t a relationship between the two variables on the sample, but in the larger population, there isn’t/is a relationship.) Berkson’s bias refers to the particular effect that when selecting from a population on two desirable traits, there often appears to be a negative correlation between the desirable traits in the selected sample.\nMulticollinearity refers to a very strong association between two or more predictor variables, conditional on the other variables in the model. When variables are multicollinear, the posterior distribution will seem to suggest that none of the multicollinear variables are truly associated with the outcome, even if the reality is that they are all strongly associated.\nPost-treatment bias: a form of included variable bias where variables that are also causal descendents of the treatment, are controlled for when assessing the response. That is, you measure something that is not the outcome of interest and is also affected by the treatment, and you adjust for that quantity when analyzing the outcome. This induces collider bias.\nControlling for a collider on a DAG induces D-separation, meaning that the DAG is no longer connected.\nWhen you condition on a collider (a common descendent), it creates statistical, although not necessarily causal, relationships between the ancestors.\nEven unmeasured causes can induce collider bias. Selection bias in a study can often be interpreted as conditioning on a collider during the sampling process. See the parents and grandparents example in section 6.3.2.\nThere are four types of elemental confounds: DAG structures that allow us to determine causal and non-causal pathways.\n\nThe fork: two variables have a common cause (Z -&gt; X; Z -&gt; Y).\nThe pipe: one variable is intermediate in the causal relationship between two others (X -&gt; Z -&gt; Y).\nThe collider: two variables have a common descenent (X -&gt; Z; Y -&gt; Z).\nThe descendant: a variable which descends from another, capturing part of the ancestor’s statistical signal (in the previous example, if we also have Z -&gt; D, D will appear to be a collider as well, even if it is just a descendant).\n\nEvery DAG is built out of these four types of relationships, and we can use specific rules for DAGs to determine what variables need to be included in models for a causal effect.\n“Multiple regression is no oracle, but only a golem.”"
  },
  {
    "objectID": "sr/cp6.html#exercises",
    "href": "sr/cp6.html#exercises",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "Exercises",
    "text": "Exercises\n\n6E1\nThree mechanisms that can produce false inferences about causal effects in a multiple regression model are: multicollinearity, the selection- distortion effect, and post-treatment bias.\n\n\n6E2\nFor an example of post-treatment bias, consider a vaccine efficacy trial for influenza (or I guess any disease). Suppose we have a known surrogate of protection, an immunological measurement that is strongly associated with protection from the disease. For influenza, one potential biomarker is hemagglutinin inhibition (HI) titer, which is typically measured before and after (around 21 to 28 days) vaccination. If one did a challenge study or a long followup period of surveillance, we can record which individuals are ultimately infected with influenza. Including participants’ HI titers before vaccination when modeling vaccine protection is OK depending on the context, but including participants HI titer after vaccination would induce post-treatment bias, because vaccination directly affects HI titer.\n\n\n6E3\n\nThe fork (Z -&gt; X; Z -&gt; Y): X ⫫ Y | Z\nThe pipe (X -&gt; Z -&gt; Y): X ⫫ Y | Z\nThe collider (X -&gt; Z; Y -&gt; Z): X ⫫ Y; X !⫫ Y | Z\nThe descendent: conditional independencies are the same as the parent\n\n\n\n6E4\nSuppose we have two variables (call one the exposure and the other the outcome) which are both causes of a third variable. If that third variable determines which observations we observe (for example, a restaurant existing or a patient agreeing to participate in a study), in our observed sample we will see a correlation between the exposure and the outcome, just because we are only seeing observations where the third variable is already specified.\nIn the funded grants example, a funded grant must be high in at least one of newsworthiness or trustworthiness, otherwise it will not be funded. If we could see all grants, we would not see a correlation between newsworthiness and trustworthiness. But when we only look at funded grants, we condition on a common descendant of both variables (a collider), which makes a spurious relationship appear."
  },
  {
    "objectID": "sr/cp6.html#m1",
    "href": "sr/cp6.html#m1",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M1",
    "text": "6M1\nThe new DAG including \\(V\\) as an unobserved common cause of \\(C\\) and \\(Y\\) looks like this.\n\ndag_6m1 &lt;- dagitty::dagitty(\n    \"dag {\n        U [unobserved]\n        V [unobserved]\n        X -&gt; Y\n        X &lt;- U &lt;- A -&gt; C -&gt; Y\n        U -&gt; B &lt;- C\n        C &lt;- V -&gt; Y\n    }\"\n)\ndagitty::coordinates(dag_6m1) &lt;-\n    list(\n        x = c(U = 1, V = 4, X = 1, Y = 3, A = 2, B = 2, C = 3),\n        y = c(U = 1.5, V = 2.5, X = 3, Y = 3, A = 1, B = 2, C = 1.5)\n    )\ndagitty::exposures(dag_6m1) &lt;- \"X\"\ndagitty::outcomes(dag_6m1) &lt;- \"Y\"\nplot(dag_6m1)\n\n\n\n\n\n\n\n\nWe still have all of the same paths from the previous example, i.e.:\n\n\\(X \\to Y\\),\n\\(X \\leftarrow U \\leftarrow A \\to C \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\to Y\\).\n\nThe first path is the direct path. The second is an open backdoor path through \\(A\\). The third is a closed backdoor path, as it passes through the collider \\(B\\). By adding the unobserved confounder \\(V\\), we create two new backdoor paths,\n\n\\(X \\leftarrow U \\leftarrow A \\to C \\leftarrow V \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\leftarrow V \\to Y\\).\n\nThe first path is an open backdoor path, and the second path is a closed backdoor path. We can check which paths exist and are open with dagitty.\n\ndagitty::paths(dag_6m1)\n\n$paths\n[1] \"X -&gt; Y\"                     \"X &lt;- U -&gt; B &lt;- C -&gt; Y\"     \n[3] \"X &lt;- U -&gt; B &lt;- C &lt;- V -&gt; Y\" \"X &lt;- U &lt;- A -&gt; C -&gt; Y\"     \n[5] \"X &lt;- U &lt;- A -&gt; C &lt;- V -&gt; Y\"\n\n$open\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\nNow we need to close the two open paths, without opening either of the closed paths. Conditioning on \\(C\\) would close both of the open paths, but would also open the fifth path. However, conditioning on \\(A\\) will close both open paths without opening either of the closed paths. So \\(\\{A\\}\\) is our sufficient adjustment set. We can verify this with dagitty.\n\ndagitty::adjustmentSets(dag_6m1)\n\n{ A }"
  },
  {
    "objectID": "sr/cp6.html#m2",
    "href": "sr/cp6.html#m2",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M2",
    "text": "6M2\nFirst we’ll do the simulation: we want \\(X\\) and \\(Z\\) to be highly correlated.\n\nset.seed(101)\nX &lt;- rnorm(100, 0, 1)\nZ &lt;- rnorm(100, X, 0.5)\nY &lt;- rnorm(100, Z, 1)\n\ncor(cbind(X, Z, Y))\n\n          X         Z         Y\nX 1.0000000 0.8925919 0.6779960\nZ 0.8925919 1.0000000 0.7998096\nY 0.6779960 0.7998096 1.0000000\n\n\nWe can see that all of the variables are strongly associated, but \\(X\\) and \\(Z\\) have a particularly strong correlation. But now we want to use a model that adjusts for both.\n\nfit_6m1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            Y ~ dnorm(mu, sigma),\n            mu &lt;- a + bX * X + bZ * Z,\n            a ~ dnorm(0, 0.5),\n            c(bX, bZ) ~ dnorm(0, 1),\n            sigma ~ dexp(1)\n        ),\n        data = list(X = X, Y = Y, Z = Z)\n    )\n\ncoeftab(fit_6m1) |&gt;\n    coeftab_plot(pars = c(\"bX\", \"bZ\"))\n\n\n\n\n\n\n\n\nInterestingly, we can see that we do not get the same problem as the previous multicollinearity example. The confidence intervals appear to be reasonable, and we see a strong effect of \\(Z\\) but no effect of \\(Y\\). Intuitively, this make sense – \\(Z\\) and \\(Y\\) have a stronger correlation than \\(X\\) and \\(Y\\), so after we control for \\(Z\\), the model “finds” all of the signal, and then does not find an effect of \\(X\\). So if we interpreted this model without considering the causal framework, we would still be mislead by the multicollinearity, but there is nothing obviously wrong – the entire causal effect of \\(X\\) on \\(Z\\) is through \\(Z\\), so this estimate of the direct causal effect of \\(X\\) makes sense."
  },
  {
    "objectID": "sr/cp6.html#m3",
    "href": "sr/cp6.html#m3",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M3",
    "text": "6M3\nHere are the adjustment sets for each of the DAGs shown.\n\nTop left DAG: Z only\nTop right: nothing\nBottom left: nothing\nBottom right: A only\n\nI also checked using dagitty to verify my answers.\n\ndag1 &lt;- dagitty::dagitty(\"dag {Z -&gt; X -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag2 &lt;- dagitty::dagitty(\"dag {X -&gt; Z -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag3 &lt;- dagitty::dagitty(\"dag {X -&gt; Y -&gt; Z; A -&gt; X -&gt; Z; A -&gt; Z}\")\ndag4 &lt;- dagitty::dagitty(\"dag {A -&gt; X -&gt; Z; A -&gt; Z -&gt; Y; X -&gt; Y}\")\n\nlapply(list(dag1, dag2, dag3, dag4), dagitty::adjustmentSets, \"X\", \"Y\")\n\n[[1]]\n{ Z }\n\n[[2]]\n {}\n\n[[3]]\n {}\n\n[[4]]\n{ A }"
  },
  {
    "objectID": "sr/cp6.html#h1",
    "href": "sr/cp6.html#h1",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H1",
    "text": "6H1\nUsing the Waffle House data, we want to find the total causal influence of number of Waffle Houses on divorce rate. First, let’s look at what we have to work with.\n\ndata(\"WaffleDivorce\")\nhead(WaffleDivorce)\n\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\n\n\nOK, so of course we will assume that there is a direct causal effect of number of Waffle Houses (\\(W\\)) on divorce rate (\\(D\\)). From previous work, we know our data are consistent with a \\(M \\to A \\to D\\) DAG structure, for \\(M\\) the marriage rate and \\(A\\) the median age at marriage, so we’ll incorporate this into our DAG. We also saw that the data are consistent with being in the South affecting \\(M\\) and \\(A\\), so we’ll include that in our DAG, and of course we expect to see \\(S \\to W\\). Finally, since we’re trying to find the total causal effect of \\(W\\), we’ll include \\(A \\leftarrow W \\rightarrow M\\) as a sub-DAG as well. Putting it all together, our DAG looks like this.\n\ndag_6h1 &lt;-\n    dagitty::dagitty(\n        \"dag {\n        M -&gt; A -&gt; D\n        M &lt;- S -&gt; A\n        S -&gt; W\n        W -&gt; D\n        M &lt;- W -&gt; A\n        }\"\n    )\ndagitty::exposures(dag_6h1) &lt;- \"W\"\ndagitty::outcomes(dag_6h1) &lt;- \"D\"\ndagitty::coordinates(dag_6h1) &lt;-\n        list(\n        x = c(S = 1, W = 2, D = 2.7, M = 1, A = 2),\n        y = c(S = 1, W = 1, D = 1.5, M = 2, A = 2)\n    )\n\nplot(dag_6h1)\n\n\n\n\n\n\n\n\nLet’s now figure out what needs to be in our model.\n\ndagitty::adjustmentSets(dag_6h1)\n\n{ S }\n\n\nWe see that to get the total cause effect of \\(W\\) on \\(D\\), we need only adjust for \\(S\\), being in the South. I should probably worry about things like transformations and zero-inflation, but for this exercise I am not going to do that.\n\nwd &lt;- \n    list(\n        W = standardize(WaffleDivorce$WaffleHouses),\n        D = standardize(WaffleDivorce$Divorce),\n        S = WaffleDivorce$South + 1\n    )\n\nNow we’ll fit the model. I’ll allow the intercept to be different for Southern and non-Southern states, but because we’re interested in the total causal effect of Waffle House Numbers, I’ll force the effect to be the same across both groups.\n\nset.seed(100)\nfit_6h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a[S] + b * W,\n            a[S] ~ dnorm(0, 1),\n            b ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd\n    )\n\nrethinking::precis(fit_6h1, depth = 2)\n\n             mean         sd        5.5%     94.5%\na[1]  -0.18668305 0.16832424 -0.45569771 0.0823316\na[2]   0.46329565 0.29995319 -0.01608748 0.9426788\nb      0.05197162 0.17641807 -0.22997853 0.3339218\nsigma  0.92058385 0.09085178  0.77538516 1.0657825\n\n\nOK, I’m getting a warning but I don’t think it’s doing anything. So I’ll just ignore it. For this model, we can see that there is a strongly positive effect of \\(b\\). Let’s look at the posterior distribution.\n\npost_6h1 &lt;- extract.samples(fit_6h1)\ndens(post_6h1$b)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that almost all of the posterior density is above zero, indicating that there is a positive effect of Waffle Houses on divorce rate. For every 1 standard deviation increase in the number of Waffle Houses in a state, the divorce rate is expected to increase by about 0.25 units. Let’s put that back from standardized units into real units.\n\n1 * attr(wd$W, \"scaled:scale\") + attr(wd$W, \"scaled:center\")\n\n[1] 98.12959\n\n0.25 * attr(wd$D, \"scaled:scale\") + attr(wd$D, \"scaled:center\")\n\n[1] 10.1432\n\n\nSo we see that we expect the divorce rate to increase by about \\(10\\%\\) for every 98 additional Waffle Houses, or approximately \\(0.1\\%\\) per Waffle House. This is the total causal effect based on our DAG, even though I would guess that the direct effect is zero and this entire effect is through location.\n\n6H2\nFirst, we need to check the implied causal independencies of the DAG.\n\ndagitty::impliedConditionalIndependencies(dag_6h1)\n\nD _||_ M | A, W\nD _||_ S | A, W\n\n\nTest one: \\(D\\) and \\(M\\) should be independent after adjusting for \\(A\\) and \\(W\\).\n\nset.seed(100)\nwd2 &lt;- c(\n    wd,\n    M = list(standardize(WaffleDivorce$Marriage)),\n    A = list(standardize(WaffleDivorce$MedianAgeMarriage))\n)\n\nfit_6h2_a1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bm, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_a2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M,\n            a ~ dnorm(0, 1),\n            c(bm) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_a1,\n    fit_6h1_a2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bm\"))\n\n\n\n\n\n\n\n\nYes, we can see that if we only include \\(M\\) (fit_6h1_a2), we see an effect, but if we control for \\(A\\) and \\(W\\), as in fit_6h1_a1, we do not.\nNow let’s check the second test: \\(D\\) and \\(S\\) should be independent if we control for \\(A\\) and \\(W\\).\n\nset.seed(100)\nfit_6h2_b1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bs, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_b2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S,\n            a ~ dnorm(0, 1),\n            c(bs) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_b1,\n    fit_6h1_b2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bs\"))\n\n\n\n\n\n\n\n\nWe see exactly the same interpretation here: when only \\(S\\) is in the model, all of the posterior density is above 0, but when we control for \\(A\\) and \\(W\\), a significant portion is below zero and the mean is lower. So I think we can say that our data appear to be consistent with the conditional independencies that our DAG implies.\nWe can also do these tests automatically used the method recommended by dagitty. I don’t know that much about these results, but they agree with our modeling results, which is good!\n\ndagitty::localTests(\n    dag_6h1,\n    sample.cov = lavaan::lavCor(as.data.frame(wd2)),\n    sample.nobs = nrow(as.data.frame(wd2))\n)\n\n                  estimate   p.value       2.5%     97.5%\nD _||_ M | A, W -0.0855530 0.5650794 -0.3613429 0.2035528\nD _||_ S | A, W  0.1349902 0.3622409 -0.1550991 0.4044034"
  },
  {
    "objectID": "sr/cp6.html#h3",
    "href": "sr/cp6.html#h3",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H3",
    "text": "6H3\nNow we are back to the fox problems. I’ll reproduce the DAG first just so I have it in my notes.\n\nfox_dag &lt;-\n    dagitty::dagitty(\n        \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n        }\"\n    )\ndagitty::coordinates(fox_dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\nplot(fox_dag)\n\n\n\n\n\n\n\n\nFirst, we want to infer the total causal influence of area (A) on weight (W). I’ll go ahead and set up the data, and standardize all the variables as McElreath recommends. I’ll also model the log weight instead of the raw weight so we can ensure that our predictions remain positive.\n\ndata(foxes)\nf2 &lt;-\n    foxes |&gt;\n    dplyr::transmute(\n        A = area,\n        F = avgfood,\n        G = groupsize,\n        W = log(weight)\n    ) |&gt;\n    as.list() |&gt;\n    lapply(FUN = rethinking::standardize)\n\nNow we’ll adopt a model for the weight. Since it is logged and standardized we’ll use a Gaussian likelihood function. To set priors, we first need to determine our adjustment set. There are two causal paths between \\(A\\) and \\(W\\):\n\n\\(A \\to F \\to W\\) and\n\\(A \\to F \\to G \\to W\\).\n\nIf we want the total causal effect of \\(A\\) on \\(W\\), there are no closed paths and we do not need to adjust for any other variables. We can confirm this with dagitty.\n\ndagitty::adjustmentSets(\n    fox_dag,\n    exposure = \"A\",\n    outcome = \"W\",\n    effect = \"total\"\n)\n\n {}\n\n\nWe get the empty set as our minimal sufficient adjustment set. So we’ll adopt the following model.\n\\[\n\\begin{align*}\n\\mathrm{Scale}\\left(\\log W \\right) &\\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\mathrm{Scale}(A)\\\\\n\\alpha &\\sim N(4.5, 0.5) \\\\\n\\beta &\\sim N(0, 0.25) \\\\\n\\sigma &\\sim \\mathrm{Exp}(10)\n\\end{align*}\n\\]\nI tuned the parameters for the prior distributions using a prior predictive simulation to ensure that the prior predictions stay within the expected outcome space. As usual, I think that making this model have an intercept of zero potentially makes more sense (a fox with no area should starve) and then we would not expect this relationship to be linear, but we will just mess with the priors until the predictions look reasonable.\n\nset.seed(101)\na &lt;- rnorm(1000, 4.5, 0.5)\nb &lt;- rnorm(1000, 0, 0.25)\nsigma &lt;- rexp(1000, 10)\n\nplot(\n    NULL,\n    xlim = c(1, 5.25),\n    ylim = c(1.5, 7.5),\n    xlab = \"Area\",\n    ylab = \"Weight\",\n    main = \"Prior predictive simulation\"\n)\n\nx &lt;- seq(min(f2$A), max(f2$A), length.out = 1000)\nxp &lt;- x * attr(f2$A, \"scaled:scale\") + attr(f2$A, \"scaled:center\")\nout &lt;- vector(length = 1000, mode = \"list\")\nfor (i in 1:1000) {\n    # Sample y's from their distribution\n    y &lt;- rnorm(1000, a[i] + b[i] * x, sigma[i])\n    \n    # Backtransform to original scale\n    yp &lt;- exp(y * attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))\n    out[[i]] &lt;- yp\n    \n    lines(x = xp, y = y, type = \"l\", col = rethinking::col.alpha(\"black\", 0.05))\n}\n\n\n\n\n\n\n\n\nI think that looks fine but I have to admit that I find assigning reasonable priors to standardized data quite difficult, it feels like just randomly picked numbers under the lines look ok. Now we can finally fit our regression and get the estimated causal effect.\n\nm_6h3 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * A,\n        a ~ dnorm(4.5, 0.5),\n        b ~ dnorm(0, 0.25),\n        sigma ~ dexp(10)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h3)\n\n            mean         sd         5.5%     94.5%\na     0.14011603 0.08988426 -0.003536369 0.2837684\nb     0.03384817 0.08470967 -0.101534245 0.1692306\nsigma 0.96550793 0.06089243  0.868190063 1.0628258\n\n\nLet’s look at the posterior distribution of \\(\\beta\\).\n\npost &lt;- extract.samples(m_6h3)\ndens(post$b, lwd = 2)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that while the density of \\(\\beta\\) is more than 50% above zero, there is a substantial amount of the density on either side of zero. So in general, it seems that area size is not directly correlated to fox weight. If anything, there is a small positive effect, but it is not very strong. We can get predictions on the original scale as well.\n\nmodel_out &lt;- rethinking::sim(m_6h3, data = list(A = x), n = 10000)\nmodel_mu &lt;-\n    colMeans(model_out) |&gt;\n    (\\(x) x *attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))() |&gt;\n    exp()\n\nmodel_pi &lt;- apply(model_out, 2, \\(x) exp(rethinking::PI(x) * \n                                        attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\")))\n\nplot(\n    xp, model_mu,\n    type = \"l\",\n    xlim = range(xp),\n    ylim = range(foxes$weight),\n    xlab = \"Simulated area\",\n    ylab = \"Counterfactual weight\"\n)\nlines(xp, model_pi[1, ], lty = 2)\nlines(xp, model_pi[2, ], lty = 2)\n\n\n\n\n\n\n\n\nAs we would expect from the model estimates, there is a slight positive trend with incredibly wide credible intervals."
  },
  {
    "objectID": "sr/cp6.html#h4",
    "href": "sr/cp6.html#h4",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H4",
    "text": "6H4\nNext we want to infer the total causal effect of adding food. Since we want to know the total causal effect, we don’t need to adjust for anything else in the model, since \\(G\\) is a mediator on the causal path. This time I’ll just use standard priors since it doesn’t really matter that much.\n\nm_6h4 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * `F`,\n        a ~ dnorm(0, 1),\n        b ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h4)\n\n               mean         sd       5.5%     94.5%\na      9.224085e-10 0.09165572 -0.1464835 0.1464835\nb     -1.533812e-02 0.09205000 -0.1624518 0.1317756\nsigma  9.913351e-01 0.06467093  0.8879784 1.0946917\n\n\nOK, we see a similar thing here. There is a possibly a slight negative relationship, but it looks like there is really no relationship here."
  },
  {
    "objectID": "sr/cp6.html#h5",
    "href": "sr/cp6.html#h5",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H5",
    "text": "6H5\nNow we want to get the total causal effect of group size, \\(G\\). Now we have to also control for food, \\(F\\), because it is a confounder on one of the paths from \\(G \\to W\\).\n\nm_6h5 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * `F` + bG * G,\n        a ~ dnorm(0, 1),\n        bF ~ dnorm(0, 1),\n        bG ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h5)\n\n               mean         sd       5.5%      94.5%\na      1.467513e-05 0.08752970 -0.1398747  0.1399040\nbF     5.602422e-01 0.19672784  0.2458332  0.8746513\nbG    -6.435327e-01 0.19672896 -0.9579436 -0.3291218\nsigma  9.463559e-01 0.06178686  0.8476086  1.0451032\n\n\nOk, interestingly we can now see a strong positive effect of \\(F\\) and a strong negative effect of \\(G\\). If we plot the data stratified by group size, we can understand this effect a bit better. This is an example of a masked relationship. Overall, average food appears to have a negative effect on weight, which doesn’t seem to make sense.\nHowever, for groups of a given size, the more food there is, the heavier those foxes tend to be. But for healthier (heavier) foxes, it is likely that more new foxes are born, and despite the fact that the foxes are in a more abundant area, there is less food per fox. So within a group size, having more food is good. But if the group size expands without a simultaneous increase in food supply, there will be less food available for each fox. However, note that for the high group sizes, only one group was observed in each. So perhaps our estimates contain selection bias, if the groups that were recorded are not typical examples of foxes with high group sizes.\n\nggplot(foxes, aes(x = avgfood, y = weight, col = factor(groupsize))) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\", alpha = 0.5) +\n    hgp::theme_ms() +\n    geom_smooth(method = \"lm\", aes(group = 1, color = \"overall\")) +\n    scale_color_manual(\n        values = c(viridisLite::viridis(7), \"black\")\n    ) +\n    labs(x = \"Average food\", y = \"Weight\", col = \"Group size\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "sr/cp6.html#h6-and-6h7",
    "href": "sr/cp6.html#h6-and-6h7",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H6 and 6H7",
    "text": "6H6 and 6H7\nI decided to skip the open research questions."
  },
  {
    "objectID": "sr/cp8.html",
    "href": "sr/cp8.html",
    "title": "Chapter 8: Conditional Manatees",
    "section": "",
    "text": "library(rethinking)\nlibrary(dagitty)\nlibrary(ggplot2)\nThis chapter is a brief introduction to the concept of conditional inference, focusing on the specific concept of linear interaction in models."
  },
  {
    "objectID": "sr/cp8.html#chapter-notes",
    "href": "sr/cp8.html#chapter-notes",
    "title": "Chapter 8: Conditional Manatees",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nBullet holes in bombers and propeller scars on manatees – both are conditional on survival. This is the motivating example for the chapter.\nAn interaction is a statistical method for modeling interdependence between two features of a model.\nUsing an interaction term in a model is nearly always better than fitting stratified models.\nIn Bayesian models, it’s better to use an index-coding approach and have parameters vary by the level of a categorical variable, rather than using an indicator-coding approach, which makes assigning priors difficult.\nAn interaction is also just a slope which is conditional on another effect – the value of one variable modifies the effect of the other.\nLinear interactions are symmetrical. If variable \\(x\\) interacts with variable \\(y\\), then \\(y\\) interacts with \\(x\\). “There is just no way to specify a simple, linear interaction in which you can say the effect of some variable \\(x\\) depends on \\(z\\), but the effect of \\(z\\) does not depend upon \\(x\\).”\nContinuous interactions are harder to think about as conditional slopes, because we would need an uncountably infinite number of categories. Instead, we can think about interactions as nested linear models.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\beta_S S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i\n\\end{aligned}\n\\]\n\nWe could include nested terms for both variables, but the resultant model has unidentifiable parameters – in the final term below, only the sum \\((\\beta_{WS} + \\beta_{SW})\\) can be estimated.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\gamma_{S,i} S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i \\\\\n\\gamma_{S_i} &= \\beta_{S} + \\beta_{SW} W_i \\\\\n\\therefore \\mu_i &= \\alpha + \\left(\\beta_{W} + \\beta_{WS} S_i\\right)W_i + \\left(\\beta_{S} + \\beta_{SW} W_i\\right) S_i \\\\\n&= \\alpha + \\beta_W W_i + \\beta_S S_i + (\\beta_{WS} + \\beta_{SW}) W_iS_i\n\\end{aligned}\n\\]\n\nThe best way to understand interactions is to plot the predictions at multiple levels of the interacting variables."
  },
  {
    "objectID": "sr/cp8.html#exercises",
    "href": "sr/cp8.html#exercises",
    "title": "Chapter 8: Conditional Manatees",
    "section": "Exercises",
    "text": "Exercises\n\n8E1\n\nBread dough rises because of yeast and temperature. Yeast amount and temperature interact to determine how much the bread dough rises.\nEducation and parents’ education could interact to determine higher income. While people with more education have higher salaries on average, people who are educated and also have educated parents are likely to have even higher average salaries at the same level of individual education.\nGasoline and pressing the accelerator make the car go. If you never press the accelerator, a full tank won’t do anything.\n\n\n\n8E2\nOnly statement one (Caramelizing onions requires cooking over a low heat and making sure the onions don’t dry out) involves an interaction. Both things must be true simultaneously, whereas the effects are independent of each other in the other statements.\n\n\n8E3\nOf course all of these models only make sense if we have a correct way to quantify those variables.\n\n\\(\\text{onion caramelization} = \\alpha + \\beta_1 \\cdot \\text{temperature} + \\beta_2 \\cdot \\text{moisture} + \\gamma_{12} \\cdot \\text{temperature} \\cdot \\text{moisture}\\)\n\\(\\text{car speed} = \\alpha + \\beta_2 \\cdot \\text{number of cylinders} + \\beta_2 \\cdot \\text{fuel injector quality}\\)\n\\(\\text{political beliefs} = \\alpha + \\beta_1 \\cdot \\text{parental beliefs} + \\beta_2 \\cdot \\text{friend beliefs}\\)\n\\(\\text{intelligence} = \\alpha + \\beta_1 \\cdot \\text{sociality} + \\beta_2 \\cdot \\text{manipulable appendages}.\\)\n\n\n\n8M1\nIn the tulips example, we saw that water and shade levels interact to affect tulip blooms. Tulips need both water and shade to produce blooms; at a low-light level, the effect of water decreases because no amount of water can replace the lost light. Similarly, if plants have no water, an adequate amount of sunlight will not produce blooms and might even become harmful.\nIf the hot temperature prevents blooms all together, then the hot temperature would modify the effect of shade, water, and their interaction to all become zero – no amount of shade or water can allow for blooms, and their interaction does not help in this context either.\n\n\n8M2\nThe linear model for the tulips example without heat was \\[\n\\mu_i = \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i.\n\\]\nWe can make all of those terms dependent on \\(H_i\\), the heat treatment, in order to accomplish this.\n\\[\n\\mu_i = \\alpha_{H[i]} + \\beta^{W}_{H[i]} + \\beta^S_{H[i]} + \\gamma^{SW}_{H[i]} S_iW_i.\n\\]\nNow it is possible for these effects to all be zero (or much smaller) if \\(H[i] = 1\\), and have their normal values if \\(H[i] = 0\\). Another way to write this model could be something like \\[\n\\begin{aligned}\n\\mu_i &= \\lambda_i (1 - H_i) \\\\\n\\lambda_i &= \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i\n\\end{aligned}\n\\] where \\(H_i\\) again takes on values of \\(0\\) (cold) and \\(1\\) (hot).\n\n\n8M3\nWe cannot create a data set where the raven population and wolf population have a linear statistical interaction, because a linear statistical interaction has at least two predictors. Here we only have an outcome (the raven population size) and a predictor (the wolf population size). This is more of an example of a differential equations type problem than a statistical interaction. In this model, the raven population size would have to vary with the wolf population size, and we do not know about the functional form of this effect, so an appropriate model would be something like\n\\[\n\\frac{dR}{dt} = f\\left(W(t)\\right),\n\\] where \\(f\\) is a function that takes the wolf population size at time \\(t\\) as an input, and returns the change in the raven population before the next time point.\n\n\n8M4\nWe’ll use the sample model for the tulip blooms without heat from the earlier exercise. The priors used in the chapter were\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Normal}(0, 0.25) \\\\\n\\beta_S &\\sim \\text{Normal}(0, 0.25) \\\\\n\\gamma_{SW} &\\sim \\text{Normal}(0, 0.25) \\\\\n\\end{aligned}\n\\]\nWe want to use new priors that constrain the effect of water to be positive and the effect of shade to be negative. At this point in the book, the distribution we learned about that has to be positive is lognormal, and we can force the effect of shade to be negative by taking the additive inverse of a lognormal prior. Since we know that having more water increases the effect of light (because if a tulip has plenty of water, getting enough sunshine is the new limiting factor on the blooms), we know that having more water should decrease the effect of shade, so we’ll make the interaction negative as well. Lognormal priors can be hard to calibrate, so we’ll adjust the parameters until the prior predictive simulation looks nice. The priors we’ll use are as follows.\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1)\n\\end{aligned}\n\\]\n\nset.seed(370)\n\n# Load the data\ndata(tulips)\nd &lt;- tulips\nd$blooms_std &lt;- d$blooms / max(d$blooms)\nd$water_cent &lt;- d$water - mean(d$water)\nd$shade_cent &lt;- d$shade - mean(d$shade)\n\n# Fit the model and extract the prior samples\nm_8m4 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a + bw * water_cent - bs * shade_cent - bws * water_cent * shade_cent,\n        a ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nprior &lt;- rethinking::extract.prior(m_8m4)\n\n# Plot the prpd\npar(mfrow = c(1, 3))\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = rethinking::rangi2\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    mu &lt;- rethinking::link(\n        m_8m4,\n        data = data.frame(shade_cent = s, water_cent = -1:1),\n        post = prior\n    )\n    for (i in 1:20) lines(-1:1, mu[i, ], col = col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\nI did a few different simulations and ultimately ended up with the prior simulation shown here. The slope priors are regularizing and skeptical, so we think that a smaller effect is more likely a priori – if the effects are large, the data can demonstrate that for us.\n\n\n8H1\nNow we want to add the bed variable to the tulips example, which we’ll denote with \\(B_i\\). We only want to include the bed effect as a main effect, which means we need to have a different intercept for each bed – so our model will assume that each bed can start at a different baseline, but the effects of water, shade, and their interaction, are homogeneous across the beds. This is probably an OK assumption in the context of a controlled greenhouse setting.\nThe model will be as follows.\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal} \\left(\\mu_i, \\sigma\\right) \\\\\n\\mu_i &= \\alpha_{B[i]} + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha_{B[i]} &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{aligned}\n\\]\nThat is, we’ll use the same constrained, regularizing priors, as we did for the previous problem, but we’ll have a separate intercept for each bed. In reality, it would probably be good to have the intercept parameters be correlated as well, but we haven’t gone over that in the book yet. Let’s go ahead and fit the model.\n\nset.seed(370)\n# For the index coding to work, we need a numeric version of the beds.\nd$b &lt;- as.integer(d$bed)\n\n# Fit the model\nm_8h1 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a[b] + bw * water_cent - bs * shade_cent -\n            bws * water_cent * shade_cent,\n        a[b] ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_8h1, depth = 2)\n\n           mean         sd       5.5%     94.5%\na[1]  0.2733479 0.03603110 0.21576327 0.3309326\na[2]  0.3964371 0.03601250 0.33888214 0.4539920\na[3]  0.4091452 0.03601137 0.35159204 0.4666983\nbw    0.2017188 0.02612291 0.15996936 0.2434683\nbs    0.1039759 0.02652450 0.06158467 0.1463672\nbws   0.1312346 0.03272451 0.07893448 0.1835347\nsigma 0.1091610 0.01511214 0.08500884 0.1333131\n\n\nFrom the precis, we can see that the first bed (bed 1, with parameter a[1]) has a lower intercept than the other two beds – maybe this bed is next to a drafty space, is the first bed in the water connection and gets the pipe sludge, or just had less bloomds for some reason. But after we account for the different baselines between beds, the estimates of the parameter effects are similar to the last model, so this should just improve the accuracy of our model predictions for the first bed. We can plot the predictions to see.\n\n# Plot the prpd\npar(mfrow = c(1, 3))\n\ncols &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\")\n\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = cols[d$b[idx]]\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    \n    for (bd in 1:3) {\n        mu &lt;- rethinking::link(\n            m_8h1,\n            data = data.frame(shade_cent = s, water_cent = -1:1, b = bd)\n        )\n        for (i in 1:20) {\n            lines(\n                -1:1, mu[i, ],\n                col = col.alpha(cols[bd], 0.3)\n            )\n        }\n    }\n}\n\nlegend(\"topright\", c(\"Bed a\", \"Bed b\", \"Bed c\"), col = cols, lty = 1)\n\n\n\n\n\n\n\n\nWhile it’s actually quite difficult to make statistical conclusions without multiple replicates (here we have only one measurement per bed per treatment), we can see the clear difference between bed a and the other two beds in the model predictions. However, it also appears that our model predictions may not capture the true effect, as from the observed data it seems plausible that the effects of water and shade vary across beds. We would need actual replicates to be more certain of that, though.\n\n\n8H2\nNow we can compare the models with and without bed using WAIC.\n\nrethinking::compare(m_8m4, m_8h1)\n\n           WAIC        SE    dWAIC      dSE     pWAIC    weight\nm_8h1 -22.20890  9.935022 0.000000       NA 10.099017 0.7754177\nm_8m4 -19.73058 10.459586 2.478319 8.389834  7.390765 0.2245823\n\n\nWe see that the WAIC is smaller for the model with bed included, although the difference is small. This implies that adding the bed variable as a main effect increases the accuracy of our posterior predictions, although the improvement is not spectacular. As we saw by looking at summaries of the posterior distribution in the previous exercise, the difference in the estimated intercept for bed A vs. bed b and bed c, without any major changes in the estimates of the slope parameters, should account for this difference.\n\n\n8H3\nFor this question, we’ll focus on the ruggedness data.\n\ndata(\"rugged\")\n# Repeating data processing steps from the book\nd &lt;- rugged\nd$log_gdp &lt;- log(d$rgdppc_2000)\ndd &lt;- d[complete.cases(d$rgdppc_2000), ]\ndd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp)\ndd$rugged_std &lt;- dd$rugged / max(dd$rugged)\n\ndd$cid &lt;- ifelse(dd$cont_africa == 1, 1, 2)\n\nNow we need to recreate model m8.5 from the chapter. Well, at least that’s what the book says to do, but model m8.5 is about the tulips example, so we’ll recreate m8.3 instead.\n\nm8.3 &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dnorm(mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3, depth = 2)\n\n            mean          sd        5.5%       94.5%\na[1]   0.8865629 0.015675157  0.86151094  0.91161480\na[2]   1.0505698 0.009936261  1.03468975  1.06644988\nb[1]   0.1325055 0.074201996  0.01391637  0.25109461\nb[2]  -0.1425764 0.054747543 -0.23007353 -0.05507924\nsigma  0.1094903 0.005934777  0.10000535  0.11897519\n\n\nWe got similar results to what’s in the book, which is good. Now we want to examine the model with PSIS to determine if the Seychelles are influential on the estimation of parameters for the Africa group.\n\nset.seed(370)\nm8.3_psis &lt;- rethinking::PSIS(m8.3, pointwise = TRUE, n = 20000)\nrownames(m8.3_psis) &lt;- dd$isocode\npsis_sort &lt;- m8.3_psis[order(m8.3_psis$k, decreasing = TRUE), ]\npsis_sort |&gt; head()\n\n          PSIS       lppd    penalty  std_err         k\nLSO -1.1417140  0.5708570 0.31929940 15.27576 0.4467221\nSYC  1.3274852 -0.6637426 0.63043059 15.27576 0.3947676\nCHE  2.8274338 -1.4137169 0.46756727 15.27576 0.3148571\nTJK  0.4998540 -0.2499270 0.30731105 15.27576 0.2365403\nGNQ  3.3713051 -1.6856526 0.21035122 15.27576 0.2249513\nMUS  0.8394823 -0.4197411 0.08626176 15.27576 0.1688589\n\n\nThe most influential country on the model fit, judging by the Pareto \\(k\\) values, are Lesotho and the Seychelles, which are both highly rugged nations in Africa.\n\npar(mfrow = c(1, 1))\ndd_sorted &lt;- dd[order(m8.3_psis$k, decreasing = TRUE), ]\nplot(\n    dd_sorted$rugged_std, psis_sort$k,\n    xlab = \"Ruggedness as prop. of maximum\",\n    ylab = \"PSIS Pareto k value\"\n)\n\n\n\n\n\n\n\n\nWe can see that highly rugged nations have the largest Pareto \\(k\\) values, indicating that they are the most influential variables. We also know that these values have a high leverage in a linear regression model, so that makes sense.\nNow that we know these nations with high ruggedness are having an oversized effect on the estimated trend, we can try to use robust regression to lower their influence. We’ll use the same model, but with a Student’s \\(t\\) distribution likelihood (with 2 d.f.) instead of a Normal likelihood. Personally I prefer 3 degrees of freedom (the variance of the distribution is infinite if the d.f. is not larger than 2, which is a prior belief that never makes sense in a physical context to me), but for now I’ll do what the textbook says.\n\nm8.3_r &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dstudent(nu = 2, mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3_r, depth = 2)\n\n             mean         sd        5.5%       94.5%\na[1]   0.86259888 0.01614146  0.83680170  0.88839606\na[2]   1.04577255 0.01097134  1.02823823  1.06330688\nb[1]   0.11241664 0.07503557 -0.00750470  0.23233797\nb[2]  -0.21378054 0.06352620 -0.31530767 -0.11225341\nsigma  0.08451473 0.00673094  0.07375738  0.09527207\n\n\nWe can see by comparing the two model summaries that a few of the parameters are slightly different. Let’s now compare the models using PSIS.\n\nset.seed(12312)\nrethinking::compare(m8.3, m8.3_r, func = PSIS, n = 20000)\n\n            PSIS       SE    dPSIS      dSE    pPSIS       weight\nm8.3   -258.8088 15.27974  0.00000       NA 5.318484 1.000000e+00\nm8.3_r -221.8080 18.11559 37.00089 5.884289 5.760308 9.233328e-09\n\n\nHere, we can see that the non-robust model actually appears to be giving us worse predictions than the non-robust model. However, we know that PSIS is just a measure of predictive performance, so it’s possible that our robust model is still a better conceptual model that provides more accurate inferences at the cost of appearing to underfit the data. Since the two models have every similar numbers of parameters, predictive accuracy criteria are likely to be more sensitive to this kind of “underfitting”, when we actually know outside of the statistics world that we’re reducing the impact of outlying values.\nAnyways, we can also look at the individual pareto \\(k\\) values for the new model.\n\nset.seed(370)\nm8.3r_psis &lt;- rethinking::PSIS(m8.3_r, pointwise = TRUE, n = 20000)\nrownames(m8.3r_psis) &lt;- dd$isocode\npsis_sort_r &lt;- m8.3r_psis[order(m8.3r_psis$k, decreasing = TRUE), ]\npsis_sort_r |&gt; head()\n\n         PSIS      lppd     penalty std_err          k\nEST -2.787885 1.3939424 0.007899841 18.1216 0.13309736\nLSO -1.553968 0.7769838 0.264018936 18.1216 0.11825246\nALB -2.720143 1.3600717 0.013599882 18.1216 0.11215073\nATG -2.777583 1.3887917 0.008395917 18.1216 0.10169758\nUGA -2.744650 1.3723251 0.010101504 18.1216 0.09032803\nBEN -2.552244 1.2761221 0.019180005 18.1216 0.08104976\n\n\nNow we can see that the pareto \\(k\\) values are all much lower. While Lesotho still appears in the top 6, it is overall much less influential, and Seychelles no longer appears in the top 6.\n\n\n8H4\nFor this problem, we’ll use the nettle data to examine the hypothesis that higher food security leads to a higher language diversity in a region.\nFirst we need to construct the outcome variable.\n\ndata(nettle)\nd &lt;- nettle\nd$lang.per.cap &lt;- d$num.lang / d$k.pop\n\n# The log of this will be our actual outcome variable\nd$log.lang.per.cap &lt;- log10(d$lang.per.cap)\n\n# Center the outcome\nybar &lt;- mean(d$log.lang.per.cap)\nd$std.log.lang.per.cap &lt;- d$log.lang.per.cap - ybar\n\n# We also need the log of the area\nd$log.area &lt;- log10(d$area)\n\nSince I don’t really know anything about this problem other than what the textbook tells me, I’ll follow the specified steps. The effects we want to evaluate here are the effects of mean.growing.season, which we’ll call \\(M\\), and sd.growing.season, on our model. We also need to consider \\(A\\), the log of area, as a potential cause. I’m not sure how we would work in the number of weather stations in our model, so for now we’ll leave that alone – although there is a noticeable trend in the data that as the number of measurement stations increased, so did the SD of the growing season length. So in a real academic paper, we would definitely need to think about how to model that.\nFirst let’s look at the bivariate relationship between each of these values and the outcome.\n\nlayout(matrix(c(1, 2, 3), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.lang.per.cap,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 number of langauges per capita\"\n)\nplot(\n    d$sd.growing.season, d$log.lang.per.cap,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\nplot(\n    d$log.area, d$log.lang.per.cap,\n    xlab = \"log10 area of country (sq. km.)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nIn general, none of these trends looks particularly strong, although there appear to be some trends with the growing season variables.\nNow, let’s try to fit a simple model that models the log languages per capita based on the mean length of the growing season. We’ll use a normal likelihood since the outcome variable is on the log scale. Since we aren’t using a count model, which would naturally constrict the domain of the outcome variable, I also chose to center the outcome variable before modeling to make assigning a prior for the intercept feasible.\nFor the intercept, we’ll use a generic prior centered at 0 (the mean after standardization). For the effect of the mean growing season length, we’ll use a regularizing, skeptical prior centered around 0.\n\n# Set seed for all of our models in this section\nset.seed(370)\nm_mgs &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs)\n\n             mean         sd        5.5%      94.5%\na      -0.5310682 0.17466211 -0.81021195 -0.2519244\nb_mgsl  0.0754357 0.02267654  0.03919421  0.1116772\nsigma   0.6095826 0.04980052  0.52999173  0.6891734\n\n\nWe can see from the summary that there is a small prior effect of mean growing season length on the outcome. Now, let’s check whether the area should be a coefficient in this model as well.\n\nm_mgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs_a)\n\n             mean         sd        5.5%      94.5%\na       0.6533186 0.83024907 -0.67357977 1.98021696\nb_mgsl  0.0629484 0.02394189  0.02468463 0.10121217\nb_area -0.1952263 0.13386781 -0.40917288 0.01872034\nsigma   0.6009410 0.04909953  0.52247048 0.67941155\n\n\nThe effect of the area variable is negative and a large amount of the density lies below zero, which suggests that the area could be a real effect that we need to control for. Controlling for the area size also affects out estimate of the effect of mean growing season length. Let’s compare the models via WAIC and see if the area improves posterior predictions.\n\nrethinking::compare(m_mgs, m_mgs_a)\n\n            WAIC       SE     dWAIC      dSE    pWAIC   weight\nm_mgs   144.5949 15.49001 0.0000000       NA 3.778308 0.539875\nm_mgs_a 144.9146 16.01579 0.3196787 3.673048 5.049032 0.460125\n\n\nThe WAICs are extremely similar, and the WAIC for the model without area is slightly better, so I don’t think we need area in this model. If we think about the problem casually, I don’t understand why the area would be a confounder or a collider in this situation, because the area doesn’t casually determine the mean growing season length in a country. However, larger countries should have more variation in the growing season length (since larger area overall means they can cover more areas of varying latitude). But, I think that a larger area should mean there is more room for multiple communities to exist and become isolated, so a larger country should also have more languages on average.\nSo I think the DAG should look something like this.\n\nlayout(c(1))\ndag &lt;- dagitty::dagitty(\n    'DAG {\n    \"languages\" &lt;- \"mean length\"\n    \"languages\" &lt;- \"SD length\" &lt;- \"area\"\n    \"languages\" &lt;- \"area\"\n    }'\n)\ncoordinates(dag) &lt;- list(\n    x = c(\"languages\" = 0, \"mean length\" = 1, \"SD length\" = -1, \"area\" = -1),\n    y = c(\"languages\" = 0, \"mean length\" = 0, \"SD length\" = 0.5, \"area\" = -0.5)\n)\nplot(dag)\n\n\n\n\n\n\n\n\nSo, in our final model we’ll need to include area anyways, so we might as well leave it in there for now.\nNext we want to examine the effect of the SD of growing season length on languages. The area variable is a confounder in this casual structure, so we need to include that as well to avoid getting a biased estimate.\n\nm_sgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_sgs_a)\n\n              mean         sd       5.5%      94.5%\na       1.43691137 0.79286148  0.1697656 2.70405716\nb_sgsl -0.09330776 0.07997807 -0.2211282 0.03451264\nb_area -0.22761855 0.15173599 -0.4701220 0.01488487\nsigma   0.62213062 0.05082317  0.5409054 0.70335587\n\n\nWe see that the SD of growing season length does appear to have a negative effective on the overall number of languages. We cannot rule out entirely the lack of an effect (assuming our causal structure is correct and linear models are appropriate), but there is likely to be a negative effect of the SD of growing length on number of languages.\nSo far we’ve seen that an average longer growing season leads to more languages per capita, meaning that more food abundance leads to smaller, more isolated social groups and the development of more languages. However, higher variation in the growing season length leads to lower languages per capita, suggesting the need to form larger social networks for insurance against short growing seasons. In both models, we saw a negative effect of area, indicating that as a country becomes larger, the number of languages becomes smaller, which is the opposite of what I would have thought. Perhaps larger countries, on average, have shorter growing seasons? We can examine that effect quickly.\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.area,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 area of country (sq. km.)\"\n)\nplot(\n    d$sd.growing.season, d$log.area,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nInterestingly, we can see slight patterns in both trends. Larger countries seem to have slightly smaller average growing seasons, and more uncertainty in their growing seasons. I think the effect on the mean length of the growing season is quite small, and is likely to not be causal, although we could postulate that larger countries tend to cover more sparsely inhabited territory which has a shorter growing season. There is a definite trend in the standard deviation of the growing season though – it’s hard to tell if this is an effect of covering more latitude areas, or a function of a number of measuring systems. We would need a variable on the range of latitude covered by each country to disentangle those effects.\nAnyways, now we can fit the main model. We’ll fit two models that include all three variables. One will include just main effects, and the other will include an interaction between the effects of mean and SD of growing season length. Then we can compare those models.\n\nm_noint &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_noint)\n\n              mean         sd        5.5%       94.5%\na      -0.19725369 0.91289412 -1.65623482  1.26172743\nb_mgsl  0.07583973 0.02418582  0.03718613  0.11449334\nb_sgsl -0.15778988 0.07809425 -0.28259957 -0.03298018\nb_area -0.01221519 0.15896098 -0.26626554  0.24183516\nsigma   0.58530486 0.04782868  0.50886538  0.66174433\n\n\nAfter including both the mean and SD of growing season length in the model, the effect of area goes away. This suggests that the effect of area on the number of languages per capita is completely explained by the effect of the growing season – we have no evidence here for a direct causal effect of area, meaning that more room for expansion doesn’t say anything about the number of languages we expect to see, unless we know how habitable the land is first. In this model, we see a positive effect of the mean and negative effect of the SD as we expect. Now we can look at a possible interaction.\n\nm_int &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_intr * mean.growing.season * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_intr ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_int)\n\n               mean         sd        5.5%       94.5%\na      -0.607666693 0.90104859 -2.04771636  0.83238298\nb_mgsl  0.128691044 0.03267891  0.07646384  0.18091825\nb_sgsl  0.179426213 0.16417241 -0.08295301  0.44180543\nb_intr -0.046560229 0.02012491 -0.07872372 -0.01439674\nb_area -0.007638564 0.15385114 -0.25352240  0.23824527\nsigma   0.565320615 0.04620557  0.49147519  0.63916604\n\n\nIt certainly looks like the results are different, which is qualitatively important to understand. But let’s first check the WAIC to get an idea of how much better our interaction model is doing.\n\nrethinking::compare(m_noint, m_int)\n\n            WAIC       SE    dWAIC      dSE    pWAIC    weight\nm_int   139.8943 16.19209 0.000000       NA 6.821918 0.8329129\nm_noint 143.1071 16.09151 3.212828 4.717323 6.073994 0.1670871\n\n\nOK, it’s only a bit better in terms of predictive accuracy, but the estimates are so different that we need to try and understand what’s going on here.\nWhen we include an interaction term, the effect of the standard deviation on its own largely goes away, and the effect of the negative is negative with almost all of the probability mass below 0. This suggests that by itself, the SD is not important for determining the number of languages – we need to know the mean first. I think we need to make a plot to really understand this effect.\n\n# Plot the prpd\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\n\nsd_vals &lt;- c(0, 1.5, 3, 4.5)\nsample_n &lt;- 500\nncolors &lt;- 10\ncols &lt;- viridisLite::plasma(ncolors)\nrank &lt;- as.factor(as.numeric(cut(d$sd.growing.season, ncolors)))\n\nfor (i in 1:length(sd_vals)) {\n    sd &lt;- sd_vals[i]\n    idx &lt;- which(dplyr::between(d$sd.growing.season, sd - 1.5, sd + 1.5))\n    plot(\n        #d$mean.growing.season[idx],\n        #d$log.lang.per.cap[idx],\n        NULL, NULL,\n        xlim = c(0, 12),\n        ylim = 10 ^ c(-4, 0),\n        xaxs = \"i\",\n        yaxs = \"i\",\n        xlab = \"Mean growing season length (months)\",\n        ylab = \"Number of languages per capita\",\n        pch = 16,\n        log = \"y\"\n        #col = cols[rank[idx]]\n    )\n    mtext(paste0(\"SD of growing season = \", sd))\n    \n    # Calculate the posterior values\n    post_data &lt;- expand.grid(\n        mean.growing.season = seq(0, 12, 0.1),\n        sd.growing.season = sd,\n        log.area = mean(d$log.area)\n    )\n    \n    mu &lt;- rethinking::link(m_int, post_data, n = sample_n)\n    \n    for (i in 1:sample_n) {\n        lines(\n            seq(0, 12, 0.1), 10 ^ (mu[i, ] + ybar),\n            col = col.alpha(\"black\", 0.1)\n        )\n    }\n}\n\n\n\n\n\n\n\n\nFrom the posterior predictions, we can understand the effect of the interaction a lot easier. The posterior predictions shown all use the average value of the log land area, but the main thing we want to understand here is the qualitative way the effect changes. When the SD value is small (the points shown in the top right panel are from 0 to 1.5), there is little variation in the length of the growing season, and the effect of the average length of the growing season on language diversity is positive. However, as the variation in growing season length increases, the effect becomes smaller and then negative, indicating that for highly variable areas, there is little effect of the average growing season on language diversity. For extremely variable areas, a long growing season may even lead to less language diversity, but we do not have enough data to say that conclusively.\n\n\n8H5\nFor this exercise, we’ll build a model using the Wines2012 dataset.\n\ndata(\"Wines2012\")\nd &lt;- Wines2012\n\nIt looks like the wines are scores out of 20, so normally I would recommend a binomial (or beta-binomial) model here. But we don’t know that for sure and we haven’t learned that yet, so we’ll standardize the scores and model the \\(z\\)-scores instead. This is a fairly interesting problem even with the small amount of data we have, and I imagine will make quite an interesting multilevel modeling problem later in the book.\nFor this question, we just need to include effects of the judge and of the wine.\n\ndd &lt;- data.frame(\n    y = rethinking::standardize(d$score),\n    j = rethinking::coerce_index(d$judge),\n    w = rethinking::coerce_index(d$wine),\n    wine_amer = d$wine.amer,\n    judge_amer = d$judge.amer,\n    red = as.integer(ifelse(d$flight == \"red\", 1, 0))\n)\ndplyr::glimpse(dd)\n\nRows: 180\nColumns: 6\n$ y          &lt;dbl&gt; -1.57660412, -0.45045832, -0.07507639, 0.30030555, -2.32736…\n$ j          &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,…\n$ w          &lt;int&gt; 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 1, 3, 5, 7, 9, 11, 13, 1…\n$ wine_amer  &lt;int&gt; 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,…\n$ judge_amer &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ red        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nFirst we can try to visualize the data. It’s a bit difficult because we have two categorical variables and one continuous (perfect two-way anova design), but we can make two plots to try and see what’s going on.\n\nd$j2 &lt;- rethinking::coerce_index(d$judge) |&gt; factor()\np1 &lt;-\n    ggplot(d) +\n    aes(x = wine, y = score) +\n    geom_point() +\n    facet_wrap(~j2) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 8, angle = 90))\n\np2 &lt;-\n    ggplot(d) +\n    aes(x = j2, y = score) +\n    geom_point() +\n    facet_wrap(~wine) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 10)) + labs(x = \"judge\")\n\ncowplot::plot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nOf course with data like these we just kind of have to eyeball them and make qualitative guesses as to what might be going on. But In general we can see there are some nicer judges (5, 3), and some meaner judges (4, 9), and one judge that got meaner as they tried more wines (8). However, most of the wines look fairly similar with the exception of a few like C2 and I2 that appeared to get worse bad reviews. I guess if we were following Agresti’s Categorical Data Analysis the next thing to do would be to get marginal and conditional mean scores, but we don’t need to do that now, we can start fitting models (which does that in an easier way, more or less).\nWe’ll use (as usual) a normal likelihood – a \\(t\\)-likelihood might be better if some of our judges or wines give outlying scores, but for now we’ll ignore that possibility. Next we need to assign priors. Fortunately for us this is easy and I’ll just assign a typical normal prior, I’m not too sure why it really needs to be justified. So let’s go ahead and fit the first model.\n\nset.seed(12312)\nm_w1 &lt;- rethinking::quap(\n    flist = alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- a_w[w] + a_j[j],\n        a_w[w] ~ dnorm(0, 2),\n        a_j[j] ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\n\nlayout(matrix(c(1, 2), nrow = 1))\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_w[\", 1:20, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Wine parameters\")\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_j[\", 1:9, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Judge parameters\")\n\n\n\n\n\n\n\n\nLike we thought from looking at the plots, the judges appear to be more different from each other than the wines are. We probably also want to know about how the judges and wines interact, but since we don’t have any replicates, we can’t really get good answers for that question, all we can do is look at the score in each cell.\n\nggplot(d) +\n    aes(x = wine, y = judge, fill = score) +\n    geom_tile() +\n    scale_fill_viridis_c(breaks = c(7, 10, 15, 19), limits = c(7, 19.5)) +\n    hgp::theme_ms() +\n    guides(fill = guide_colorbar(barwidth = 15))\n\n\n\n\n\n\n\n\n\n\n8H6\nNow instead of looking at the variability across judges and across wines, we want to try and use the characteristics of the judges and wines to understand the scores. For this problem, we won’t include any interactions. Again, we’ll use standard priors, and the variables we’ll include as main effects are flight (whether the wine is red or white), wine.amer (if the wine was made in America), and judge.amer (whether the judge is American).\nFor whatever reason, the book says to use indicator coding for this problem and the next problem (coding interactions between two categorical variables is annoying somehow, I tried it and couldn’t figure it out). So we’ll do that.\n\nm_w2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w2)\n\n              mean         sd        5.5%      94.5%\na     -0.020886459 0.15873255 -0.27457173 0.23279881\na_wa  -0.191038758 0.14889489 -0.42900156 0.04692404\na_ja   0.247752111 0.14683135  0.01308725 0.48241697\na_red -0.004204578 0.14595115 -0.23746271 0.22905355\nsigma  0.982341917 0.05156316  0.89993403 1.06474980\n\n\nOverall, the red and white wines were judged similarly without a large discrepancy between groups. American judges tended to be more generous, and American wines tended to be rated slightly worse. However, we have a great deal of uncertainty about all of these parameters.\n\n\n8H7\nApparently doing the interactions IS the reason for using indicator coding here, quap I guess can’t handle all the stuff that Stan can. Now we want to include all of the third-level interactions that we can make.\n\nm_w3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red +\n                # American wine/american judge\n                g_wawj * wine_amer * judge_amer +\n                # American red wines\n                g_rwa * wine_amer * red +\n                # Red wines / american judge\n                g_rja * judge_amer * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            g_wawj ~ dnorm(0, 2),\n            g_rwa ~ dnorm(0, 2),\n            g_rja ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w3, depth = 2)\n\n              mean         sd       5.5%      94.5%\na      -0.21373267 0.21651035 -0.5597580  0.1322927\na_wa    0.15272583 0.26069131 -0.2639092  0.5693609\na_ja    0.29244412 0.26673524 -0.1338503  0.7187386\na_red   0.31208633 0.27442703 -0.1265011  0.7506737\ng_wawj -0.11070543 0.29175737 -0.5769901  0.3555792\ng_rwa  -0.56956098 0.29026516 -1.0334608 -0.1056612\ng_rja   0.04190135 0.28649695 -0.4159761  0.4997788\nsigma   0.97130887 0.05098788  0.8898204  1.0527973\n\n\nUsually I hate looking at tables, but this one is not too bad because the trend is pretty obvious. All of the parameters are zero-ish (lots of probably mass on either side of 0), except for one, which is g_rwa. This is the interaction term between American wines and red wines – so it looks like American red wines were much worse on average than non-american and/or non-red wines.\nIn the solutions guide, Richard actually gives a good reason for using more skeptical priors for the interaction terms than on the main effect terms. I need to remember that for the future."
  },
  {
    "objectID": "sr/week4.html",
    "href": "sr/week4.html",
    "title": "2023 Homework, week 4",
    "section": "",
    "text": "This homework covers the material from Lectures 7 and 8, and the content from book Chapters 7, 8, and 9. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\n\n\n\n\n\n\n1. Revisit the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again (pages 178-179). Compare these two models using both PSIS and WAIC. Which model is expected to make better predictions, according to these criteria, and which model yields the correct causal inference?\n\n\n\nOK, first we will fit the models. Since these are exactly the same as in the book, the results will be similar and I won’t spend a lot of time on them.\n\n# Data setup\nd &lt;- sim_happiness(seed = 1977, N_years = 1000)\nd2 &lt;- d[d$age &gt; 17, ]\nd2$A &lt;- (d2$age - 18) / (65 - 18)\nd2$M &lt;- d2$married + 1\n\nset.seed(134123)\n\n# First model: \nm6.9 &lt;-\n    quap(\n        alist(\n            happiness ~ dnorm(mu, sigma),\n            mu &lt;- a[M] + bA * A,\n            a[M] ~ dnorm(0, 1),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = d2\n    )\n\n# Second model\nm6.10 &lt;-\n    quap(\n        alist(\n            happiness ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 1),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = d2\n    )\n\nOK, now we want to score the models by PSIS and WAIC. Let’s do PSIS first.\n\nPSIS(m6.9)\n\n      PSIS      lppd  penalty  std_err\n1 2713.834 -1356.917 3.687653 37.43357\n\nPSIS(m6.10)\n\n      PSIS      lppd  penalty  std_err\n1 3101.878 -1550.939 2.321511 27.82064\n\n\nAnd now the WAIC.\n\nWAIC(m6.9)\n\n      WAIC      lppd  penalty  std_err\n1 2713.726 -1353.256 3.606974 37.38137\n\nWAIC(m6.10)\n\n     WAIC      lppd penalty  std_err\n1 3102.34 -1548.622 2.54812 27.77344\n\n\nOK, so for this model they are basically the same. But either way, we see that model m6.9, which stratifies by marriage, is better at prediction! However, we know that m6.10 actually makes the correct causal inference. This should not surprised us, because colliders contain information – even if they distort causal estimates, including them will often work better for prediction.\n\n\n\n\n\n\n2. Reconsider the urban fox analysis from last week’s homework. On the basis of PSIS and WAIC scores, which combination of variables best predicts body weight? What causal interpretation can you assign each coefficient from the best scoring model?\n\n\n\nFirst I’ll fit the two models that we were using at the end of last week’s homework – the model for the direct effect of \\(F\\) and the model for the total effect of \\(F\\) (which we recall that we could not accurately estimate due to an unmeasured confounder). We’ll also add an additional “kitchen sink” model that includes age (even though it is a precision parasite), just because we also have that. There are seven different models that we could choose, but I think these three will probably be sufficient.\n\n# Set up data\ndata(foxes)\nD &lt;-\n    foxes |&gt;\n    dplyr::select(\n        F = avgfood,\n        A = area,\n        W = weight,\n        G = groupsize\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(dplyr::everything(), standardize)\n    ) |&gt;\n    as.list()\n\n# Fit the two models\nset.seed(193482)\nkitchen_sink &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F + bG * G + bA * A,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            bG ~ dnorm(0, 2),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\nf_direct &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F + bG * G,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            bG ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\nf_total &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\n\ncoeftab(kitchen_sink, f_direct, f_total) |&gt;\n    coeftab_plot(pars = c(\"bA\", \"bF\", \"bG\"))\n\n\n\n\n\n\n\n\nOK, now we can calculate the PSIS and WAIC for all of these.\n\nsapply(list(kitchen_sink, f_direct, f_total), PSIS) |&gt;\n    `colnames&lt;-`(c(\"A, G, F\", \"F, G\", \"F\"))\n\n        A, G, F   F, G      F        \nPSIS    324.0635  324.3235  333.7011 \nlppd    -162.0317 -162.1617 -166.8506\npenalty 5.523674  4.297541  2.555102 \nstd_err 17.02429  16.84845  13.91888 \n\nsapply(list(kitchen_sink, f_direct, f_total), WAIC) |&gt;\n    `colnames&lt;-`(c(\"A, G, F\", \"F, G\", \"F\"))\n\n        A, G, F   F, G      F       \nWAIC    323.9801  324.5721  334.1936\nlppd    -156.4935 -157.8467 -164.32 \npenalty 5.496565  4.43933   2.776743\nstd_err 16.89324  17.05316  13.91089\n\n\nHere we get a little bit of a competition, but not much of one. We can see that the “kitchen sink” model wins by less than a point for PSIS, but the direct causal model wins by less than a point for WAIC. That’s obviously due to approximation error, it doesn’t matter which one wins by just a few points. Clearly we can see that the model for the direct causal effect beats out the model for the total causal effect, which makes sense because both of these variables encode unique information about the outcome (from our causal model, \\(A\\) does not).\nFor the best model, we can infer the direct causal effects of \\(F\\) and \\(G\\), but not the total causal effect of \\(F\\).\n\n\n\n\n\n\n3. Build a predictive model of the relationship shown on the cover of the book, the relationship between the timing of cherry blossoms and March temperature in the same year. The data are found in data(cherry_blossoms). Consider at least two different models (functional relationships) to predict doy with temp. Compare them with PSIS or WAIC.\nSuppose March temperatures reach 9 degrees by the year 2050. What does your best model predict for the predictive distribution of the day-in-year that the cherry trees will blossom?\n\n\n\nOK, I feel like I kind of already did this one in the textbook work I’ve done, but let’s go through it as a fun exercise anyways. First we need to process the data, which I’ll do the same way he did in the book.\n\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy, d$temp), ]\n\n# Also standardize the data\nd3 &lt;-\n    list(\n        doy = standardize(d2$doy),\n        temp = standardize(d2$temp)\n    )\n\nI’ll compare the null model, a linear model, a quadratic model, and a spline-based model similar to the one from the book. I’ll just use kind of random priors on this without a lot of explanation, just fiddling until the models seems to work correctly. Then I’ll compare the models.\n\n# Null model (intercept only)\nq3_m1 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a,\n            a ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Linear model\nq3_m2 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b * temp,\n            a ~ dnorm(0, 3),\n            b ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Quadratic model\nq3_m3 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b1 * temp + b2 * (temp ^ 2),\n            a ~ dnorm(0, 3),\n            b1 ~ dnorm(0, 3),\n            b2 ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Power law model\nq3_m4 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b * temp,\n            a ~ dnorm(0, 3),\n            b ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            doy = standardize(log(d2$doy)),\n            temp = standardize(log(d2$temp))\n        )\n    )\n\n# Spline model\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d3$temp, probs = seq(0, 1, length.out = nk))\nB &lt;-\n    splines::bs(\n        x = d3$temp,\n        knots = knot_list[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\nq3_m5 &lt;-\n    quap(\n        flist = alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(doy = d3$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\nAll the models fit fine so I assume my randomly guessed priors were OK. First we’ll compare with WAIC.\n\ncompare(q3_m1, q3_m2, q3_m3, q3_m4, q3_m5)\n\n          WAIC       SE     dWAIC        dSE     pWAIC       weight\nq3_m2 2149.026 40.95329  0.000000         NA  2.721421 6.209027e-01\nq3_m3 2151.255 41.03282  2.228806  0.2597071  3.724725 2.037251e-01\nq3_m4 2151.557 42.18830  2.531068  5.5127294  3.057807 1.751496e-01\nq3_m5 2164.893 41.43704 15.866859  8.0200012 17.965816 2.226276e-04\nq3_m1 2236.671 39.77412 87.644915 16.8970472  2.121330 5.769951e-20\n\n\nAnd now with PSIS.\n\ncompare(\n    q3_m1, q3_m2, q3_m3, q3_m4, q3_m5,\n    sort = \"PSIS\",\n    func = PSIS\n)\n\nSome Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points.\n\n\n          PSIS       SE     dPSIS        dSE     pPSIS       weight\nq3_m2 2149.271 40.84579  0.000000         NA  2.841805 5.643948e-01\nq3_m3 2150.928 41.03927  1.657495  0.2972671  3.559165 2.464124e-01\nq3_m4 2151.458 42.14878  2.187364  5.5039158  3.004142 1.890615e-01\nq3_m5 2166.003 41.54788 16.731654  8.1282313 18.531424 1.313259e-04\nq3_m1 2236.743 39.70714 87.471828 16.8454321  2.157284 5.718959e-20\n\n\nOK, so both the WAIC the the PSIS comparison give us the same result. The linear model is actually the best fit, jujst by a hair. If we take the SE into account, the linear, quadratic, and power law models all seem to fit pretty decent. The spline model is a bit worse, within one SE, and all of the “real” models are better than the null model. My guess is that while the spline model is flexible, it doesn’t gain enough predictive power from this flexibility in order to be “worth” the large number of parameters needed.\nNow let’s plot the results to get a better idea of what we’re looking at.\n\npost &lt;- sim(q3_m2)\nout &lt;- colMeans(post * sd(d2$doy) + mean(d2$doy))\nplot(\n    d2$doy, out,\n    xlab = \"Actual DoY\", ylab = \"Predicted DoY\"\n)\n\n\n\n\n\n\n\nplot(\n    d2$year, d2$doy, type = \"l\"\n)\nshade(\n    apply(post, 2, \\(x) rethinking::PI(x* sd(d2$doy) + mean(d2$doy)) ),\n    d2$year,\n    col = rethinking::col.alpha(\"gray\", alpha = 0.75)\n)\nlines(d2$year, out, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\nplot(\n    NULL, xlim = range(d2$year), ylim = c(-20, 20)\n)\nshade(\n    apply(post, 2, \\(x) rethinking::PI(x  * sd(d2$doy) + mean(d2$doy) - d2$doy)),\n    d2$year,\n    col = rethinking::col.alpha(\"gray\", alpha = 0.75)\n)\n\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\n\nlines(d2$year, out - d2$doy, type = \"l\", col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFinally, let’s look at the distribution of predictions for if we set the temperature to 9 degrees.\n\n\n\n\n\n\n4. The data in data(Dinosaurs) are body mass estimates at different estimated ages for six different dinosaur species. Choose one or more of these species and make a predictive model of body mass using age as a predictor. Consider two or more model types for the function relating age to body mass and score each using PSIS and WAIC.\nWhich model do you think is best, on predictive grounds? On scientific grounds? If your answers to these questions differ, why?"
  },
  {
    "objectID": "sr/week2.html",
    "href": "sr/week2.html",
    "title": "2023 Homework, week 2",
    "section": "",
    "text": "This homework covers the material from Lectures 3 and 4, and the content from book Chapter 4. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\n\n\n\n\n\n\n1. From the Howell1 dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume that age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions.\nDraw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.\n\n\n\nOK, I will assume that the first paragraph is just an introduction to the homework set and the actual task for this question is in the second paragraph. Here is the DAG for this problem.\n\n# Specify the relationships in the DAG\ndag &lt;-\n    dagitty::dagitty(\n        \"dag {\n            age -&gt; height -&gt; weight\n            age -&gt; weight\n        }\"\n    )\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(dag) &lt;-\n    list(\n        x = c(age = 1, height = 2, weight = 2),\n        y = c(age = 2, height = 3, weight = 1)\n    )\nplot(dag)\n\n\n\n\n\n\n\n\nNow we can write a generative simulation. First, let’s look at the pairwise correlations so we can get sort of an idea of the data distributions and the effects we should simulate.\n\nlibrary(rethinking)\ndata(Howell1)\nh1 &lt;-\n    Howell1 |&gt;\n    dplyr::filter(age &lt; 13)\n\npairs(h1[1:3])\n\n\n\n\n\n\n\n\nOk, so with that plot in mind we see that ages are discrete from 0 to 13, height ranges from about 50 units to 150 units, and weight ranges from about 5 units to 35 units. So our generative simulation should stay within those ranges.\n\n# Set the seed so the simulation makes the same numbers every time\nset.seed(101)\n\n# This part does the simulation and puts it into a tibble for storage\nsim &lt;- tibble::tibble(\n    # Just randomly draw an age. In the original data the ages are not\n    # 100% even but I think this is fine.\n    age = sample(0:13, nrow(h1), replace = TRUE),\n    # Height and weight simulations using dag relationships and made up numbers.\n    height = rnorm(nrow(h1), 60 + 5 * age, 10),\n    weight = rnorm(nrow(h1), 3 + 0.1 * age + 0.2 * height, 1)\n)\n\n# Put the columns into the same order for easier comparisons and plot\nsim &lt;- sim[, c(\"height\", \"weight\", \"age\")]\npairs(sim)\n\n\n\n\n\n\n\n\nI just randomly picked these numbers and fiddled with it a bit until the two plots looked similar, and I think I was able to get them pretty close for such a simple simulation using linear effects and normal errors.\n\n\n\n\n\n\n2. Use a linear regression to estimate the total causal effect of each year of growth on weight.\n\n\n\nBased on the DAG, to obtain the total causal effect of a year of growth (the interpretation of the parameter associated with the independent variable age) we want to use age as the only independent variable in the model. If we controlled for height, the parameter would estimate the direct causal effect of age, but we want the total effect. So the basic structure of our model will look like this. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Prior}() \\\\\n\\sigma &\\sim \\text{Prior}()\n\\end{align*}\\]\nWe will need to assign some priors to our data. In general, I tend to prefer weakly informative priors, whereas I think McElreath tends to prefer less broad priors. I’ll base my priors off the default recommended priors from the Stan devs’ prior choice recommendations. Of course they also recommend rescaling all variables before modeling, which I think is a good idea, but I won’t do it here because I’m lazy and I don’t think it’s going to be particularly useful here.\nOne additional constraint that we have is that the \\(\\alpha\\) and \\(\\beta\\) parameters should both be positive! It doesn’t make sense for someone to shrink as they get older (at least not for ages 0 to 13, maybe for seniors but not here). And it certainly doesn’t make sense for someone to ever have a negative weight, even at age zero. So we’ll use a distribution that has to be positive. I’ll choose a half-normal distribution, which is easy to sample by just taking the absolute value of a random normal sample.\n\\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\beta &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\nPerhaps we should do a prior predictive check to visualize them before doing anything else.\n\nset.seed(101)\npps &lt;-\n    tibble::tibble(\n        a = abs(rnorm(1000, 0, 1)),\n        b = abs(rnorm(1000, 0, 1)),\n        s = rexp(1000, 1)\n    )\n\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 40),\n    xlab = \"Age\", ylab = \"Simulated mu\",\n    main = \"Prior predictive simulation of E[weight | age]\"\n)\n\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.1)\n    )\n}\n\n\n\n\n\n\n\n\nWell, some of those are way too flat, and some of them are way too steep, but overall I think this encompasses a good range of possibilities. Let’s also look at the prior predictive distribution of the actual outcomes. Here, I’ll take random samples of age from a discrete uniform distribution. That’s probably not the best way to do it but it seems easiest.\n\n# Do the simulation\nset.seed(102)\nsim_age &lt;- sample(0:13, 1000, replace = TRUE)\nsim_y &lt;- rnorm(\n    1000,\n    mean = pps$a + pps$b * sim_age,\n    sd = pps$s\n)\nlayout(matrix(c(1, 2), nrow = 1))\n\n# Histogram of all y values\nhist(\n    sim_y,\n    xlab = \"Simulated outcome\",\n    main = \"Distribution of simulated weights\",\n    breaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 40),\n    xlab = \"Age\", ylab = \"Simulated weight\",\n    main = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\npoints(sim_age, sim_y)\n\n\n\n\n\n\n\n\nWell, we ended up with a few negative and a few ridiculously large weights, but since we’re just doing a linear regression here I think we can live with that and let the data inform the golem that no one has negative heights. Probably we would want to either change the likelihood function or transform something (e.g. use a log link) to prevent any negative responses, but this will probably wash out in the fitting. So let’s do that.\n\nfit &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a + b * age,\n            a ~ dnorm(0, 1),\n            b ~ dnorm(0, 1),\n            sigma ~ dexp(1)\n        ),\n        constraints = alist(\n            a = \"lower=0\",\n            b = \"lower=0\"\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age\n        )\n    )\n\nHmm. Looks like quap does not take a constraints argument the way I thought it did. So I guess we will just have to settle for exponential priors, which like I mentioned, is probably the easiest way (not the best way) to get a strictly positive prior. Let’s redo the prior predictive simulation using this model. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Exponential}(1) \\\\\n\\beta &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\n\nlayout(matrix(c(1, 2), nrow = 1))\nset.seed(101)\npps &lt;-\n    tibble::tibble(\n        a = rexp(1000, 1),\n        b = rexp(1000, 1),\n        s = rexp(1000, 1)\n    )\n\nset.seed(102)\nsim_age &lt;- sample(0:13, 1000, replace = TRUE)\nsim_y &lt;- rnorm(\n    1000,\n    mean = pps$a + pps$b * sim_age,\n    sd = pps$s\n)\n# Histogram of all y values\nhist(\n    sim_y,\n    xlab = \"Simulated outcome\",\n    main = \"Distribution of simulated weights\",\n    breaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 150),\n    xlab = \"Age\", ylab = \"Simulated weight\",\n    main = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\npoints(sim_age, sim_y)\n\n\n\n\n\n\n\n\nYes, this definitely results in a more left-skewed distribution, but I think that is actually good, since we don’t expect a lot of kids to weight 50+ kilograms. So I’m not too pressed about it. Now let’s finally fit the model, for real.\n\nset.seed(101)\nfit &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a + b * age,\n            a ~ dexp(1),\n            b ~ dexp(1),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age\n        )\n    )\n\nrethinking::precis(fit)\n\n          mean         sd     5.5%    94.5%\na     7.332870 0.35961024 6.758143 7.907596\nb     1.354753 0.05438219 1.267840 1.441667\nsigma 2.503612 0.14476058 2.272256 2.734967\n\n\nSo our estimate for the total causal effect of age on weight is 1.35. In other words, we would expect that the average individual is born at weight 7.33 units, and increases in weight by 1.35 units each year.\n\n\n\n\n\n\n3. Now suppose the causal association between age and weight might be different for boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do girls and boys differ? Provide one or more posterior contrasts as a summary.\n\n\n\nSo what we are assuming here is that the effect of age is different for males and females – we’ll also allow the intercept to vary by sex, meaning that males and females can also have a different weight at birth on average. We’ll use a similar model from before other than these changes. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Sex}_i} + \\beta \\cdot \\text{age}_{\\text{Sex}_i} \\\\\n\\alpha_{j} &\\sim \\text{Exponential}(1) \\\\\n\\beta_{j} &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\nSo now we will fit the model.\n\nset.seed(103)\nfit2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a[sex] + b[sex] * age,\n            a[sex] ~ dexp(1),\n            b[sex] ~ dexp(1),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age,\n            # We have to add 1 for the index coding to work right\n            sex = h1$male + 1\n        ),\n        start = list(a = c(1, 1), b = c(1, 1), sigma = 0.5)\n    )\n\nrethinking::precis(fit2, depth = 2)\n\n          mean         sd     5.5%    94.5%\na[1]  6.861204 0.47573974 6.100880 7.621528\na[2]  7.664618 0.50386642 6.859343 8.469894\nb[1]  1.312828 0.07258895 1.196816 1.428839\nb[2]  1.414252 0.07545861 1.293655 1.534850\nsigma 2.406891 0.13948954 2.183960 2.629822\n\n\nOk, so I had some issues with the start values here. Probably because the priors are very diffuse, quap sometimes cannot get to the MAP from randomly sampled starting locations. However, I just had to find a seed that works, because it seems to be ignoring the start value for the vector-valued parameters (or I am specifying it incorrectly) in the error messages I get. But this one fit, so let’s go. There appears to be a slight difference between the two slope parameters, but we cannot allow ourselves to be mislead by the table of coefficients. We must compute the contrast distribution to truly understand what is happening here.\nNow, in order to understand the differences, we have to construct the contrast distribution.\n\nN &lt;- 2500\n\nage_vals &lt;- 0:12\nmean_f &lt;- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n                                                     n = N)\nmean_m &lt;- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n                                                     n = N)\n\nmean_contrast &lt;- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(0, 30),\n    xlab = \"Simulated age\",\n    ylab = \"Weight (kg)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_m[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"dodgerblue2\", 0.01)\n    )\n    lines(\n        age_vals, mean_f[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"hotpink1\", 0.01)\n    )\n}\n\nlegend(\"topleft\", c(\"Males\", \"Females\"), col = c(\"dodgerblue2\", \"hotpink1\"),\n             lty = c(1, 1))\n\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(-3, 5),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_contrast[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nHere, we can observe that the regression lines for males tend to be steeper than the regression lines for females. From the contrast, we can see that most of the time, the effect is positive and the line lies above zero. Let’s compute a posterior interval for the mean.\n\nlayout(1)\nplot(\n    NULL,\n    xlim = c(0, 12),\n    ylim = c(-3, 5),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    shade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nFrom the contrast, we can see that men tend to be heavier at nearly all ages. The weight of males and females tends to be closer at birth than on average at older ages.\nThis contrast is between the estimates of the conditional mean response. To incorporate individual variance into the uncertainty around the estimate, we can also use samples from the posterior of the conditional mean, along with samples from the posterior of the scale parameter to simulate individual responses.\n\nN &lt;- 2500\n\nage_vals &lt;- 0:12\nmean_f &lt;- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n                                                     n = N)\nmean_m &lt;- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n                                                     n = N)\n\nmean_contrast &lt;- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(-12, 12),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_contrast[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\nplot(\n    NULL,\n    xlim = c(0, 12),\n    ylim = c(-12, 12),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    shade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nInterestingly, the variance tends to be quite large. However, as we see from the plot on the left, we could probably use a better variance structure for this model. If each line represents an individual, the spikes over time are unlikely to be this drastic, especially for children. We still see (from the right plot) that a given male is more likely to be heavier than a given female of the same age, but there is a lot of variation between individuals so this will not always be true.\n\n\n\n\n\n\n4. The data in data(Oxboys) are growth records for 26 boys measured over 9 periods. I want you to model their growth. Specifically, model the increments in growth from one Occassion to the next. Each increment is simply the difference between height in one occasion and height in the previous occasion. Since none of these boys shrunk during the study, all of the growth increments are greater than zero. Estimate the posterior distribution of these increments. Constrain the distribution so it is always positive – it should not be possible for the model to think that boys can shrink from year to year. Finally computer the posterior distribution of the total growth over all 9 occasions.\n\n\n\n\ndata(Oxboys)\ndplyr::glimpse(Oxboys)\n\nRows: 234\nColumns: 4\n$ Subject  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3…\n$ age      &lt;dbl&gt; -1.0000, -0.7479, -0.4630, -0.1643, -0.0027, 0.2466, 0.5562, …\n$ height   &lt;dbl&gt; 140.5, 143.4, 144.8, 147.1, 147.7, 150.2, 151.7, 153.3, 155.8…\n$ Occasion &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3…\n\n\nFrom the data documentation, we see that age and Occasion basically represent the same thing, so I’ll ignore age for now. I think for this model, we don’t even need to draw a DAG as there are only two variables we really care about.\nThe first thing I’ll do is the necessary data processing. We want to get the increments in growth by subtracting the previous measurement from each height value. This will give us the amount of growth in between each measurement. Note that this will reduce us from 9 total measurements per boy to 8 total differences per boy, because there is nothing to subtract from the first measurement.\n\no2 &lt;- Oxboys |&gt;\n    dplyr::group_by(Subject) |&gt;\n    dplyr::mutate(prev = dplyr::lag(height)) |&gt;\n    dplyr::mutate(\n        increment = height - prev,\n        period = Occasion - 1\n    ) |&gt;\n    dplyr::filter(\n        Occasion != 1\n    )\n\no2_l &lt;- list(\n    increment = o2$increment,\n    period = o2$period\n)\n\nSo let’s plot the distribution of each increment.\n\nset.seed(100)\nlayout(cbind(\n    matrix(c(1:8), nrow = 2, ncol = 4, byrow = TRUE)\n))\n\n# Histograms of each increment\nfor (i in 1:8) {\n    dat &lt;- subset(o2, period == i)\n    hist(\n        dat$increment,\n        breaks = seq(0, 5, 0.5),\n        xlab = NULL,\n        main = paste(\"Increment\", i),\n        cex.lab = 1.25,\n        cex.axis = 1.25\n    )\n}\n\n\n\n\n\n\n\nlayout(1)\n# Boxplot for each increment\nboxplot(\n    increment ~ period,\n    data = o2_l,\n    cex.lab = 1.75,\n    cex.axis = 1.75,\n    xlab = \"Period\", ylab = \"Increment\"\n)\nstripchart(\n    increment ~ period,\n    data = o2_l,\n    method = \"jitter\",\n    vertical = TRUE,\n    add = TRUE,\n    pch = 21,\n    col = rethinking::col.alpha(\"red\", 0.5),\n    cex = 1.25\n)\n\n\n\n\n\n\n\n\nThere is definitely some variance in each increment, so I think it makes sense to use a model that looks like this \\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\ninstead of a model that looks like this\n\\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta_i \\cdot \\text{Period}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\n\nMcElreath also chose to constrain the alpha parameters to all be the same, which I think is fine and makes the model much simpler. But since we were interested in the posterior distribution of their sum anyways, I prefer to leave the model like this anyways. Besides, from the histogram we saw that some of them were different – I’m not sure if this is a good biological assumption or not though since I don’t really know enough about this problem.\n\nwhere increment is the difference, and period is the same as occasion minus 1. We still need to choose \\(k\\). Since we have a lognormal distribution, the variance parameter should be biased to be small (untuitively we do this by making the exponential parameter larger) or else when we do the log part, the means will get really big. This is also true for \\(k\\), which I decided to simulate instead of just picking to be big.\n\nNote that I originally used a normal regression with a lognormal prior, but I read McElreath’s solution and decided to update this to use a lognormal regression with a normal prior, which I think is actually much better. If you use a Gaussian model for this, you run the risk of randomly getting growth increments from the model that are negative, which we specifically wanted to avoid in this problem.\n\nSo the increment over the first period is the difference between the second height measurement and the first height measurement. (I also think that this is what the problem is asking for.)\nIf we use period as an index variable, we could also include an “overall” intercept, but we don’t really need to (the models are computationally equivalent parametrizations). Sometimes adding the overall intercept makes the model run better, or so I’ve heard, but I’ll ignore it for now.\nI choose a standard exponential prior on sigma, but for each alpha I choose a lognormal prior. This also constrains the effect of each increment to be strictly positive, but prefers a moderate effect rather than having most of the weight near zero. In order to choose the variance constant \\(k\\) that I wrote in above as a placeholder, I think it might be best to do a small prior predictive simulation. I’ll just simulate one increment, since the model will allow them to be different regardless.\nI probably should have done this before looking at the data, but I think for now it will be OK. We want to choose a diffuse prior, but not one that produces insane results.\n\nset.seed(2323)\nk &lt;- c(0.05, 0.1, 0.25, 0.5, 1, 1.5) # Values to try\nN &lt;- 1000 # Number of simulations\n\n# Create a container for the output\npps &lt;- matrix(nrow = N, ncol = length(k))\n\n# Do the simulation\nfor (i in 1:length(k)) {\n    pps[, i] &lt;- rlnorm(N, rnorm(N, 0, k[i]), rexp(N, 10))\n    \n}\n\n# Make a plot\nlayout(matrix(1:length(k), nrow = 2, byrow = TRUE))\nfor (i in 1:length(k)) {\n    hist(pps[, i], breaks = \"FD\", main = paste(\"k =\", k[i]), xlab = NULL)\n} \n\n\n\n\n\n\n\n\nWow, I forgot how hard it is to choose parameters for a lognormal prior. That scaling parameter really starts to act up quickly. So for this problem, I’ll choose \\(k=0.5\\) since it creates a prior that, in general, constrains our values to what seems “reasonable” but also allows the parameter to get large if it needs to, due to the tail of the distribution.\nSo to recap before we fit, our final model will be this. \\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Lognormal}(0, 0.5) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\n\nset.seed(105)\nfit_ob &lt;-\n    rethinking::quap(\n        flist = alist(\n            increment ~ dlnorm(mu, sigma),\n            mu &lt;- a[period],\n            a[period] ~ dnorm(0, 0.5),\n            sigma ~ dexp(10)\n        ),\n        data = o2_l,\n        control = list(maxit = 500)\n    )\nrethinking::precis(fit_ob, depth = 2)\n\n            mean         sd        5.5%     94.5%\na[1]   0.3978845 0.10740207  0.22623527 0.5695338\na[2]   0.2322873 0.10739249  0.06065337 0.4039213\na[3]   0.5005341 0.10741057  0.32887130 0.6721970\na[4]  -0.1303368 0.10738912 -0.30196531 0.0412918\na[5]   0.1171602 0.10738882 -0.05446786 0.2887883\na[6]   0.6545573 0.10742686  0.48286841 0.8262461\na[7]   0.5295969 0.10741329  0.35792970 0.7012641\na[8]   0.3111911 0.10739646  0.13955079 0.4828314\nsigma  0.5606550 0.02697832  0.51753842 0.6037716\n\n\nOkay, so we can see that all of the parameters are in the same sort of neighborhood, but they definitely appear to be somewhat different. However! Recall that we cannot allow ourselves to stare into the void and be mislead. We shall first visualize the posterior distributions of these parameters.\n\nset.seed(1344134)\npost &lt;- extract.samples(fit_ob)\nmn &lt;- apply(post$a, 2, mean)\npi &lt;- apply(post$a, 2, rethinking::PI)\n\nlayout(matrix(1:8, nrow = 2, byrow = TRUE))\nfor (i in 1:ncol(post$a)) {\n    # Blank plot\n    plot(\n        NULL,\n        xlim = c(-1.5, 1.5), ylim = c(0, 4.5),\n        xlab = paste(\"Increment\", i, \"effect (cm)\"),\n        ylab = \"Density\",\n        xaxs = \"i\", yaxs = \"i\",\n        cex.lab = 1.5,\n        cex.axis = 1.5\n    )\n    \n    # Mean and 89% PI\n    abline(v = mn[i], col = \"red\", lty = 2, lwd = 2)\n    abline(v = pi[1, i], col = \"red\", lty = 2)\n    abline(v = pi[2, i], col = \"red\", lty = 2)\n    \n    # Density curve\n    rethinking::dens(post$a[, i], add = TRUE, lwd = 3)\n}\n\n# Common title\nmtext(\n    paste(\n        \"Posterior distributions of individual increment effects\",\n        \"(mean and 89% posterior interval)\",\n        sep = \"\\n\"\n    ),\n    side = 3,\n    line = -3,\n    outer = TRUE,\n    cex = 1.1\n    )\n\n\n\n\n\n\n\n\nAgain, we must be careful not to make any undue comparisons here. These summaries are fine for thinking about individual effects, but it does not make sense to compare them across parameters. So now we shall compute the contrast of interest. We can get the posterior of the total growth by creating a contrast across each of the growth increments. Of course this contrast will look like \\(c^{\\mkern-1.5mu\\mathsf{T}} = \\langle 1, 1, \\ldots, 1 \\rangle\\) because we will just be adding up the samples.\n\nlayout(1)\n\n# Sample the growth increments from the posterior\n# and then add them up\ncontrast &lt;- sapply(\n    1:1000,\n    \\(i) sum(rlnorm(8, post$a[i, ], post$sigma[i]))\n)\ncontrast_mean &lt;- mean(contrast)\n\nplot(\n    NULL,\n    xlim = c(0, 30),\n    ylim = c(0, 0.18),\n    xlab = \"Sum of growth increments (cm)\",\n    ylab = \"Density\",\n    xaxs = \"i\", yaxs = \"i\",\n    cex.axis = 1.25,\n    cex.lab = 1.1,\n    tcl = -0.25\n)\n\nplot_dens &lt;- density(contrast, adjust = 0.5)\n\nfor (i in c(67, 89, 95, 99) / 100) {\n    shade(plot_dens, PI(contrast, prob = i))\n}\n\nlines(\n    x = rep(contrast_mean, 2),\n    y = c(0, plot_dens$y[which.min(abs(plot_dens$x - contrast_mean))]),\n    lty = 2, lwd = 2\n)\nrethinking::dens(contrast, lwd = 3, add = TRUE)\n\n\n\n\n\n\n\n\nHere the dashed line shows the posterior mean. The dashed line shows the posterior mean, and the shaded regions show equal-tailed credible intervals. From darkest to lightest, the coverage for the CIs are, respectively: 67%, 89%, 95%, 99%.\nI think we should also probably do this and include the individual-level variance (the estimated sigma parameter), but for right now I’ll call it a day. We can see that the mean total growth was most likely to be 13 cm over the nine occasions, although a range of values from around 12 to 14 were also quite plausible.\n\nThis looks quite similar to McElreath’s solution where the alpha for each increment is required to be the same, with one major difference. The slope of the density is much more gradual to the left of the mean in my estimated distribution. This could be sampling variation, or it could be that one (or more) of the increments is typically smaller than the others in this population, and drags down the overall average parameter value when they are constrained to be the same."
  },
  {
    "objectID": "sr/week1.html",
    "href": "sr/week1.html",
    "title": "2023 Homework, week 1",
    "section": "",
    "text": "This homework covers the material from Lectures 1 and 2, and the content from book Chapters 1, 2, and 3. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions. As of 2023-01-17, I believe my solutions are consistent with the now-released official solutions by McElreath and I do not intend to edit these further.\n\n\n\n\n\n\n1. Suppose the globe tossing data (Lecture 2, Chapter 2) had turned out to be 4 water and 11 land. Construct the posterior distribution.\n\n\n\nFortunately, I already did a very similar problem in the textbook, so I used the function that I already wrote. This assumes a uniform prior distribution for \\(p\\).\n\nglobe_post &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n# Calculate the posterior after assuming there are 4 water observations and\n# 11 land observations\npost &lt;- globe_post(w = 4, l = 11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Using the posterior distribution from 1, compute the posterior predictive distribution for the next 5 tosses of the same globe. I recommend you use the sampling method.\n\n\n\nFirst we construct a set of 10000 samples from the posterior distribution. We sample from the same grid of \\(p\\) values that we used to calculate the posterior distribution, weighting the likelihood of each value by the calculated posterior density.\n\nsamples &lt;- sample(\n    x = seq(from = 0, to = 1, by = 0.01),\n    size = 1e5,\n    prob = post,\n    replace = TRUE\n)\n\nNext we compute the posterior predictive distribution. We draw from the likelihood function (with \\(n = 5\\) to represent the next five samples) using the sampled values of \\(p\\).\n\npost_pred &lt;- rbinom(10000, size = 5, prob = samples)\nrethinking::simplehist(post_pred)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use the posterior predictive distribution from 2 to calculate the probability of 3 or more water samples in the next 5 tosses.\n\n\n\nNow that we have the samples from the posterior predictive distribution, this probability is simple to approximate.\n\nmean(post_pred &gt;= 3)\n\n[1] 0.1789\n\n\nThe probability of three or more water samples in the next 5 tosses is approximately \\(18.28\\%\\), based on our posterior predictive distribution.\n\n\n\n\n\n\n4. Suppose you observe \\(W = 5\\) water points, but you forgot to write down how many times the globe was tossed, so you don’t know the number of land points, \\(L\\). Assume that \\(p = 0.7\\) and compute the posterior distribution of the number of tosses \\(N\\).\n\n\n\nIf we assume that \\(p = 0.5\\) and we observe \\(W = 5\\) water points, then the likelihood as a function of \\(N\\) (unknown) is\n\\[P(N) = \\left( N\\atop{5} \\right) 5^{0.7} (N-5)^{0.3}.\\]\nWe need to choose a prior for \\(N\\) in order to calculate the posterior distribution. We haven’t discussed many different distributions for priors yet, but I’ll choose a Poisson prior because it has the set \\(\\{0, 1, \\ldots, \\infty\\}\\) as its support, so there is no upper limit on \\(N\\) that we have to choose. The Poisson distribution has one parameter, called \\(\\lambda\\), I’ll choose to be \\(5\\) for no other reason than that is the number of throws we know we observed. It would also make sense to choose a uniform prior with some very large upper boundary, say \\(\\mathrm{Uniform}(5, 100)\\). (Or even smaller than that, because we can probably place a relatively small upper bound on the number of times we threw the ball, since we know \\(p = 0.7\\).)\nOther than changing the prior, we calculate this the same way. Since it’s impossible for us to evaluate every \\(N\\) up to \\(\\infty\\), I’ll just calculate up to 25 and that should be plenty.\n\n# Define the grid of points to evaluate\nn_grid &lt;- seq(from = 0, to = 25, by = 1)\n# Poisson prior on n\nprior &lt;- dpois(n_grid, lambda = 5)\n# Compute the likelihood over the grid given the observed sample\nlikelihood &lt;- dbinom(5, size = n_grid, prob = 0.7)\n# Compute the unstandardized posterior\nunstd.posterior &lt;- likelihood * prior\n# Standardize the posterior\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n# Make the plot\nplot(n_grid, posterior, type = \"b\", xlab = \"Number of samples (N)\",\n         ylab = \"Posterior probability\")\nlines(n_grid, prior, type = \"l\", lty = 2)\n\n\n\n\n\n\n\n\nIn this figure, the points are the observed posterior probabilities. The solid line just connects them to give us a better idea of the shape of the distribution. The dashed line shows the Poisson prior on \\(N\\).\nAs we can see in the posterior, it is impossible that we drew less than 5 samples. That’s good, that means that our model reflects reality, which is a good sanity check for our golem. Given that \\(p = 0.7\\) however, it is actually quite plausible that we only tossed the globe 5 times, which could explain why we didn’t write down the number of tosses or the number of land throws – there weren’t any.\nHowever, the MAP estimate would be that we threw the globe 6 times, with 7 times in a close second place. As we get to 10 or higher, it becomes incredibly unlikely that we would throw the ball this many times and only observe 5 water tosses."
  },
  {
    "objectID": "sr/week3.html",
    "href": "sr/week3.html",
    "title": "2023 Homework, week 3",
    "section": "",
    "text": "This homework covers the material from Lectures 5 and 6, and the content from book Chapters 5 and 6. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\nFrom the Homework:\n\nThe problems are based on the same data. The data in data(foxes) are 116 foxes from 30 different urban groups in Engalnd. These fox groups are like street gangs. Group size (groupsize) varies from 2 to 8 individuals. Each group maintains its own (almsot exclusive) urban territory. Some territories are larger than others. The area variable encodes this information. Some territories also have more avgfood than others. And food influences the weight of each fox. Assume this DAG:\n\n\nlibrary(dagitty)\ndag &lt;- dagitty::dagitty(\n    \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n    }\"\n)\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\n\nplot(dag)\n\n\n\n\n\n\n\n\n\nwhere \\(F\\) is avgfood, \\(G\\) is groupsize, \\(A\\) is area, and \\(W\\) is weight.\n\n\n\n\n\n\n\n1. Use the backdoor criterion and estimate the total causal influence of \\(A\\) on \\(F\\). What effect would increasing the area of a territory have on the amount of food inside of it?\n\n\n\nThere are no confounds and there is in fact nothing else on the causal pathway from \\(A\\) to \\(F\\). So these are the only two variables that need to be in this model. I’ll do a simple linear regression with just some default priors to estimate this.\n\n# Setup\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(foxes)\n\n# Standardize the variables\nD &lt;-\n    foxes |&gt;\n    dplyr::select(\n        F = avgfood,\n        A = area,\n        W = weight,\n        G = groupsize\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(dplyr::everything(), standardize)\n    ) |&gt;\n    as.list()\n\n\nset.seed(54564)\n# The model\na_on_f &lt;- rethinking::quap(\n    flist = alist(\n        F ~ dnorm(mu, sigma),\n        mu &lt;- a + b * A,\n        a ~ dnorm(0, 5),\n        b ~ dnorm(0, 5),\n        sigma ~ dexp(1)\n    ),\n    data = D\n)\nrethinking::precis(a_on_f)\n\n               mean         sd        5.5%      94.5%\na     -5.554240e-09 0.04328528 -0.06917824 0.06917822\nb      8.830368e-01 0.04347305  0.81355842 0.95251509\nsigma  4.662142e-01 0.03051589  0.41744386 0.51498444\n\n\nSo we see that the estimate of the total causal effect of \\(A\\) on \\(F\\) is \\(0.88\\) with an \\(89\\%\\) CI of \\(0.81\\) to \\(0.95\\). Let’s plot the distribution really quick just to get a better idea.\n\nset.seed(100)\nrethinking::dens(extract.samples(a_on_f)$b, lwd = 3)\n\n\n\n\n\n\n\n\nThe density is centered around 0.88, with our model identifying a range of plausible values for around 0.81 to 0.95. In general, we can observe a positive effect of area size on the amount of food available in a territory. This makes sense, because if all of these foxes are from around the same region, there should be a similar amount of food available across the area, so increasing the size of the territory increases the amount of available food. This might not have been the case if the foxes were from varying environments, where a large territory might ne necessary for a fox in a food-poor environment to have the same food availbility in a food-rich environment.\n\n\n\n\n\n\n2. Infer the total causal effect of adding food \\(F\\) to a territory on the weight \\(W\\) of foxes. Can you calculate the causal effect by simulating an intervention on food?\n\n\n\nAgain, we don’t need to control anything to estimate this total causal effect. \\(G\\) is a mediator of the relationship of \\(F\\) on \\(W\\) and \\(A\\) is a cause of \\(F\\) so we don’t need to control for either of these. I’ll fit another model with nondescript priors.\n\nset.seed(100)\nf_on_w_total &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * F,\n        a ~ dnorm(0, 1),\n        bF ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = D\n)\nf_on_w_total |&gt; rethinking::coeftab() |&gt; rethinking::coeftab_plot()\n\n\n\n\n\n\n\n\nFrom the model estimates, we can see that, in general, the effect of food availibility on fox weight does not seem to be very strong. The point estimate is slightly negative, with the credible interval reflecting a wide range of potential values of either direction.\nNow we can simulate an intervention on the amount of food. That is, we’re estimating \\(f(W \\mid \\text{do}(F))\\) by simulation. First we will draw the DAG for when we \\(\\text{do}(F)\\). In this DAG, we delete all arrows into \\(F\\) (because we are controlling the value of it).\n\ndo_f &lt;- dagitty::dagitty(\n    \"dag {\n        F -&gt; G -&gt; W\n        F -&gt; W\n    }\"\n)\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(do_f) &lt;-\n    list(\n        x = c(F = 1, G = 3, W = 2),\n        y = c(F = 2, G = 2, W = 3)\n    )\n\nplot(do_f)\n\n\n\n\n\n\n\n\nSince \\(G\\) is a pipe, we can ignore simulating \\(G\\) and instead only simulate \\(F\\). We’ll simulate this intervention by controlling for the value of \\(F\\) and using the posterior samples to calculate the values of \\(W\\) for each \\(F\\).\n\n# Setup\nN &lt;- 1e3\nset.seed(1234819084)\n\n# Extract the posterior samples\npost &lt;- rethinking::extract.samples(f_on_w_total, n = N)\n\n# Values of F to simulate\nf_vec &lt;- seq(0, 1.5, 0.01)\n\n# Container for results\nout &lt;- matrix(nrow = N, ncol = length(f_vec))\n\n# Simulate the results\nfor (i in 1:length(f_vec)) {\n    out[, i] &lt;- with(\n        post,\n        rnorm(N, a + bF * f_vec[[i]], sigma)\n    )\n}\n\n# Summarize the output matrix\nplot(\n    NULL,\n    xlim = c(0, 1.5), ylim = c(-5, 5),\n    xlab = \"Manipulated F\", ylab = \"Simulated W\"\n)\n\n# for (i in 1:length(f_vec)) {\n#   lines(x = f_vec, y = out[i, ], col = rethinking::col.alpha(\"black\", 0.05))\n# }\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    interval &lt;- apply(out, 2, rethinking::PI, prob = p)\n    rethinking::shade(interval, f_vec)\n}\n\nlines(\n    x = f_vec, y = colMeans(out),\n    type = \"l\", col = \"black\", lwd = 3,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Infer the direct causal effect of adding food \\(F\\) to a territory on the weight \\(W\\) of foxes. In light of your estmiates from this problem and the previous one, what do you think is going on with these foxes?\n\n\n\nBased on the DAG, to get the direct causal effect of foxes, we also need to stratify by \\(G\\) in our model. So we will fit that model first.\n\nset.seed(100)\nf_on_w_direct &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * F + bG * G,\n        a ~ dnorm(0, 5),\n        bF ~ dnorm(0, 5),\n        bG ~ dnorm(0, 5),\n        sigma ~ dexp(1)\n    ),\n    data = D,\n    control = list(maxit = 500)\n)\nf_on_w_direct |&gt; rethinking::coeftab() |&gt; rethinking::coeftab_plot()\n\n\n\n\n\n\n\n\nOK, now let’s compare the coefficients of the two models.\n\nrethinking::coeftab(f_on_w_total, f_on_w_direct) |&gt;\n    rethinking::coeftab_plot(pars  = c(\"bF\", \"bG\"))\n\n\n\n\n\n\n\n\nSo when we only stratify by \\(F\\), we don’t see an effect. But when we stratify by \\(F\\) and \\(G\\), we see an effect of both variables! The optional problem sort of spoiled this, but it seems like the most likely explanation here is negative confounding by an unobserved variable. When we don’t control for \\(G\\), the confounder still has a backdoor pathway through to \\(W\\), but when we control for \\(G\\) and \\(F\\) simultaneously, the backdoor path is closed and the effect of \\(U\\) will be absorbed into the estimated effects of \\(F\\) and \\(G\\) (as it should be). So this is an example of a masked relationship.\n\n\n\n\n\n\n4. Suppose there is an unobserved confound that influences \\(F\\) and \\(G\\), saying \\(\\boxed{U}\\). Assuming this is the correct DAG, again estimate both the total and direct causal effects of \\(F\\) on \\(W\\). What impact does the unobserved confound have?\n\n\n\nI didn’t finish this question because I didn’t know how, and apparently it is a trick question because it is not possible to get the total causal effect of \\(F\\) (since \\(U\\) is unobserved). That makes me feel better. We would estimate the direct causal effect of \\(F\\) like we already did under this model."
  },
  {
    "objectID": "sr.html",
    "href": "sr.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Notes and solutions for “Statistical Rethinking”, 2nd edition, by Richard McElreath, 2020, CRC Press.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8: Conditional Manatees\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Ulysses’ Compass\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: The Haunted DAG and the Causal Terror\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 4\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 3\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 2\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 1\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Geocentric Models\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Sampling the Imaginary\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Small Worlds and Large Worlds\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: The Many Variables and the Spurious Waffles\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: The Golem of Prague\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  }
]