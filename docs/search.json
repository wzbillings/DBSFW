[
  {
    "objectID": "sr.html",
    "href": "sr.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Notes and solutions for “Statistical Rethinking”, 2nd edition, by Richard McElreath, 2020, CRC Press.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8: Conditional Manatees\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Ulysses’ Compass\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: The Haunted DAG and the Causal Terror\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 4\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 3\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 2\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Homework, week 1\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Geocentric Models\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Sampling the Imaginary\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Small Worlds and Large Worlds\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: The Many Variables and the Spurious Waffles\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: The Golem of Prague\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sr/week3.html",
    "href": "sr/week3.html",
    "title": "2023 Homework, week 3",
    "section": "",
    "text": "This homework covers the material from Lectures 5 and 6, and the content from book Chapters 5 and 6. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\nFrom the Homework:\n\nThe problems are based on the same data. The data in data(foxes) are 116 foxes from 30 different urban groups in Engalnd. These fox groups are like street gangs. Group size (groupsize) varies from 2 to 8 individuals. Each group maintains its own (almsot exclusive) urban territory. Some territories are larger than others. The area variable encodes this information. Some territories also have more avgfood than others. And food influences the weight of each fox. Assume this DAG:\n\n\nlibrary(dagitty)\ndag &lt;- dagitty::dagitty(\n    \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n    }\"\n)\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\n\nplot(dag)\n\n\n\n\n\n\n\n\n\nwhere \\(F\\) is avgfood, \\(G\\) is groupsize, \\(A\\) is area, and \\(W\\) is weight.\n\n\n\n\n\n\n\n1. Use the backdoor criterion and estimate the total causal influence of \\(A\\) on \\(F\\). What effect would increasing the area of a territory have on the amount of food inside of it?\n\n\n\nThere are no confounds and there is in fact nothing else on the causal pathway from \\(A\\) to \\(F\\). So these are the only two variables that need to be in this model. I’ll do a simple linear regression with just some default priors to estimate this.\n\n# Setup\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(foxes)\n\n# Standardize the variables\nD &lt;-\n    foxes |&gt;\n    dplyr::select(\n        F = avgfood,\n        A = area,\n        W = weight,\n        G = groupsize\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(dplyr::everything(), standardize)\n    ) |&gt;\n    as.list()\n\n\nset.seed(54564)\n# The model\na_on_f &lt;- rethinking::quap(\n    flist = alist(\n        F ~ dnorm(mu, sigma),\n        mu &lt;- a + b * A,\n        a ~ dnorm(0, 5),\n        b ~ dnorm(0, 5),\n        sigma ~ dexp(1)\n    ),\n    data = D\n)\nrethinking::precis(a_on_f)\n\n               mean         sd        5.5%      94.5%\na     -5.554240e-09 0.04328528 -0.06917824 0.06917822\nb      8.830368e-01 0.04347305  0.81355842 0.95251509\nsigma  4.662142e-01 0.03051589  0.41744386 0.51498444\n\n\nSo we see that the estimate of the total causal effect of \\(A\\) on \\(F\\) is \\(0.88\\) with an \\(89\\%\\) CI of \\(0.81\\) to \\(0.95\\). Let’s plot the distribution really quick just to get a better idea.\n\nset.seed(100)\nrethinking::dens(extract.samples(a_on_f)$b, lwd = 3)\n\n\n\n\n\n\n\n\nThe density is centered around 0.88, with our model identifying a range of plausible values for around 0.81 to 0.95. In general, we can observe a positive effect of area size on the amount of food available in a territory. This makes sense, because if all of these foxes are from around the same region, there should be a similar amount of food available across the area, so increasing the size of the territory increases the amount of available food. This might not have been the case if the foxes were from varying environments, where a large territory might ne necessary for a fox in a food-poor environment to have the same food availbility in a food-rich environment.\n\n\n\n\n\n\n2. Infer the total causal effect of adding food \\(F\\) to a territory on the weight \\(W\\) of foxes. Can you calculate the causal effect by simulating an intervention on food?\n\n\n\nAgain, we don’t need to control anything to estimate this total causal effect. \\(G\\) is a mediator of the relationship of \\(F\\) on \\(W\\) and \\(A\\) is a cause of \\(F\\) so we don’t need to control for either of these. I’ll fit another model with nondescript priors.\n\nset.seed(100)\nf_on_w_total &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * F,\n        a ~ dnorm(0, 1),\n        bF ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = D\n)\nf_on_w_total |&gt; rethinking::coeftab() |&gt; rethinking::coeftab_plot()\n\n\n\n\n\n\n\n\nFrom the model estimates, we can see that, in general, the effect of food availibility on fox weight does not seem to be very strong. The point estimate is slightly negative, with the credible interval reflecting a wide range of potential values of either direction.\nNow we can simulate an intervention on the amount of food. That is, we’re estimating \\(f(W \\mid \\text{do}(F))\\) by simulation. First we will draw the DAG for when we \\(\\text{do}(F)\\). In this DAG, we delete all arrows into \\(F\\) (because we are controlling the value of it).\n\ndo_f &lt;- dagitty::dagitty(\n    \"dag {\n        F -&gt; G -&gt; W\n        F -&gt; W\n    }\"\n)\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(do_f) &lt;-\n    list(\n        x = c(F = 1, G = 3, W = 2),\n        y = c(F = 2, G = 2, W = 3)\n    )\n\nplot(do_f)\n\n\n\n\n\n\n\n\nSince \\(G\\) is a pipe, we can ignore simulating \\(G\\) and instead only simulate \\(F\\). We’ll simulate this intervention by controlling for the value of \\(F\\) and using the posterior samples to calculate the values of \\(W\\) for each \\(F\\).\n\n# Setup\nN &lt;- 1e3\nset.seed(1234819084)\n\n# Extract the posterior samples\npost &lt;- rethinking::extract.samples(f_on_w_total, n = N)\n\n# Values of F to simulate\nf_vec &lt;- seq(0, 1.5, 0.01)\n\n# Container for results\nout &lt;- matrix(nrow = N, ncol = length(f_vec))\n\n# Simulate the results\nfor (i in 1:length(f_vec)) {\n    out[, i] &lt;- with(\n        post,\n        rnorm(N, a + bF * f_vec[[i]], sigma)\n    )\n}\n\n# Summarize the output matrix\nplot(\n    NULL,\n    xlim = c(0, 1.5), ylim = c(-5, 5),\n    xlab = \"Manipulated F\", ylab = \"Simulated W\"\n)\n\n# for (i in 1:length(f_vec)) {\n#   lines(x = f_vec, y = out[i, ], col = rethinking::col.alpha(\"black\", 0.05))\n# }\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    interval &lt;- apply(out, 2, rethinking::PI, prob = p)\n    rethinking::shade(interval, f_vec)\n}\n\nlines(\n    x = f_vec, y = colMeans(out),\n    type = \"l\", col = \"black\", lwd = 3,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Infer the direct causal effect of adding food \\(F\\) to a territory on the weight \\(W\\) of foxes. In light of your estmiates from this problem and the previous one, what do you think is going on with these foxes?\n\n\n\nBased on the DAG, to get the direct causal effect of foxes, we also need to stratify by \\(G\\) in our model. So we will fit that model first.\n\nset.seed(100)\nf_on_w_direct &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * F + bG * G,\n        a ~ dnorm(0, 5),\n        bF ~ dnorm(0, 5),\n        bG ~ dnorm(0, 5),\n        sigma ~ dexp(1)\n    ),\n    data = D,\n    control = list(maxit = 500)\n)\nf_on_w_direct |&gt; rethinking::coeftab() |&gt; rethinking::coeftab_plot()\n\n\n\n\n\n\n\n\nOK, now let’s compare the coefficients of the two models.\n\nrethinking::coeftab(f_on_w_total, f_on_w_direct) |&gt;\n    rethinking::coeftab_plot(pars  = c(\"bF\", \"bG\"))\n\n\n\n\n\n\n\n\nSo when we only stratify by \\(F\\), we don’t see an effect. But when we stratify by \\(F\\) and \\(G\\), we see an effect of both variables! The optional problem sort of spoiled this, but it seems like the most likely explanation here is negative confounding by an unobserved variable. When we don’t control for \\(G\\), the confounder still has a backdoor pathway through to \\(W\\), but when we control for \\(G\\) and \\(F\\) simultaneously, the backdoor path is closed and the effect of \\(U\\) will be absorbed into the estimated effects of \\(F\\) and \\(G\\) (as it should be). So this is an example of a masked relationship.\n\n\n\n\n\n\n4. Suppose there is an unobserved confound that influences \\(F\\) and \\(G\\), saying \\(\\boxed{U}\\). Assuming this is the correct DAG, again estimate both the total and direct causal effects of \\(F\\) on \\(W\\). What impact does the unobserved confound have?\n\n\n\nI didn’t finish this question because I didn’t know how, and apparently it is a trick question because it is not possible to get the total causal effect of \\(F\\) (since \\(U\\) is unobserved). That makes me feel better. We would estimate the direct causal effect of \\(F\\) like we already did under this model."
  },
  {
    "objectID": "sr/week1.html",
    "href": "sr/week1.html",
    "title": "2023 Homework, week 1",
    "section": "",
    "text": "This homework covers the material from Lectures 1 and 2, and the content from book Chapters 1, 2, and 3. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions. As of 2023-01-17, I believe my solutions are consistent with the now-released official solutions by McElreath and I do not intend to edit these further.\n\n\n\n\n\n\n1. Suppose the globe tossing data (Lecture 2, Chapter 2) had turned out to be 4 water and 11 land. Construct the posterior distribution.\n\n\n\nFortunately, I already did a very similar problem in the textbook, so I used the function that I already wrote. This assumes a uniform prior distribution for \\(p\\).\n\nglobe_post &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n# Calculate the posterior after assuming there are 4 water observations and\n# 11 land observations\npost &lt;- globe_post(w = 4, l = 11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Using the posterior distribution from 1, compute the posterior predictive distribution for the next 5 tosses of the same globe. I recommend you use the sampling method.\n\n\n\nFirst we construct a set of 10000 samples from the posterior distribution. We sample from the same grid of \\(p\\) values that we used to calculate the posterior distribution, weighting the likelihood of each value by the calculated posterior density.\n\nsamples &lt;- sample(\n    x = seq(from = 0, to = 1, by = 0.01),\n    size = 1e5,\n    prob = post,\n    replace = TRUE\n)\n\nNext we compute the posterior predictive distribution. We draw from the likelihood function (with \\(n = 5\\) to represent the next five samples) using the sampled values of \\(p\\).\n\npost_pred &lt;- rbinom(10000, size = 5, prob = samples)\nrethinking::simplehist(post_pred)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use the posterior predictive distribution from 2 to calculate the probability of 3 or more water samples in the next 5 tosses.\n\n\n\nNow that we have the samples from the posterior predictive distribution, this probability is simple to approximate.\n\nmean(post_pred &gt;= 3)\n\n[1] 0.1789\n\n\nThe probability of three or more water samples in the next 5 tosses is approximately \\(18.28\\%\\), based on our posterior predictive distribution.\n\n\n\n\n\n\n4. Suppose you observe \\(W = 5\\) water points, but you forgot to write down how many times the globe was tossed, so you don’t know the number of land points, \\(L\\). Assume that \\(p = 0.7\\) and compute the posterior distribution of the number of tosses \\(N\\).\n\n\n\nIf we assume that \\(p = 0.5\\) and we observe \\(W = 5\\) water points, then the likelihood as a function of \\(N\\) (unknown) is\n\\[P(N) = \\left( N\\atop{5} \\right) 5^{0.7} (N-5)^{0.3}.\\]\nWe need to choose a prior for \\(N\\) in order to calculate the posterior distribution. We haven’t discussed many different distributions for priors yet, but I’ll choose a Poisson prior because it has the set \\(\\{0, 1, \\ldots, \\infty\\}\\) as its support, so there is no upper limit on \\(N\\) that we have to choose. The Poisson distribution has one parameter, called \\(\\lambda\\), I’ll choose to be \\(5\\) for no other reason than that is the number of throws we know we observed. It would also make sense to choose a uniform prior with some very large upper boundary, say \\(\\mathrm{Uniform}(5, 100)\\). (Or even smaller than that, because we can probably place a relatively small upper bound on the number of times we threw the ball, since we know \\(p = 0.7\\).)\nOther than changing the prior, we calculate this the same way. Since it’s impossible for us to evaluate every \\(N\\) up to \\(\\infty\\), I’ll just calculate up to 25 and that should be plenty.\n\n# Define the grid of points to evaluate\nn_grid &lt;- seq(from = 0, to = 25, by = 1)\n# Poisson prior on n\nprior &lt;- dpois(n_grid, lambda = 5)\n# Compute the likelihood over the grid given the observed sample\nlikelihood &lt;- dbinom(5, size = n_grid, prob = 0.7)\n# Compute the unstandardized posterior\nunstd.posterior &lt;- likelihood * prior\n# Standardize the posterior\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n# Make the plot\nplot(n_grid, posterior, type = \"b\", xlab = \"Number of samples (N)\",\n         ylab = \"Posterior probability\")\nlines(n_grid, prior, type = \"l\", lty = 2)\n\n\n\n\n\n\n\n\nIn this figure, the points are the observed posterior probabilities. The solid line just connects them to give us a better idea of the shape of the distribution. The dashed line shows the Poisson prior on \\(N\\).\nAs we can see in the posterior, it is impossible that we drew less than 5 samples. That’s good, that means that our model reflects reality, which is a good sanity check for our golem. Given that \\(p = 0.7\\) however, it is actually quite plausible that we only tossed the globe 5 times, which could explain why we didn’t write down the number of tosses or the number of land throws – there weren’t any.\nHowever, the MAP estimate would be that we threw the globe 6 times, with 7 times in a close second place. As we get to 10 or higher, it becomes incredibly unlikely that we would throw the ball this many times and only observe 5 water tosses."
  },
  {
    "objectID": "sr/cp8.html",
    "href": "sr/cp8.html",
    "title": "Chapter 8: Conditional Manatees",
    "section": "",
    "text": "library(rethinking)\nlibrary(dagitty)\nlibrary(ggplot2)\nThis chapter is a brief introduction to the concept of conditional inference, focusing on the specific concept of linear interaction in models."
  },
  {
    "objectID": "sr/cp8.html#chapter-notes",
    "href": "sr/cp8.html#chapter-notes",
    "title": "Chapter 8: Conditional Manatees",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nBullet holes in bombers and propeller scars on manatees – both are conditional on survival. This is the motivating example for the chapter.\nAn interaction is a statistical method for modeling interdependence between two features of a model.\nUsing an interaction term in a model is nearly always better than fitting stratified models.\nIn Bayesian models, it’s better to use an index-coding approach and have parameters vary by the level of a categorical variable, rather than using an indicator-coding approach, which makes assigning priors difficult.\nAn interaction is also just a slope which is conditional on another effect – the value of one variable modifies the effect of the other.\nLinear interactions are symmetrical. If variable \\(x\\) interacts with variable \\(y\\), then \\(y\\) interacts with \\(x\\). “There is just no way to specify a simple, linear interaction in which you can say the effect of some variable \\(x\\) depends on \\(z\\), but the effect of \\(z\\) does not depend upon \\(x\\).”\nContinuous interactions are harder to think about as conditional slopes, because we would need an uncountably infinite number of categories. Instead, we can think about interactions as nested linear models.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\beta_S S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i\n\\end{aligned}\n\\]\n\nWe could include nested terms for both variables, but the resultant model has unidentifiable parameters – in the final term below, only the sum \\((\\beta_{WS} + \\beta_{SW})\\) can be estimated.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\gamma_{S,i} S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i \\\\\n\\gamma_{S_i} &= \\beta_{S} + \\beta_{SW} W_i \\\\\n\\therefore \\mu_i &= \\alpha + \\left(\\beta_{W} + \\beta_{WS} S_i\\right)W_i + \\left(\\beta_{S} + \\beta_{SW} W_i\\right) S_i \\\\\n&= \\alpha + \\beta_W W_i + \\beta_S S_i + (\\beta_{WS} + \\beta_{SW}) W_iS_i\n\\end{aligned}\n\\]\n\nThe best way to understand interactions is to plot the predictions at multiple levels of the interacting variables."
  },
  {
    "objectID": "sr/cp8.html#exercises",
    "href": "sr/cp8.html#exercises",
    "title": "Chapter 8: Conditional Manatees",
    "section": "Exercises",
    "text": "Exercises\n\n8E1\n\nBread dough rises because of yeast and temperature. Yeast amount and temperature interact to determine how much the bread dough rises.\nEducation and parents’ education could interact to determine higher income. While people with more education have higher salaries on average, people who are educated and also have educated parents are likely to have even higher average salaries at the same level of individual education.\nGasoline and pressing the accelerator make the car go. If you never press the accelerator, a full tank won’t do anything.\n\n\n\n8E2\nOnly statement one (Caramelizing onions requires cooking over a low heat and making sure the onions don’t dry out) involves an interaction. Both things must be true simultaneously, whereas the effects are independent of each other in the other statements.\n\n\n8E3\nOf course all of these models only make sense if we have a correct way to quantify those variables.\n\n\\(\\text{onion caramelization} = \\alpha + \\beta_1 \\cdot \\text{temperature} + \\beta_2 \\cdot \\text{moisture} + \\gamma_{12} \\cdot \\text{temperature} \\cdot \\text{moisture}\\)\n\\(\\text{car speed} = \\alpha + \\beta_2 \\cdot \\text{number of cylinders} + \\beta_2 \\cdot \\text{fuel injector quality}\\)\n\\(\\text{political beliefs} = \\alpha + \\beta_1 \\cdot \\text{parental beliefs} + \\beta_2 \\cdot \\text{friend beliefs}\\)\n\\(\\text{intelligence} = \\alpha + \\beta_1 \\cdot \\text{sociality} + \\beta_2 \\cdot \\text{manipulable appendages}.\\)\n\n\n\n8M1\nIn the tulips example, we saw that water and shade levels interact to affect tulip blooms. Tulips need both water and shade to produce blooms; at a low-light level, the effect of water decreases because no amount of water can replace the lost light. Similarly, if plants have no water, an adequate amount of sunlight will not produce blooms and might even become harmful.\nIf the hot temperature prevents blooms all together, then the hot temperature would modify the effect of shade, water, and their interaction to all become zero – no amount of shade or water can allow for blooms, and their interaction does not help in this context either.\n\n\n8M2\nThe linear model for the tulips example without heat was \\[\n\\mu_i = \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i.\n\\]\nWe can make all of those terms dependent on \\(H_i\\), the heat treatment, in order to accomplish this.\n\\[\n\\mu_i = \\alpha_{H[i]} + \\beta^{W}_{H[i]} + \\beta^S_{H[i]} + \\gamma^{SW}_{H[i]} S_iW_i.\n\\]\nNow it is possible for these effects to all be zero (or much smaller) if \\(H[i] = 1\\), and have their normal values if \\(H[i] = 0\\). Another way to write this model could be something like \\[\n\\begin{aligned}\n\\mu_i &= \\lambda_i (1 - H_i) \\\\\n\\lambda_i &= \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i\n\\end{aligned}\n\\] where \\(H_i\\) again takes on values of \\(0\\) (cold) and \\(1\\) (hot).\n\n\n8M3\nWe cannot create a data set where the raven population and wolf population have a linear statistical interaction, because a linear statistical interaction has at least two predictors. Here we only have an outcome (the raven population size) and a predictor (the wolf population size). This is more of an example of a differential equations type problem than a statistical interaction. In this model, the raven population size would have to vary with the wolf population size, and we do not know about the functional form of this effect, so an appropriate model would be something like\n\\[\n\\frac{dR}{dt} = f\\left(W(t)\\right),\n\\] where \\(f\\) is a function that takes the wolf population size at time \\(t\\) as an input, and returns the change in the raven population before the next time point.\n\n\n8M4\nWe’ll use the sample model for the tulip blooms without heat from the earlier exercise. The priors used in the chapter were\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Normal}(0, 0.25) \\\\\n\\beta_S &\\sim \\text{Normal}(0, 0.25) \\\\\n\\gamma_{SW} &\\sim \\text{Normal}(0, 0.25) \\\\\n\\end{aligned}\n\\]\nWe want to use new priors that constrain the effect of water to be positive and the effect of shade to be negative. At this point in the book, the distribution we learned about that has to be positive is lognormal, and we can force the effect of shade to be negative by taking the additive inverse of a lognormal prior. Since we know that having more water increases the effect of light (because if a tulip has plenty of water, getting enough sunshine is the new limiting factor on the blooms), we know that having more water should decrease the effect of shade, so we’ll make the interaction negative as well. Lognormal priors can be hard to calibrate, so we’ll adjust the parameters until the prior predictive simulation looks nice. The priors we’ll use are as follows.\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1)\n\\end{aligned}\n\\]\n\nset.seed(370)\n\n# Load the data\ndata(tulips)\nd &lt;- tulips\nd$blooms_std &lt;- d$blooms / max(d$blooms)\nd$water_cent &lt;- d$water - mean(d$water)\nd$shade_cent &lt;- d$shade - mean(d$shade)\n\n# Fit the model and extract the prior samples\nm_8m4 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a + bw * water_cent - bs * shade_cent - bws * water_cent * shade_cent,\n        a ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nprior &lt;- rethinking::extract.prior(m_8m4)\n\n# Plot the prpd\npar(mfrow = c(1, 3))\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = rethinking::rangi2\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    mu &lt;- rethinking::link(\n        m_8m4,\n        data = data.frame(shade_cent = s, water_cent = -1:1),\n        post = prior\n    )\n    for (i in 1:20) lines(-1:1, mu[i, ], col = col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\nI did a few different simulations and ultimately ended up with the prior simulation shown here. The slope priors are regularizing and skeptical, so we think that a smaller effect is more likely a priori – if the effects are large, the data can demonstrate that for us.\n\n\n8H1\nNow we want to add the bed variable to the tulips example, which we’ll denote with \\(B_i\\). We only want to include the bed effect as a main effect, which means we need to have a different intercept for each bed – so our model will assume that each bed can start at a different baseline, but the effects of water, shade, and their interaction, are homogeneous across the beds. This is probably an OK assumption in the context of a controlled greenhouse setting.\nThe model will be as follows.\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal} \\left(\\mu_i, \\sigma\\right) \\\\\n\\mu_i &= \\alpha_{B[i]} + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha_{B[i]} &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{aligned}\n\\]\nThat is, we’ll use the same constrained, regularizing priors, as we did for the previous problem, but we’ll have a separate intercept for each bed. In reality, it would probably be good to have the intercept parameters be correlated as well, but we haven’t gone over that in the book yet. Let’s go ahead and fit the model.\n\nset.seed(370)\n# For the index coding to work, we need a numeric version of the beds.\nd$b &lt;- as.integer(d$bed)\n\n# Fit the model\nm_8h1 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a[b] + bw * water_cent - bs * shade_cent -\n            bws * water_cent * shade_cent,\n        a[b] ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_8h1, depth = 2)\n\n           mean         sd       5.5%     94.5%\na[1]  0.2733479 0.03603110 0.21576327 0.3309326\na[2]  0.3964371 0.03601250 0.33888214 0.4539920\na[3]  0.4091452 0.03601137 0.35159204 0.4666983\nbw    0.2017188 0.02612291 0.15996936 0.2434683\nbs    0.1039759 0.02652450 0.06158467 0.1463672\nbws   0.1312346 0.03272451 0.07893448 0.1835347\nsigma 0.1091610 0.01511214 0.08500884 0.1333131\n\n\nFrom the precis, we can see that the first bed (bed 1, with parameter a[1]) has a lower intercept than the other two beds – maybe this bed is next to a drafty space, is the first bed in the water connection and gets the pipe sludge, or just had less bloomds for some reason. But after we account for the different baselines between beds, the estimates of the parameter effects are similar to the last model, so this should just improve the accuracy of our model predictions for the first bed. We can plot the predictions to see.\n\n# Plot the prpd\npar(mfrow = c(1, 3))\n\ncols &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\")\n\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = cols[d$b[idx]]\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    \n    for (bd in 1:3) {\n        mu &lt;- rethinking::link(\n            m_8h1,\n            data = data.frame(shade_cent = s, water_cent = -1:1, b = bd)\n        )\n        for (i in 1:20) {\n            lines(\n                -1:1, mu[i, ],\n                col = col.alpha(cols[bd], 0.3)\n            )\n        }\n    }\n}\n\nlegend(\"topright\", c(\"Bed a\", \"Bed b\", \"Bed c\"), col = cols, lty = 1)\n\n\n\n\n\n\n\n\nWhile it’s actually quite difficult to make statistical conclusions without multiple replicates (here we have only one measurement per bed per treatment), we can see the clear difference between bed a and the other two beds in the model predictions. However, it also appears that our model predictions may not capture the true effect, as from the observed data it seems plausible that the effects of water and shade vary across beds. We would need actual replicates to be more certain of that, though.\n\n\n8H2\nNow we can compare the models with and without bed using WAIC.\n\nrethinking::compare(m_8m4, m_8h1)\n\n           WAIC        SE    dWAIC      dSE     pWAIC    weight\nm_8h1 -22.20890  9.935022 0.000000       NA 10.099017 0.7754177\nm_8m4 -19.73058 10.459586 2.478319 8.389834  7.390765 0.2245823\n\n\nWe see that the WAIC is smaller for the model with bed included, although the difference is small. This implies that adding the bed variable as a main effect increases the accuracy of our posterior predictions, although the improvement is not spectacular. As we saw by looking at summaries of the posterior distribution in the previous exercise, the difference in the estimated intercept for bed A vs. bed b and bed c, without any major changes in the estimates of the slope parameters, should account for this difference.\n\n\n8H3\nFor this question, we’ll focus on the ruggedness data.\n\ndata(\"rugged\")\n# Repeating data processing steps from the book\nd &lt;- rugged\nd$log_gdp &lt;- log(d$rgdppc_2000)\ndd &lt;- d[complete.cases(d$rgdppc_2000), ]\ndd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp)\ndd$rugged_std &lt;- dd$rugged / max(dd$rugged)\n\ndd$cid &lt;- ifelse(dd$cont_africa == 1, 1, 2)\n\nNow we need to recreate model m8.5 from the chapter. Well, at least that’s what the book says to do, but model m8.5 is about the tulips example, so we’ll recreate m8.3 instead.\n\nm8.3 &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dnorm(mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3, depth = 2)\n\n            mean          sd        5.5%       94.5%\na[1]   0.8865629 0.015675157  0.86151094  0.91161480\na[2]   1.0505698 0.009936261  1.03468975  1.06644988\nb[1]   0.1325055 0.074201996  0.01391637  0.25109461\nb[2]  -0.1425764 0.054747543 -0.23007353 -0.05507924\nsigma  0.1094903 0.005934777  0.10000535  0.11897519\n\n\nWe got similar results to what’s in the book, which is good. Now we want to examine the model with PSIS to determine if the Seychelles are influential on the estimation of parameters for the Africa group.\n\nset.seed(370)\nm8.3_psis &lt;- rethinking::PSIS(m8.3, pointwise = TRUE, n = 20000)\nrownames(m8.3_psis) &lt;- dd$isocode\npsis_sort &lt;- m8.3_psis[order(m8.3_psis$k, decreasing = TRUE), ]\npsis_sort |&gt; head()\n\n          PSIS       lppd    penalty  std_err         k\nLSO -1.1417140  0.5708570 0.31929940 15.27576 0.4467221\nSYC  1.3274852 -0.6637426 0.63043059 15.27576 0.3947676\nCHE  2.8274338 -1.4137169 0.46756727 15.27576 0.3148571\nTJK  0.4998540 -0.2499270 0.30731105 15.27576 0.2365403\nGNQ  3.3713051 -1.6856526 0.21035122 15.27576 0.2249513\nMUS  0.8394823 -0.4197411 0.08626176 15.27576 0.1688589\n\n\nThe most influential country on the model fit, judging by the Pareto \\(k\\) values, are Lesotho and the Seychelles, which are both highly rugged nations in Africa.\n\npar(mfrow = c(1, 1))\ndd_sorted &lt;- dd[order(m8.3_psis$k, decreasing = TRUE), ]\nplot(\n    dd_sorted$rugged_std, psis_sort$k,\n    xlab = \"Ruggedness as prop. of maximum\",\n    ylab = \"PSIS Pareto k value\"\n)\n\n\n\n\n\n\n\n\nWe can see that highly rugged nations have the largest Pareto \\(k\\) values, indicating that they are the most influential variables. We also know that these values have a high leverage in a linear regression model, so that makes sense.\nNow that we know these nations with high ruggedness are having an oversized effect on the estimated trend, we can try to use robust regression to lower their influence. We’ll use the same model, but with a Student’s \\(t\\) distribution likelihood (with 2 d.f.) instead of a Normal likelihood. Personally I prefer 3 degrees of freedom (the variance of the distribution is infinite if the d.f. is not larger than 2, which is a prior belief that never makes sense in a physical context to me), but for now I’ll do what the textbook says.\n\nm8.3_r &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dstudent(nu = 2, mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3_r, depth = 2)\n\n             mean         sd        5.5%       94.5%\na[1]   0.86259888 0.01614146  0.83680170  0.88839606\na[2]   1.04577255 0.01097134  1.02823823  1.06330688\nb[1]   0.11241664 0.07503557 -0.00750470  0.23233797\nb[2]  -0.21378054 0.06352620 -0.31530767 -0.11225341\nsigma  0.08451473 0.00673094  0.07375738  0.09527207\n\n\nWe can see by comparing the two model summaries that a few of the parameters are slightly different. Let’s now compare the models using PSIS.\n\nset.seed(12312)\nrethinking::compare(m8.3, m8.3_r, func = PSIS, n = 20000)\n\n            PSIS       SE    dPSIS      dSE    pPSIS       weight\nm8.3   -258.8088 15.27974  0.00000       NA 5.318484 1.000000e+00\nm8.3_r -221.8080 18.11559 37.00089 5.884289 5.760308 9.233328e-09\n\n\nHere, we can see that the non-robust model actually appears to be giving us worse predictions than the non-robust model. However, we know that PSIS is just a measure of predictive performance, so it’s possible that our robust model is still a better conceptual model that provides more accurate inferences at the cost of appearing to underfit the data. Since the two models have every similar numbers of parameters, predictive accuracy criteria are likely to be more sensitive to this kind of “underfitting”, when we actually know outside of the statistics world that we’re reducing the impact of outlying values.\nAnyways, we can also look at the individual pareto \\(k\\) values for the new model.\n\nset.seed(370)\nm8.3r_psis &lt;- rethinking::PSIS(m8.3_r, pointwise = TRUE, n = 20000)\nrownames(m8.3r_psis) &lt;- dd$isocode\npsis_sort_r &lt;- m8.3r_psis[order(m8.3r_psis$k, decreasing = TRUE), ]\npsis_sort_r |&gt; head()\n\n         PSIS      lppd     penalty std_err          k\nEST -2.787885 1.3939424 0.007899841 18.1216 0.13309736\nLSO -1.553968 0.7769838 0.264018936 18.1216 0.11825246\nALB -2.720143 1.3600717 0.013599882 18.1216 0.11215073\nATG -2.777583 1.3887917 0.008395917 18.1216 0.10169758\nUGA -2.744650 1.3723251 0.010101504 18.1216 0.09032803\nBEN -2.552244 1.2761221 0.019180005 18.1216 0.08104976\n\n\nNow we can see that the pareto \\(k\\) values are all much lower. While Lesotho still appears in the top 6, it is overall much less influential, and Seychelles no longer appears in the top 6.\n\n\n8H4\nFor this problem, we’ll use the nettle data to examine the hypothesis that higher food security leads to a higher language diversity in a region.\nFirst we need to construct the outcome variable.\n\ndata(nettle)\nd &lt;- nettle\nd$lang.per.cap &lt;- d$num.lang / d$k.pop\n\n# The log of this will be our actual outcome variable\nd$log.lang.per.cap &lt;- log10(d$lang.per.cap)\n\n# Center the outcome\nybar &lt;- mean(d$log.lang.per.cap)\nd$std.log.lang.per.cap &lt;- d$log.lang.per.cap - ybar\n\n# We also need the log of the area\nd$log.area &lt;- log10(d$area)\n\nSince I don’t really know anything about this problem other than what the textbook tells me, I’ll follow the specified steps. The effects we want to evaluate here are the effects of mean.growing.season, which we’ll call \\(M\\), and sd.growing.season, on our model. We also need to consider \\(A\\), the log of area, as a potential cause. I’m not sure how we would work in the number of weather stations in our model, so for now we’ll leave that alone – although there is a noticeable trend in the data that as the number of measurement stations increased, so did the SD of the growing season length. So in a real academic paper, we would definitely need to think about how to model that.\nFirst let’s look at the bivariate relationship between each of these values and the outcome.\n\nlayout(matrix(c(1, 2, 3), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.lang.per.cap,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 number of langauges per capita\"\n)\nplot(\n    d$sd.growing.season, d$log.lang.per.cap,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\nplot(\n    d$log.area, d$log.lang.per.cap,\n    xlab = \"log10 area of country (sq. km.)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nIn general, none of these trends looks particularly strong, although there appear to be some trends with the growing season variables.\nNow, let’s try to fit a simple model that models the log languages per capita based on the mean length of the growing season. We’ll use a normal likelihood since the outcome variable is on the log scale. Since we aren’t using a count model, which would naturally constrict the domain of the outcome variable, I also chose to center the outcome variable before modeling to make assigning a prior for the intercept feasible.\nFor the intercept, we’ll use a generic prior centered at 0 (the mean after standardization). For the effect of the mean growing season length, we’ll use a regularizing, skeptical prior centered around 0.\n\n# Set seed for all of our models in this section\nset.seed(370)\nm_mgs &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs)\n\n             mean         sd        5.5%      94.5%\na      -0.5310682 0.17466211 -0.81021195 -0.2519244\nb_mgsl  0.0754357 0.02267654  0.03919421  0.1116772\nsigma   0.6095826 0.04980052  0.52999173  0.6891734\n\n\nWe can see from the summary that there is a small prior effect of mean growing season length on the outcome. Now, let’s check whether the area should be a coefficient in this model as well.\n\nm_mgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs_a)\n\n             mean         sd        5.5%      94.5%\na       0.6533186 0.83024907 -0.67357977 1.98021696\nb_mgsl  0.0629484 0.02394189  0.02468463 0.10121217\nb_area -0.1952263 0.13386781 -0.40917288 0.01872034\nsigma   0.6009410 0.04909953  0.52247048 0.67941155\n\n\nThe effect of the area variable is negative and a large amount of the density lies below zero, which suggests that the area could be a real effect that we need to control for. Controlling for the area size also affects out estimate of the effect of mean growing season length. Let’s compare the models via WAIC and see if the area improves posterior predictions.\n\nrethinking::compare(m_mgs, m_mgs_a)\n\n            WAIC       SE     dWAIC      dSE    pWAIC   weight\nm_mgs   144.5949 15.49001 0.0000000       NA 3.778308 0.539875\nm_mgs_a 144.9146 16.01579 0.3196787 3.673048 5.049032 0.460125\n\n\nThe WAICs are extremely similar, and the WAIC for the model without area is slightly better, so I don’t think we need area in this model. If we think about the problem casually, I don’t understand why the area would be a confounder or a collider in this situation, because the area doesn’t casually determine the mean growing season length in a country. However, larger countries should have more variation in the growing season length (since larger area overall means they can cover more areas of varying latitude). But, I think that a larger area should mean there is more room for multiple communities to exist and become isolated, so a larger country should also have more languages on average.\nSo I think the DAG should look something like this.\n\nlayout(c(1))\ndag &lt;- dagitty::dagitty(\n    'DAG {\n    \"languages\" &lt;- \"mean length\"\n    \"languages\" &lt;- \"SD length\" &lt;- \"area\"\n    \"languages\" &lt;- \"area\"\n    }'\n)\ncoordinates(dag) &lt;- list(\n    x = c(\"languages\" = 0, \"mean length\" = 1, \"SD length\" = -1, \"area\" = -1),\n    y = c(\"languages\" = 0, \"mean length\" = 0, \"SD length\" = 0.5, \"area\" = -0.5)\n)\nplot(dag)\n\n\n\n\n\n\n\n\nSo, in our final model we’ll need to include area anyways, so we might as well leave it in there for now.\nNext we want to examine the effect of the SD of growing season length on languages. The area variable is a confounder in this casual structure, so we need to include that as well to avoid getting a biased estimate.\n\nm_sgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_sgs_a)\n\n              mean         sd       5.5%      94.5%\na       1.43691137 0.79286148  0.1697656 2.70405716\nb_sgsl -0.09330776 0.07997807 -0.2211282 0.03451264\nb_area -0.22761855 0.15173599 -0.4701220 0.01488487\nsigma   0.62213062 0.05082317  0.5409054 0.70335587\n\n\nWe see that the SD of growing season length does appear to have a negative effective on the overall number of languages. We cannot rule out entirely the lack of an effect (assuming our causal structure is correct and linear models are appropriate), but there is likely to be a negative effect of the SD of growing length on number of languages.\nSo far we’ve seen that an average longer growing season leads to more languages per capita, meaning that more food abundance leads to smaller, more isolated social groups and the development of more languages. However, higher variation in the growing season length leads to lower languages per capita, suggesting the need to form larger social networks for insurance against short growing seasons. In both models, we saw a negative effect of area, indicating that as a country becomes larger, the number of languages becomes smaller, which is the opposite of what I would have thought. Perhaps larger countries, on average, have shorter growing seasons? We can examine that effect quickly.\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.area,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 area of country (sq. km.)\"\n)\nplot(\n    d$sd.growing.season, d$log.area,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nInterestingly, we can see slight patterns in both trends. Larger countries seem to have slightly smaller average growing seasons, and more uncertainty in their growing seasons. I think the effect on the mean length of the growing season is quite small, and is likely to not be causal, although we could postulate that larger countries tend to cover more sparsely inhabited territory which has a shorter growing season. There is a definite trend in the standard deviation of the growing season though – it’s hard to tell if this is an effect of covering more latitude areas, or a function of a number of measuring systems. We would need a variable on the range of latitude covered by each country to disentangle those effects.\nAnyways, now we can fit the main model. We’ll fit two models that include all three variables. One will include just main effects, and the other will include an interaction between the effects of mean and SD of growing season length. Then we can compare those models.\n\nm_noint &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_noint)\n\n              mean         sd        5.5%       94.5%\na      -0.19725369 0.91289412 -1.65623482  1.26172743\nb_mgsl  0.07583973 0.02418582  0.03718613  0.11449334\nb_sgsl -0.15778988 0.07809425 -0.28259957 -0.03298018\nb_area -0.01221519 0.15896098 -0.26626554  0.24183516\nsigma   0.58530486 0.04782868  0.50886538  0.66174433\n\n\nAfter including both the mean and SD of growing season length in the model, the effect of area goes away. This suggests that the effect of area on the number of languages per capita is completely explained by the effect of the growing season – we have no evidence here for a direct causal effect of area, meaning that more room for expansion doesn’t say anything about the number of languages we expect to see, unless we know how habitable the land is first. In this model, we see a positive effect of the mean and negative effect of the SD as we expect. Now we can look at a possible interaction.\n\nm_int &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_intr * mean.growing.season * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_intr ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_int)\n\n               mean         sd        5.5%       94.5%\na      -0.607666693 0.90104859 -2.04771636  0.83238298\nb_mgsl  0.128691044 0.03267891  0.07646384  0.18091825\nb_sgsl  0.179426213 0.16417241 -0.08295301  0.44180543\nb_intr -0.046560229 0.02012491 -0.07872372 -0.01439674\nb_area -0.007638564 0.15385114 -0.25352240  0.23824527\nsigma   0.565320615 0.04620557  0.49147519  0.63916604\n\n\nIt certainly looks like the results are different, which is qualitatively important to understand. But let’s first check the WAIC to get an idea of how much better our interaction model is doing.\n\nrethinking::compare(m_noint, m_int)\n\n            WAIC       SE    dWAIC      dSE    pWAIC    weight\nm_int   139.8943 16.19209 0.000000       NA 6.821918 0.8329129\nm_noint 143.1071 16.09151 3.212828 4.717323 6.073994 0.1670871\n\n\nOK, it’s only a bit better in terms of predictive accuracy, but the estimates are so different that we need to try and understand what’s going on here.\nWhen we include an interaction term, the effect of the standard deviation on its own largely goes away, and the effect of the negative is negative with almost all of the probability mass below 0. This suggests that by itself, the SD is not important for determining the number of languages – we need to know the mean first. I think we need to make a plot to really understand this effect.\n\n# Plot the prpd\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\n\nsd_vals &lt;- c(0, 1.5, 3, 4.5)\nsample_n &lt;- 500\nncolors &lt;- 10\ncols &lt;- viridisLite::plasma(ncolors)\nrank &lt;- as.factor(as.numeric(cut(d$sd.growing.season, ncolors)))\n\nfor (i in 1:length(sd_vals)) {\n    sd &lt;- sd_vals[i]\n    idx &lt;- which(dplyr::between(d$sd.growing.season, sd - 1.5, sd + 1.5))\n    plot(\n        #d$mean.growing.season[idx],\n        #d$log.lang.per.cap[idx],\n        NULL, NULL,\n        xlim = c(0, 12),\n        ylim = 10 ^ c(-4, 0),\n        xaxs = \"i\",\n        yaxs = \"i\",\n        xlab = \"Mean growing season length (months)\",\n        ylab = \"Number of languages per capita\",\n        pch = 16,\n        log = \"y\"\n        #col = cols[rank[idx]]\n    )\n    mtext(paste0(\"SD of growing season = \", sd))\n    \n    # Calculate the posterior values\n    post_data &lt;- expand.grid(\n        mean.growing.season = seq(0, 12, 0.1),\n        sd.growing.season = sd,\n        log.area = mean(d$log.area)\n    )\n    \n    mu &lt;- rethinking::link(m_int, post_data, n = sample_n)\n    \n    for (i in 1:sample_n) {\n        lines(\n            seq(0, 12, 0.1), 10 ^ (mu[i, ] + ybar),\n            col = col.alpha(\"black\", 0.1)\n        )\n    }\n}\n\n\n\n\n\n\n\n\nFrom the posterior predictions, we can understand the effect of the interaction a lot easier. The posterior predictions shown all use the average value of the log land area, but the main thing we want to understand here is the qualitative way the effect changes. When the SD value is small (the points shown in the top right panel are from 0 to 1.5), there is little variation in the length of the growing season, and the effect of the average length of the growing season on language diversity is positive. However, as the variation in growing season length increases, the effect becomes smaller and then negative, indicating that for highly variable areas, there is little effect of the average growing season on language diversity. For extremely variable areas, a long growing season may even lead to less language diversity, but we do not have enough data to say that conclusively.\n\n\n8H5\nFor this exercise, we’ll build a model using the Wines2012 dataset.\n\ndata(\"Wines2012\")\nd &lt;- Wines2012\n\nIt looks like the wines are scores out of 20, so normally I would recommend a binomial (or beta-binomial) model here. But we don’t know that for sure and we haven’t learned that yet, so we’ll standardize the scores and model the \\(z\\)-scores instead. This is a fairly interesting problem even with the small amount of data we have, and I imagine will make quite an interesting multilevel modeling problem later in the book.\nFor this question, we just need to include effects of the judge and of the wine.\n\ndd &lt;- data.frame(\n    y = rethinking::standardize(d$score),\n    j = rethinking::coerce_index(d$judge),\n    w = rethinking::coerce_index(d$wine),\n    wine_amer = d$wine.amer,\n    judge_amer = d$judge.amer,\n    red = as.integer(ifelse(d$flight == \"red\", 1, 0))\n)\ndplyr::glimpse(dd)\n\nRows: 180\nColumns: 6\n$ y          &lt;dbl&gt; -1.57660412, -0.45045832, -0.07507639, 0.30030555, -2.32736…\n$ j          &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,…\n$ w          &lt;int&gt; 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 1, 3, 5, 7, 9, 11, 13, 1…\n$ wine_amer  &lt;int&gt; 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,…\n$ judge_amer &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ red        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nFirst we can try to visualize the data. It’s a bit difficult because we have two categorical variables and one continuous (perfect two-way anova design), but we can make two plots to try and see what’s going on.\n\nd$j2 &lt;- rethinking::coerce_index(d$judge) |&gt; factor()\np1 &lt;-\n    ggplot(d) +\n    aes(x = wine, y = score) +\n    geom_point() +\n    facet_wrap(~j2) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 8, angle = 90))\n\np2 &lt;-\n    ggplot(d) +\n    aes(x = j2, y = score) +\n    geom_point() +\n    facet_wrap(~wine) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 10)) + labs(x = \"judge\")\n\ncowplot::plot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nOf course with data like these we just kind of have to eyeball them and make qualitative guesses as to what might be going on. But In general we can see there are some nicer judges (5, 3), and some meaner judges (4, 9), and one judge that got meaner as they tried more wines (8). However, most of the wines look fairly similar with the exception of a few like C2 and I2 that appeared to get worse bad reviews. I guess if we were following Agresti’s Categorical Data Analysis the next thing to do would be to get marginal and conditional mean scores, but we don’t need to do that now, we can start fitting models (which does that in an easier way, more or less).\nWe’ll use (as usual) a normal likelihood – a \\(t\\)-likelihood might be better if some of our judges or wines give outlying scores, but for now we’ll ignore that possibility. Next we need to assign priors. Fortunately for us this is easy and I’ll just assign a typical normal prior, I’m not too sure why it really needs to be justified. So let’s go ahead and fit the first model.\n\nset.seed(12312)\nm_w1 &lt;- rethinking::quap(\n    flist = alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- a_w[w] + a_j[j],\n        a_w[w] ~ dnorm(0, 2),\n        a_j[j] ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\n\nlayout(matrix(c(1, 2), nrow = 1))\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_w[\", 1:20, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Wine parameters\")\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_j[\", 1:9, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Judge parameters\")\n\n\n\n\n\n\n\n\nLike we thought from looking at the plots, the judges appear to be more different from each other than the wines are. We probably also want to know about how the judges and wines interact, but since we don’t have any replicates, we can’t really get good answers for that question, all we can do is look at the score in each cell.\n\nggplot(d) +\n    aes(x = wine, y = judge, fill = score) +\n    geom_tile() +\n    scale_fill_viridis_c(breaks = c(7, 10, 15, 19), limits = c(7, 19.5)) +\n    hgp::theme_ms() +\n    guides(fill = guide_colorbar(barwidth = 15))\n\n\n\n\n\n\n\n\n\n\n8H6\nNow instead of looking at the variability across judges and across wines, we want to try and use the characteristics of the judges and wines to understand the scores. For this problem, we won’t include any interactions. Again, we’ll use standard priors, and the variables we’ll include as main effects are flight (whether the wine is red or white), wine.amer (if the wine was made in America), and judge.amer (whether the judge is American).\nFor whatever reason, the book says to use indicator coding for this problem and the next problem (coding interactions between two categorical variables is annoying somehow, I tried it and couldn’t figure it out). So we’ll do that.\n\nm_w2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w2)\n\n              mean         sd        5.5%      94.5%\na     -0.020886459 0.15873255 -0.27457173 0.23279881\na_wa  -0.191038758 0.14889489 -0.42900156 0.04692404\na_ja   0.247752111 0.14683135  0.01308725 0.48241697\na_red -0.004204578 0.14595115 -0.23746271 0.22905355\nsigma  0.982341917 0.05156316  0.89993403 1.06474980\n\n\nOverall, the red and white wines were judged similarly without a large discrepancy between groups. American judges tended to be more generous, and American wines tended to be rated slightly worse. However, we have a great deal of uncertainty about all of these parameters.\n\n\n8H7\nApparently doing the interactions IS the reason for using indicator coding here, quap I guess can’t handle all the stuff that Stan can. Now we want to include all of the third-level interactions that we can make.\n\nm_w3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red +\n                # American wine/american judge\n                g_wawj * wine_amer * judge_amer +\n                # American red wines\n                g_rwa * wine_amer * red +\n                # Red wines / american judge\n                g_rja * judge_amer * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            g_wawj ~ dnorm(0, 2),\n            g_rwa ~ dnorm(0, 2),\n            g_rja ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w3, depth = 2)\n\n              mean         sd       5.5%      94.5%\na      -0.21373267 0.21651035 -0.5597580  0.1322927\na_wa    0.15272583 0.26069131 -0.2639092  0.5693609\na_ja    0.29244412 0.26673524 -0.1338503  0.7187386\na_red   0.31208633 0.27442703 -0.1265011  0.7506737\ng_wawj -0.11070543 0.29175737 -0.5769901  0.3555792\ng_rwa  -0.56956098 0.29026516 -1.0334608 -0.1056612\ng_rja   0.04190135 0.28649695 -0.4159761  0.4997788\nsigma   0.97130887 0.05098788  0.8898204  1.0527973\n\n\nUsually I hate looking at tables, but this one is not too bad because the trend is pretty obvious. All of the parameters are zero-ish (lots of probably mass on either side of 0), except for one, which is g_rwa. This is the interaction term between American wines and red wines – so it looks like American red wines were much worse on average than non-american and/or non-red wines.\nIn the solutions guide, Richard actually gives a good reason for using more skeptical priors for the interaction terms than on the main effect terms. I need to remember that for the future."
  },
  {
    "objectID": "sr/cp6.html",
    "href": "sr/cp6.html",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "",
    "text": "This chapter discusses three common pitfalls that can lead our statistical models to misbehave and make our causal interpretations difficult or incorrect. The three major topics are collider bias (selection-distortion), multicollinearity in regression models, and post-treatment bias. The chapter further expands on the idea of DAGs as graphical causal models that was introduced in the previous chapter."
  },
  {
    "objectID": "sr/cp6.html#chapter-notes",
    "href": "sr/cp6.html#chapter-notes",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nThe Selection-distortion effect (AKA Berkson’s/Berksonian bias, generalized to the idea of collider bias) occurs when the selection of a sample changes the relationship between the observed variables. (I.e. there is/isn’t a relationship between the two variables on the sample, but in the larger population, there isn’t/is a relationship.) Berkson’s bias refers to the particular effect that when selecting from a population on two desirable traits, there often appears to be a negative correlation between the desirable traits in the selected sample.\nMulticollinearity refers to a very strong association between two or more predictor variables, conditional on the other variables in the model. When variables are multicollinear, the posterior distribution will seem to suggest that none of the multicollinear variables are truly associated with the outcome, even if the reality is that they are all strongly associated.\nPost-treatment bias: a form of included variable bias where variables that are also causal descendents of the treatment, are controlled for when assessing the response. That is, you measure something that is not the outcome of interest and is also affected by the treatment, and you adjust for that quantity when analyzing the outcome. This induces collider bias.\nControlling for a collider on a DAG induces D-separation, meaning that the DAG is no longer connected.\nWhen you condition on a collider (a common descendent), it creates statistical, although not necessarily causal, relationships between the ancestors.\nEven unmeasured causes can induce collider bias. Selection bias in a study can often be interpreted as conditioning on a collider during the sampling process. See the parents and grandparents example in section 6.3.2.\nThere are four types of elemental confounds: DAG structures that allow us to determine causal and non-causal pathways.\n\nThe fork: two variables have a common cause (Z -&gt; X; Z -&gt; Y).\nThe pipe: one variable is intermediate in the causal relationship between two others (X -&gt; Z -&gt; Y).\nThe collider: two variables have a common descenent (X -&gt; Z; Y -&gt; Z).\nThe descendant: a variable which descends from another, capturing part of the ancestor’s statistical signal (in the previous example, if we also have Z -&gt; D, D will appear to be a collider as well, even if it is just a descendant).\n\nEvery DAG is built out of these four types of relationships, and we can use specific rules for DAGs to determine what variables need to be included in models for a causal effect.\n“Multiple regression is no oracle, but only a golem.”"
  },
  {
    "objectID": "sr/cp6.html#exercises",
    "href": "sr/cp6.html#exercises",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "Exercises",
    "text": "Exercises\n\n6E1\nThree mechanisms that can produce false inferences about causal effects in a multiple regression model are: multicollinearity, the selection- distortion effect, and post-treatment bias.\n\n\n6E2\nFor an example of post-treatment bias, consider a vaccine efficacy trial for influenza (or I guess any disease). Suppose we have a known surrogate of protection, an immunological measurement that is strongly associated with protection from the disease. For influenza, one potential biomarker is hemagglutinin inhibition (HI) titer, which is typically measured before and after (around 21 to 28 days) vaccination. If one did a challenge study or a long followup period of surveillance, we can record which individuals are ultimately infected with influenza. Including participants’ HI titers before vaccination when modeling vaccine protection is OK depending on the context, but including participants HI titer after vaccination would induce post-treatment bias, because vaccination directly affects HI titer.\n\n\n6E3\n\nThe fork (Z -&gt; X; Z -&gt; Y): X ⫫ Y | Z\nThe pipe (X -&gt; Z -&gt; Y): X ⫫ Y | Z\nThe collider (X -&gt; Z; Y -&gt; Z): X ⫫ Y; X !⫫ Y | Z\nThe descendent: conditional independencies are the same as the parent\n\n\n\n6E4\nSuppose we have two variables (call one the exposure and the other the outcome) which are both causes of a third variable. If that third variable determines which observations we observe (for example, a restaurant existing or a patient agreeing to participate in a study), in our observed sample we will see a correlation between the exposure and the outcome, just because we are only seeing observations where the third variable is already specified.\nIn the funded grants example, a funded grant must be high in at least one of newsworthiness or trustworthiness, otherwise it will not be funded. If we could see all grants, we would not see a correlation between newsworthiness and trustworthiness. But when we only look at funded grants, we condition on a common descendant of both variables (a collider), which makes a spurious relationship appear."
  },
  {
    "objectID": "sr/cp6.html#m1",
    "href": "sr/cp6.html#m1",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M1",
    "text": "6M1\nThe new DAG including \\(V\\) as an unobserved common cause of \\(C\\) and \\(Y\\) looks like this.\n\ndag_6m1 &lt;- dagitty::dagitty(\n    \"dag {\n        U [unobserved]\n        V [unobserved]\n        X -&gt; Y\n        X &lt;- U &lt;- A -&gt; C -&gt; Y\n        U -&gt; B &lt;- C\n        C &lt;- V -&gt; Y\n    }\"\n)\ndagitty::coordinates(dag_6m1) &lt;-\n    list(\n        x = c(U = 1, V = 4, X = 1, Y = 3, A = 2, B = 2, C = 3),\n        y = c(U = 1.5, V = 2.5, X = 3, Y = 3, A = 1, B = 2, C = 1.5)\n    )\ndagitty::exposures(dag_6m1) &lt;- \"X\"\ndagitty::outcomes(dag_6m1) &lt;- \"Y\"\nplot(dag_6m1)\n\n\n\n\n\n\n\n\nWe still have all of the same paths from the previous example, i.e.:\n\n\\(X \\to Y\\),\n\\(X \\leftarrow U \\leftarrow A \\to C \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\to Y\\).\n\nThe first path is the direct path. The second is an open backdoor path through \\(A\\). The third is a closed backdoor path, as it passes through the collider \\(B\\). By adding the unobserved confounder \\(V\\), we create two new backdoor paths,\n\n\\(X \\leftarrow U \\leftarrow A \\to C \\leftarrow V \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\leftarrow V \\to Y\\).\n\nThe first path is an open backdoor path, and the second path is a closed backdoor path. We can check which paths exist and are open with dagitty.\n\ndagitty::paths(dag_6m1)\n\n$paths\n[1] \"X -&gt; Y\"                     \"X &lt;- U -&gt; B &lt;- C -&gt; Y\"     \n[3] \"X &lt;- U -&gt; B &lt;- C &lt;- V -&gt; Y\" \"X &lt;- U &lt;- A -&gt; C -&gt; Y\"     \n[5] \"X &lt;- U &lt;- A -&gt; C &lt;- V -&gt; Y\"\n\n$open\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\nNow we need to close the two open paths, without opening either of the closed paths. Conditioning on \\(C\\) would close both of the open paths, but would also open the fifth path. However, conditioning on \\(A\\) will close both open paths without opening either of the closed paths. So \\(\\{A\\}\\) is our sufficient adjustment set. We can verify this with dagitty.\n\ndagitty::adjustmentSets(dag_6m1)\n\n{ A }"
  },
  {
    "objectID": "sr/cp6.html#m2",
    "href": "sr/cp6.html#m2",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M2",
    "text": "6M2\nFirst we’ll do the simulation: we want \\(X\\) and \\(Z\\) to be highly correlated.\n\nset.seed(101)\nX &lt;- rnorm(100, 0, 1)\nZ &lt;- rnorm(100, X, 0.5)\nY &lt;- rnorm(100, Z, 1)\n\ncor(cbind(X, Z, Y))\n\n          X         Z         Y\nX 1.0000000 0.8925919 0.6779960\nZ 0.8925919 1.0000000 0.7998096\nY 0.6779960 0.7998096 1.0000000\n\n\nWe can see that all of the variables are strongly associated, but \\(X\\) and \\(Z\\) have a particularly strong correlation. But now we want to use a model that adjusts for both.\n\nfit_6m1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            Y ~ dnorm(mu, sigma),\n            mu &lt;- a + bX * X + bZ * Z,\n            a ~ dnorm(0, 0.5),\n            c(bX, bZ) ~ dnorm(0, 1),\n            sigma ~ dexp(1)\n        ),\n        data = list(X = X, Y = Y, Z = Z)\n    )\n\ncoeftab(fit_6m1) |&gt;\n    coeftab_plot(pars = c(\"bX\", \"bZ\"))\n\n\n\n\n\n\n\n\nInterestingly, we can see that we do not get the same problem as the previous multicollinearity example. The confidence intervals appear to be reasonable, and we see a strong effect of \\(Z\\) but no effect of \\(Y\\). Intuitively, this make sense – \\(Z\\) and \\(Y\\) have a stronger correlation than \\(X\\) and \\(Y\\), so after we control for \\(Z\\), the model “finds” all of the signal, and then does not find an effect of \\(X\\). So if we interpreted this model without considering the causal framework, we would still be mislead by the multicollinearity, but there is nothing obviously wrong – the entire causal effect of \\(X\\) on \\(Z\\) is through \\(Z\\), so this estimate of the direct causal effect of \\(X\\) makes sense."
  },
  {
    "objectID": "sr/cp6.html#m3",
    "href": "sr/cp6.html#m3",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6M3",
    "text": "6M3\nHere are the adjustment sets for each of the DAGs shown.\n\nTop left DAG: Z only\nTop right: nothing\nBottom left: nothing\nBottom right: A only\n\nI also checked using dagitty to verify my answers.\n\ndag1 &lt;- dagitty::dagitty(\"dag {Z -&gt; X -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag2 &lt;- dagitty::dagitty(\"dag {X -&gt; Z -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag3 &lt;- dagitty::dagitty(\"dag {X -&gt; Y -&gt; Z; A -&gt; X -&gt; Z; A -&gt; Z}\")\ndag4 &lt;- dagitty::dagitty(\"dag {A -&gt; X -&gt; Z; A -&gt; Z -&gt; Y; X -&gt; Y}\")\n\nlapply(list(dag1, dag2, dag3, dag4), dagitty::adjustmentSets, \"X\", \"Y\")\n\n[[1]]\n{ Z }\n\n[[2]]\n {}\n\n[[3]]\n {}\n\n[[4]]\n{ A }"
  },
  {
    "objectID": "sr/cp6.html#h1",
    "href": "sr/cp6.html#h1",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H1",
    "text": "6H1\nUsing the Waffle House data, we want to find the total causal influence of number of Waffle Houses on divorce rate. First, let’s look at what we have to work with.\n\ndata(\"WaffleDivorce\")\nhead(WaffleDivorce)\n\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\n\n\nOK, so of course we will assume that there is a direct causal effect of number of Waffle Houses (\\(W\\)) on divorce rate (\\(D\\)). From previous work, we know our data are consistent with a \\(M \\to A \\to D\\) DAG structure, for \\(M\\) the marriage rate and \\(A\\) the median age at marriage, so we’ll incorporate this into our DAG. We also saw that the data are consistent with being in the South affecting \\(M\\) and \\(A\\), so we’ll include that in our DAG, and of course we expect to see \\(S \\to W\\). Finally, since we’re trying to find the total causal effect of \\(W\\), we’ll include \\(A \\leftarrow W \\rightarrow M\\) as a sub-DAG as well. Putting it all together, our DAG looks like this.\n\ndag_6h1 &lt;-\n    dagitty::dagitty(\n        \"dag {\n        M -&gt; A -&gt; D\n        M &lt;- S -&gt; A\n        S -&gt; W\n        W -&gt; D\n        M &lt;- W -&gt; A\n        }\"\n    )\ndagitty::exposures(dag_6h1) &lt;- \"W\"\ndagitty::outcomes(dag_6h1) &lt;- \"D\"\ndagitty::coordinates(dag_6h1) &lt;-\n        list(\n        x = c(S = 1, W = 2, D = 2.7, M = 1, A = 2),\n        y = c(S = 1, W = 1, D = 1.5, M = 2, A = 2)\n    )\n\nplot(dag_6h1)\n\n\n\n\n\n\n\n\nLet’s now figure out what needs to be in our model.\n\ndagitty::adjustmentSets(dag_6h1)\n\n{ S }\n\n\nWe see that to get the total cause effect of \\(W\\) on \\(D\\), we need only adjust for \\(S\\), being in the South. I should probably worry about things like transformations and zero-inflation, but for this exercise I am not going to do that.\n\nwd &lt;- \n    list(\n        W = standardize(WaffleDivorce$WaffleHouses),\n        D = standardize(WaffleDivorce$Divorce),\n        S = WaffleDivorce$South + 1\n    )\n\nNow we’ll fit the model. I’ll allow the intercept to be different for Southern and non-Southern states, but because we’re interested in the total causal effect of Waffle House Numbers, I’ll force the effect to be the same across both groups.\n\nset.seed(100)\nfit_6h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a[S] + b * W,\n            a[S] ~ dnorm(0, 1),\n            b ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd\n    )\n\nrethinking::precis(fit_6h1, depth = 2)\n\n             mean         sd        5.5%     94.5%\na[1]  -0.18668305 0.16832424 -0.45569771 0.0823316\na[2]   0.46329565 0.29995319 -0.01608748 0.9426788\nb      0.05197162 0.17641807 -0.22997853 0.3339218\nsigma  0.92058385 0.09085178  0.77538516 1.0657825\n\n\nOK, I’m getting a warning but I don’t think it’s doing anything. So I’ll just ignore it. For this model, we can see that there is a strongly positive effect of \\(b\\). Let’s look at the posterior distribution.\n\npost_6h1 &lt;- extract.samples(fit_6h1)\ndens(post_6h1$b)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that almost all of the posterior density is above zero, indicating that there is a positive effect of Waffle Houses on divorce rate. For every 1 standard deviation increase in the number of Waffle Houses in a state, the divorce rate is expected to increase by about 0.25 units. Let’s put that back from standardized units into real units.\n\n1 * attr(wd$W, \"scaled:scale\") + attr(wd$W, \"scaled:center\")\n\n[1] 98.12959\n\n0.25 * attr(wd$D, \"scaled:scale\") + attr(wd$D, \"scaled:center\")\n\n[1] 10.1432\n\n\nSo we see that we expect the divorce rate to increase by about \\(10\\%\\) for every 98 additional Waffle Houses, or approximately \\(0.1\\%\\) per Waffle House. This is the total causal effect based on our DAG, even though I would guess that the direct effect is zero and this entire effect is through location.\n\n6H2\nFirst, we need to check the implied causal independencies of the DAG.\n\ndagitty::impliedConditionalIndependencies(dag_6h1)\n\nD _||_ M | A, W\nD _||_ S | A, W\n\n\nTest one: \\(D\\) and \\(M\\) should be independent after adjusting for \\(A\\) and \\(W\\).\n\nset.seed(100)\nwd2 &lt;- c(\n    wd,\n    M = list(standardize(WaffleDivorce$Marriage)),\n    A = list(standardize(WaffleDivorce$MedianAgeMarriage))\n)\n\nfit_6h2_a1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bm, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_a2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M,\n            a ~ dnorm(0, 1),\n            c(bm) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_a1,\n    fit_6h1_a2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bm\"))\n\n\n\n\n\n\n\n\nYes, we can see that if we only include \\(M\\) (fit_6h1_a2), we see an effect, but if we control for \\(A\\) and \\(W\\), as in fit_6h1_a1, we do not.\nNow let’s check the second test: \\(D\\) and \\(S\\) should be independent if we control for \\(A\\) and \\(W\\).\n\nset.seed(100)\nfit_6h2_b1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bs, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_b2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S,\n            a ~ dnorm(0, 1),\n            c(bs) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_b1,\n    fit_6h1_b2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bs\"))\n\n\n\n\n\n\n\n\nWe see exactly the same interpretation here: when only \\(S\\) is in the model, all of the posterior density is above 0, but when we control for \\(A\\) and \\(W\\), a significant portion is below zero and the mean is lower. So I think we can say that our data appear to be consistent with the conditional independencies that our DAG implies.\nWe can also do these tests automatically used the method recommended by dagitty. I don’t know that much about these results, but they agree with our modeling results, which is good!\n\ndagitty::localTests(\n    dag_6h1,\n    sample.cov = lavaan::lavCor(as.data.frame(wd2)),\n    sample.nobs = nrow(as.data.frame(wd2))\n)\n\n                  estimate   p.value       2.5%     97.5%\nD _||_ M | A, W -0.0855530 0.5650794 -0.3613429 0.2035528\nD _||_ S | A, W  0.1349902 0.3622409 -0.1550991 0.4044034"
  },
  {
    "objectID": "sr/cp6.html#h3",
    "href": "sr/cp6.html#h3",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H3",
    "text": "6H3\nNow we are back to the fox problems. I’ll reproduce the DAG first just so I have it in my notes.\n\nfox_dag &lt;-\n    dagitty::dagitty(\n        \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n        }\"\n    )\ndagitty::coordinates(fox_dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\nplot(fox_dag)\n\n\n\n\n\n\n\n\nFirst, we want to infer the total causal influence of area (A) on weight (W). I’ll go ahead and set up the data, and standardize all the variables as McElreath recommends. I’ll also model the log weight instead of the raw weight so we can ensure that our predictions remain positive.\n\ndata(foxes)\nf2 &lt;-\n    foxes |&gt;\n    dplyr::transmute(\n        A = area,\n        F = avgfood,\n        G = groupsize,\n        W = log(weight)\n    ) |&gt;\n    as.list() |&gt;\n    lapply(FUN = rethinking::standardize)\n\nNow we’ll adopt a model for the weight. Since it is logged and standardized we’ll use a Gaussian likelihood function. To set priors, we first need to determine our adjustment set. There are two causal paths between \\(A\\) and \\(W\\):\n\n\\(A \\to F \\to W\\) and\n\\(A \\to F \\to G \\to W\\).\n\nIf we want the total causal effect of \\(A\\) on \\(W\\), there are no closed paths and we do not need to adjust for any other variables. We can confirm this with dagitty.\n\ndagitty::adjustmentSets(\n    fox_dag,\n    exposure = \"A\",\n    outcome = \"W\",\n    effect = \"total\"\n)\n\n {}\n\n\nWe get the empty set as our minimal sufficient adjustment set. So we’ll adopt the following model.\n\\[\n\\begin{align*}\n\\mathrm{Scale}\\left(\\log W \\right) &\\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\mathrm{Scale}(A)\\\\\n\\alpha &\\sim N(4.5, 0.5) \\\\\n\\beta &\\sim N(0, 0.25) \\\\\n\\sigma &\\sim \\mathrm{Exp}(10)\n\\end{align*}\n\\]\nI tuned the parameters for the prior distributions using a prior predictive simulation to ensure that the prior predictions stay within the expected outcome space. As usual, I think that making this model have an intercept of zero potentially makes more sense (a fox with no area should starve) and then we would not expect this relationship to be linear, but we will just mess with the priors until the predictions look reasonable.\n\nset.seed(101)\na &lt;- rnorm(1000, 4.5, 0.5)\nb &lt;- rnorm(1000, 0, 0.25)\nsigma &lt;- rexp(1000, 10)\n\nplot(\n    NULL,\n    xlim = c(1, 5.25),\n    ylim = c(1.5, 7.5),\n    xlab = \"Area\",\n    ylab = \"Weight\",\n    main = \"Prior predictive simulation\"\n)\n\nx &lt;- seq(min(f2$A), max(f2$A), length.out = 1000)\nxp &lt;- x * attr(f2$A, \"scaled:scale\") + attr(f2$A, \"scaled:center\")\nout &lt;- vector(length = 1000, mode = \"list\")\nfor (i in 1:1000) {\n    # Sample y's from their distribution\n    y &lt;- rnorm(1000, a[i] + b[i] * x, sigma[i])\n    \n    # Backtransform to original scale\n    yp &lt;- exp(y * attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))\n    out[[i]] &lt;- yp\n    \n    lines(x = xp, y = y, type = \"l\", col = rethinking::col.alpha(\"black\", 0.05))\n}\n\n\n\n\n\n\n\n\nI think that looks fine but I have to admit that I find assigning reasonable priors to standardized data quite difficult, it feels like just randomly picked numbers under the lines look ok. Now we can finally fit our regression and get the estimated causal effect.\n\nm_6h3 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * A,\n        a ~ dnorm(4.5, 0.5),\n        b ~ dnorm(0, 0.25),\n        sigma ~ dexp(10)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h3)\n\n            mean         sd         5.5%     94.5%\na     0.14011603 0.08988426 -0.003536369 0.2837684\nb     0.03384817 0.08470967 -0.101534245 0.1692306\nsigma 0.96550793 0.06089243  0.868190063 1.0628258\n\n\nLet’s look at the posterior distribution of \\(\\beta\\).\n\npost &lt;- extract.samples(m_6h3)\ndens(post$b, lwd = 2)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that while the density of \\(\\beta\\) is more than 50% above zero, there is a substantial amount of the density on either side of zero. So in general, it seems that area size is not directly correlated to fox weight. If anything, there is a small positive effect, but it is not very strong. We can get predictions on the original scale as well.\n\nmodel_out &lt;- rethinking::sim(m_6h3, data = list(A = x), n = 10000)\nmodel_mu &lt;-\n    colMeans(model_out) |&gt;\n    (\\(x) x *attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))() |&gt;\n    exp()\n\nmodel_pi &lt;- apply(model_out, 2, \\(x) exp(rethinking::PI(x) * \n                                        attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\")))\n\nplot(\n    xp, model_mu,\n    type = \"l\",\n    xlim = range(xp),\n    ylim = range(foxes$weight),\n    xlab = \"Simulated area\",\n    ylab = \"Counterfactual weight\"\n)\nlines(xp, model_pi[1, ], lty = 2)\nlines(xp, model_pi[2, ], lty = 2)\n\n\n\n\n\n\n\n\nAs we would expect from the model estimates, there is a slight positive trend with incredibly wide credible intervals."
  },
  {
    "objectID": "sr/cp6.html#h4",
    "href": "sr/cp6.html#h4",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H4",
    "text": "6H4\nNext we want to infer the total causal effect of adding food. Since we want to know the total causal effect, we don’t need to adjust for anything else in the model, since \\(G\\) is a mediator on the causal path. This time I’ll just use standard priors since it doesn’t really matter that much.\n\nm_6h4 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * `F`,\n        a ~ dnorm(0, 1),\n        b ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h4)\n\n               mean         sd       5.5%     94.5%\na      9.224085e-10 0.09165572 -0.1464835 0.1464835\nb     -1.533812e-02 0.09205000 -0.1624518 0.1317756\nsigma  9.913351e-01 0.06467093  0.8879784 1.0946917\n\n\nOK, we see a similar thing here. There is a possibly a slight negative relationship, but it looks like there is really no relationship here."
  },
  {
    "objectID": "sr/cp6.html#h5",
    "href": "sr/cp6.html#h5",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H5",
    "text": "6H5\nNow we want to get the total causal effect of group size, \\(G\\). Now we have to also control for food, \\(F\\), because it is a confounder on one of the paths from \\(G \\to W\\).\n\nm_6h5 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * `F` + bG * G,\n        a ~ dnorm(0, 1),\n        bF ~ dnorm(0, 1),\n        bG ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h5)\n\n               mean         sd       5.5%      94.5%\na      1.467513e-05 0.08752970 -0.1398747  0.1399040\nbF     5.602422e-01 0.19672784  0.2458332  0.8746513\nbG    -6.435327e-01 0.19672896 -0.9579436 -0.3291218\nsigma  9.463559e-01 0.06178686  0.8476086  1.0451032\n\n\nOk, interestingly we can now see a strong positive effect of \\(F\\) and a strong negative effect of \\(G\\). If we plot the data stratified by group size, we can understand this effect a bit better. This is an example of a masked relationship. Overall, average food appears to have a negative effect on weight, which doesn’t seem to make sense.\nHowever, for groups of a given size, the more food there is, the heavier those foxes tend to be. But for healthier (heavier) foxes, it is likely that more new foxes are born, and despite the fact that the foxes are in a more abundant area, there is less food per fox. So within a group size, having more food is good. But if the group size expands without a simultaneous increase in food supply, there will be less food available for each fox. However, note that for the high group sizes, only one group was observed in each. So perhaps our estimates contain selection bias, if the groups that were recorded are not typical examples of foxes with high group sizes.\n\nggplot(foxes, aes(x = avgfood, y = weight, col = factor(groupsize))) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\", alpha = 0.5) +\n    hgp::theme_ms() +\n    geom_smooth(method = \"lm\", aes(group = 1, color = \"overall\")) +\n    scale_color_manual(\n        values = c(viridisLite::viridis(7), \"black\")\n    ) +\n    labs(x = \"Average food\", y = \"Weight\", col = \"Group size\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "sr/cp6.html#h6-and-6h7",
    "href": "sr/cp6.html#h6-and-6h7",
    "title": "Chapter 6: The Haunted DAG and the Causal Terror",
    "section": "6H6 and 6H7",
    "text": "6H6 and 6H7\nI decided to skip the open research questions."
  },
  {
    "objectID": "sr/cp4.html",
    "href": "sr/cp4.html",
    "title": "Chapter 4: Geocentric Models",
    "section": "",
    "text": "This chapter discusses basic Bayesian model fitting with grid and quadratic approximations. The title, “geocentric models,” refers to models that make good predictions, but do not provide causal information about a question. One of these examples is linear regression. This chapter gives examples of basic bayesian linear regression and includes details such as prior predictive checks and how to fit curves with polynomials and b-splines."
  },
  {
    "objectID": "sr/cp4.html#chapter-notes",
    "href": "sr/cp4.html#chapter-notes",
    "title": "Chapter 4: Geocentric Models",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nNormal distributions arise from sums of random fluctuations. Lognormal distributions arise from products of random fluctuations. This property explains why normal distributions are so good at modeling real world data (ontological justification).\nNormal distributions can also be justified by the principle of maximum entropy – if all we are willing to specify about a distribution is its mean and variance, then the normal distribution contains the least amount of information (epistemological justification).\nIndex coding (as opposed to dummy coding or similar methods) makes specification of priors for categorical variables easier.\nThe prior predictive simulation, drawing samples from the distribution implied by the priors, is essential for ensuring that our priors are reasonable. Note that we should not compare the prior predictive simulation to the observed data, only to our preexisting knowledge of constraints on the model.\nMany models which are written in the “plus epsilon” form (see pg 81) (typical for linear models) can be rewritten in this more general framework, which will be easier for non-Gaussian models.\nQuadratic approximation, estimating the peak of the posterior distribution with a multivariate normal distribution, is easier than grid approximation and works well when the posterior is approximately Gaussian (many simple examples are). The peak of the quadratic approximate posterior is the maximum a posteriori estimate.\nRecall that even though grid and quadratic approximate posteriors provide an actual estimate of the posterior distribution, we can (and should) still sample from the posterior. This mimics the process for inference on more complicated models that must be fit with MCMC algorithms.\nA linear model fits the mean, \\(\\mu\\), of an outcome as a linear function of the predictor variable(s) and some parameters. These models are often geocentric – they provide good answers, but often say nothing about causality.\nPlotting simulations of the posterior distribution can provide a lot of information about the posterior (see pg 99), often much more than tables of calculations alone.\nThese types of Bayesian models incorporate two different types of uncertainty – uncertainty in parameter values, which is based on the plausibility of parameter values after seeing the data, and uncertainty from sampling processes.\nWe can extend linear models to fit curved patterns in datas in several ways, but two of the easiest are polynomials and b-splines. Priors can be difficult to fit to both, as these models are also geocentric. They can accurately fit curves, but do not describe the mechanism or process that generates curved data in the first place. See pg. 119 for an example of fitting a b-spline model using rethinking. One further extension is the generalized additive model (GAM) which incorporates smoothing over continuous predictor variables."
  },
  {
    "objectID": "sr/cp4.html#exercises",
    "href": "sr/cp4.html#exercises",
    "title": "Chapter 4: Geocentric Models",
    "section": "Exercises",
    "text": "Exercises\nThe first few questions are about the following model. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n4E1\nIn the model shown, the line \\(y_i \\sim \\mathrm{Normal}(\\mu, \\sigma)\\) is the likelihood.\n\n\n4E2\nThe model contains two parameters (\\(\\mu\\) and \\(\\sigma\\)).\n\n\n4E3\nTo use Bayes’ theorem to calculate the posterior, we would write \\[\n\\frac{\\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1)}{\\int\\int \\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1) \\ \\mathrm{d}\\mu \\ \\mathrm{d}\\sigma}.\n\\]\n\n\n4E4\nIn the model shown below, the linear model is the line \\(\\mu_i = \\alpha + \\beta x_i\\). \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(2)\n\\end{align*}\\]\n\n\n4E5\nThere are three parameters in the posterior distribution of the model shown (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)) – \\(\\mu\\) is no longer a parameter of the model since it is calculated deterministically.\n\n\n4M1\nFor the model definition below, simulate observed \\(y\\) values from the prior. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n# Do the simulation\nmu &lt;- rnorm(1000, mean = 0, sd = 10)\nsigma &lt;- rexp(1000, rate = 1)\ny &lt;- rnorm(1000, mu, sigma)\n\n# Plot the results\nlayout(matrix(c(1, 2, 3), ncol = 3))\nhist(y, breaks = \"FD\", main = \"y\")\nhist(mu, breaks = \"FD\", main = \"mu\")\nhist(sigma, breaks = \"FD\", main = \"sigma\")\n\n\n\n\n\n\n\n\n\n\n4M2\nTranslate the model into a quap formula.\n\ny ~ dnorm(mu, sigma),\nmu ~ dnorm(0, 10),\nsigma ~ dexp(1)\n\n\n\n4M3\nTranslate the quap model formula below into a mathematical model definition.\n\ny ~ dnorm(mu, sigma),\nmu &lt;- a + b * x,\na ~ dnorm(0, 10),\nb ~ dunif(0, 1),\nsigma ~ dexp(1)\n\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= a + b * x_i \\\\\na &\\sim \\mathrm{Normal}(0, 10) \\\\\nb &\\sim \\mathrm{Uniform}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n\n4M4\nA sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model, defending any priors you choose.\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta * (\\mathrm{year}_i - \\min \\mathrm{year}_i) \\\\\n\\alpha &\\sim \\mathrm{Normal}(120, 30) \\\\\n\\beta &\\sim \\mathrm{Log-Normal}(0, 1.5) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(0, 0.2)\n\\end{align*}\\]\nTo create the priors, I assumed that height was measured in centimeters. If it is not, transforming either the data or the prior coefficients is a simple linear transformation. The prior for the intercept is centered at a relatively small height (around four feet) with a large spread to allow for differences in biological sex or age distributions in the population, since these were not specified in the question. I subtracted the minimum year in the model so that the years would be scaled as 0, 1, 2, 3, instead of the actual numeric value of the year.\nIn general, we know that height increases over time, so I used a lognormal prior for the slope to enforce a positivity constraint. The prior has a location parameter of 0, allowing for the chance of no growth over the three years, but a wide spread was chosen by experimenting until the prior predictive simulation represented a wide range of possible trajectories with very few trajectories appearing to be biologically unreasonable. Again, the value was chosen by experimenting with prior predictive simulations until the result appeared to capture a large variety of biologically meaningful trajectories.\nI assigned an exponential prior to sigma to reflect the fact that all variances are positive, and most tend to be small-ish, but can be large. I had a difficult time with this one in particular because this model structure seems to imply that people can shrink in-between years. This would be possible with measurement error, but I think it would be quite difficult to have measurement error this severe in something like height, which is easy to measure. However, in order to include a constraint that height can only increase, I think we would need to change the likelihood in the model, which we haven’t discussed yet in the book, so I didn’t want to worry about that. (For example, we could make height lognormally distributed, so random fluctuations would only increase height, as opposed to the current model where random fluctuations can decrease height.)\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 0, 1.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- rexp(1000, 1 / 5)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\n\n\n4M5\nIf I were reminded that every student got taller each year, this would not really change my choice of priors, but it does make me consider the same issues. I already accounted for this in the prior for \\(\\beta\\). However, it does make me think more about the likelihood I used – I really dislike that this likelihood allows for fluctuations that show up like this. Height is not measured perfectly, but large variations are uncommon. So perhaps it makes more sense to have quite a small variance parameter (\\(\\sigma\\)). Or perhaps we could structure the model so that each student has a common variance parameter that does not change every year. We could also consider making the effect of \\(\\beta\\) stronger, so that there is an assumed growth effect and not growing each year is more rare. So perhaps we could consider the following adjusted priors.\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- rexp(1000, 1)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\nThese priors still represent a wide range of biologically accurate values, but there is much less internal (within-subject) fluctuation in height between years, and on average, the slope is steeper.\n\n\n4M6\nIf I had the information that variance among heights for students of the same age is never more than 64cm, this would change my choice of variance prior (assuming that this is a priori information and not measured from our sample). If this were measured from the sample, I would not change my priors. But we could consider changing the prior for \\(\\sigma\\) like so. Using the empirical rule for normal distributions, I reasoned that 21 is a reasonable constraint for an upper bound on a Uniform prior for sigma – the resultant likelihood will have approximately 99% of observations within \\(3 \\times 21 = 63\\) cm of the mean.\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- runif(1000, 0, 21)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\nThis variance is quite large, which I think intensifies the problem I was previously discussing. Since we consider each annual measurement to be independent, without any “clustering” (we haven’t used that word yet so I don’t want to use it wrong) by inviduals, following this model to the letter allows for wide fluctuations within the predicted trajectory – to me it just doesn’t make sense for the simulated trajectory to be lower in year \\(N + 1\\) than in year \\(N\\).\n\n\n4M7\nRefit model m4.3 but omit the mean weight xbar. Compare the new model’s posterior to that or the original model, then compare the posterior predictions.\n\nset.seed(100)\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(Howell1)\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18, ]\nm_4m7 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * weight,\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d2\n    )\n\nFirst we’ll look at the posterior summary.\n\nprecis(m_4m7)\n\n             mean         sd        5.5%       94.5%\na     114.5302928 1.89715027 111.4982802 117.5623053\nb       0.8908259 0.04174484   0.8241096   0.9575422\nsigma   5.0711281 0.19109884   4.7657152   5.3765409\n\n\nThe intercept estimate is quite different – the previous model reported the following statistics for a: mean 154.6, sd 0.27, 5.5% 154.17, 94.5% 155.05. So our parameter is much smaller with a larger variance. However, the estimates for b and sigma are almost exactly the same as the estimates given for the previous model (see book pg 99). In particular, we should look at the covariance matrix according to the question.\n\nvcov(m_4m7) |&gt; round(digits = 3)\n\n           a      b sigma\na      3.599 -0.078 0.009\nb     -0.078  0.002 0.000\nsigma  0.009  0.000 0.037\n\n\nThe variance for a is much higher, while the variance for b and sigma is exactly the same as the book reports. However, there is now some covariance between a and b, and between a and sigma (but not between b and sigma). Next I’ll plot a sample of prior predictions. Since the uncertainty is so narrow, I decided to only plot 100 posterior samples. The red line shows the maximum a posteriori estimate.\n\npost &lt;- extract.samples(m_4m7, n = 100)\nlayout(1)\nplot(\n    x = d2$weight, y = d2$height,\n    xlim = range(d2$weight),\n    ylim = range(d2$height),\n    col = rangi2,\n    xlab = \"weight\", ylab = \"height\"\n)\nmtext(\"Sampled posterior predictions\")\n\n# Plot the lines\nfor (i in 1:length(post$a)) {\n    curve(post$a[i] + post$b[i] * x,\n                col = col.alpha(\"black\", 0.1),\n                add = TRUE)\n}\n\nabline(\n    a = mean(post$a),\n    b = mean(post$b),\n    col = \"red\",\n    lty = 2\n)\n\n\n\n\n\n\n\n\nThe posterior predictions look about the same to me. So I guess that even when we use different parametrizations (thus changing the interpretation and scale of our alpha parameter), the predictions still work out to be about the same. I think this is foreshadowing MCMC convergence diagnostics with different parametrizations in the future.\n\n\n4M8\nRefit the cherry blossom spline and experiment with changing the number of knots and the width of the prior on the weights. What do you think the combination of these controls?\nFirst we’ll just refit the example.\n\n# Data import\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm1 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m1)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"Original model\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nNow let’s fit an example with more knots, say 30. This is a dramatic increase but I really want to see the effect.\n\nset.seed(100)\n# Create the splits\nnk &lt;- 30\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm2 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m2)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nNow we’ll also increase the width of the prior for \\(w\\). Again, I’ll increase this dramatically to make the effect easier to see.\n\nset.seed(100)\n# Create the splits\nnk &lt;- 20\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm3 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 70),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m3)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots and wider prior\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nHmm, it’s hard to see a difference. Let’s try plotting the three lines on top of each other.\n\nlayout(1)\nplot(\n    x = d2$year, y = d2$doy,\n    xlab = \"year\", ylab = \"Day in year\",\n    xlim = range(d2$year), ylim = range(d2$doy),\n    col = col.alpha(rangi2, 0.3), pch = 16\n)\nlines(d2$year, y = apply(link(m1), 2, mean), col = \"black\")\nlines(d2$year, y = apply(link(m2), 2, mean), col = \"blue\")\nlines(d2$year, y = apply(link(m3), 2, mean), col = \"red\")\nlegend(\n    x = \"top\",\n    c(\"original\", \"more knots\", \"more knots and wider prior\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = c(1, 1, 1)\n)\n\n\n\n\n\n\n\n\nYep, from this plot the difference is pretty easy to see. The number of knots and the prior for the weights controls the “wiggliness” (the smoothness or penalty) of the splines. More knots or a wider prior allows for more local variation in the spline curve, whereas constraining the number of knots (or shrinking the weights toward 0) constrains the spline, forcing it to vary less. I guess that the prior on the weights here is equivalent to increasing the penalty term of some other kind of spline, and a higher number of knots allows for a higher degree of interpolation as more inflection points are included.\n\n\n4H1\nWe want to estimate the expected heights of the 5 individuals from the !Kung census with weights given in the text. Since we already saw that the predictions from our fitted model earlier were the same as the predictions in the book, I’ll use that model to do this. All we need to do is create a data frame with the heights of these individuals and pass it to link with the model.\n\n# Create the data for the new individuals\nnew_indiv &lt;-\n    tibble::tribble(\n        ~individual, ~weight,\n        1, 46.95,\n        2, 43.72,\n        3, 64.78,\n        4, 32.59,\n        5, 54.63\n    )\n\n# Get the samples from the posterior at the given weights\npreds &lt;- rethinking::link(fit = m_4m7, data = new_indiv)\n\n# Get the means and round it\nnew_indiv$means &lt;- apply(preds, 2, function(x) round(mean(x), 2))\n\n# Calculate the PI and format it\nPIs &lt;- apply(preds, 2, PI, 0.89)\nnew_indiv$PIs &lt;- apply(PIs, 2, function(x) paste0(round(x, 2), collapse = \", \"))\n\n# Format names and create the table\nnames(new_indiv) &lt;- c(\"Individial\", \"weight\", \"expected height\", \"89% interval\")\nknitr::kable(\n    new_indiv,\n    caption = paste0(\"Expected heights with 89% equal-tailed posterior intervals\",\n                                     \" for the five individuals wiuthout recorded heights in the\",\n                                     \" !Kung census data, using link.\")\n)\n\n\nExpected heights with 89% equal-tailed posterior intervals for the five individuals wiuthout recorded heights in the !Kung census data, using link.\n\n\nIndividial\nweight\nexpected height\n89% interval\n\n\n\n\n1\n46.95\n156.35\n155.88, 156.77\n\n\n2\n43.72\n153.47\n153.04, 153.92\n\n\n3\n64.78\n172.21\n170.85, 173.53\n\n\n4\n32.59\n143.57\n142.62, 144.51\n\n\n5\n54.63\n163.18\n162.4, 163.89\n\n\n\n\n\nAfter talking to my colleague Juliana, I also decided to rerun this using sim() – with predicted heights that include variance from the mean. I think this is probably “more correct” so credit to Juliana for pointing that out to me.\n\nnew_indiv &lt;-\n    tibble::tribble(\n        ~individual, ~weight,\n        1, 46.95,\n        2, 43.72,\n        3, 64.78,\n        4, 32.59,\n        5, 54.63\n    )\n# Get the samples from the posterior at the given weights\npreds &lt;- rethinking::sim(fit = m_4m7, data = new_indiv)\n\n# Get the means and round it\nnew_indiv$means &lt;- apply(preds, 2, function(x) round(mean(x), 2))\n\n# Calculate the PI and format it\nPIs &lt;- apply(preds, 2, PI, 0.89)\nnew_indiv$PIs &lt;- apply(PIs, 2, function(x) paste0(round(x, 2), collapse = \", \"))\n\n# Format names and create the table\nnames(new_indiv) &lt;- c(\"Individial\", \"weight\", \"expected height\", \"89% interval\")\nknitr::kable(\n    new_indiv,\n    caption = paste0(\"Expected heights with 89% equal-tailed posterior intervals\",\n                                     \" for the five individuals wiuthout recorded heights in the\",\n                                     \" !Kung census data, using sim.\")\n)\n\n\nExpected heights with 89% equal-tailed posterior intervals for the five individuals wiuthout recorded heights in the !Kung census data, using sim.\n\n\nIndividial\nweight\nexpected height\n89% interval\n\n\n\n\n1\n46.95\n156.36\n148.29, 164.14\n\n\n2\n43.72\n153.79\n145.86, 161.95\n\n\n3\n64.78\n172.15\n164.02, 181.17\n\n\n4\n32.59\n143.91\n135.7, 152.15\n\n\n5\n54.63\n163.11\n154.61, 171.11\n\n\n\n\n\nSo we can see that the results are not that different, but the 89% intervals are a bit wider, which is good. The new intervals incorporate an additional component of the variance that my previous estimates did not, so if I were publishing this model, I would definitely publish the sim intervals instead. I guess these are analogous to frequentist prediction intervals as opposed to confidence intervals.\n\n\n4H2\nSelect out all the rows in the Howell1 data with ages below 18 years of age. Fit a linear regression to these data using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?\nOk, first we’ll subset the data and make a quick plot.\n\nd &lt;- Howell1[Howell1$age &lt; 18, ]\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n\n\n\n\n\n\nd_xbar &lt;- mean(d$weight)\n\nNow let’s fit the model. I’ll use the same priors as before, since I already looked at the data. We probably want to actually change the prior on the intercept since children have a lower mean height than adults, but I think it will work out alright.\n\nm4h2 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * (weight - d_xbar),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d\n    )\nprecis(m4h2)\n\n            mean         sd       5.5%      94.5%\na     108.383621 0.60867543 107.410840 109.356402\nb       2.716654 0.06831783   2.607469   2.825839\nsigma   8.437466 0.43060814   7.749271   9.125661\n\n\nWe can see that the estimate slope is \\(\\beta = 2.69\\). So for every 10 units of increase in weight, our model predicts that a child’s height will increase by \\(26.9\\) units.\nNext, we’ll plot the MAP prediction line with an 89% interval, along with the 89% prediction interval for heights.\n\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n# Weight values to predict over\nweight_seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 0.5)\n\n# Get the posterior predictions\nset.seed(100)\nlink_4h2 &lt;- link(m4h2, data = data.frame(weight = weight_seq))\nsim_4h2  &lt;-  sim(m4h2, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu_mean &lt;- apply(link_4h2, 2, mean)\nmu_CI &lt;- apply(link_4h2, 2, PI, 0.89)\nmu_PI &lt;- apply(sim_4h2, 2, PI, 0.89)\n\n# Plot the results -- we want to go backwards, starting with the wider\n# interval first\nshade(mu_PI, weight_seq, col = col.alpha(\"black\", 0.5))\nshade(mu_CI, weight_seq, col = col.alpha(\"lightgray\", 0.75))\nlines(weight_seq, y = mu_mean, col = \"red\", lty = 2)\n\n\n\n\nThe observed height and weight of the children in the !Kung census data (circular points) with the MAP linear regression line (red dashed line), 89% equal-tailed posterior interval of this line (light gray shading), and 89% equal-tailed posterior interval for predicted heights ( dark gray shading).\n\n\n\n\nThe main issue with the model is that there is clearly some curvature to the trend in the data. There seems to be an inflection point around a weight of 30 where the trend is no longer linear, which will make the entire model fit worse (particularly because this region has high leverage). There is also some curvature in the lower weight values as well. Perhaps it would be better to fit this model with a spline or with a dummy variable for “age categories” that interacts with the slope term, allowing the model to “bend” at particular points – the same thing could be accomplished using splines with degree 1. Normally I hate discretization, but in this case these trends are likely to map to our approximate knowledge of human growth. Babies grow faster than toddlers, and teenagers grow faster than younger children. So perhaps including more information in our model could help solve this problem as well. But if we don’t want to include age, a spline could probably work better.\n\n\n4H3\nBased on our colleague’s suggestion, we want to use the natural logarithm of weight to model height. We’ll do that using the entire Howell1 dataset. I’ll use the same priors as before.\n\nm4h3 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * log(weight),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = Howell1\n    )\nprecis(m4h3)\n\n            mean        sd       5.5%      94.5%\na     -22.874317 1.3342911 -25.006772 -20.741862\nb      46.817789 0.3823240  46.206761  47.428816\nsigma   5.137088 0.1558847   4.887954   5.386222\n\n\nThese estimates indicate that an individual from the census with mean weight is expected to have a height of \\(138.27\\) units. The slope (b) indicates that for every 1 unit increase in log-weight, height is expected to increase by about 47 units. I think that log-weight units are a bit difficult to think about, this would probably be easier to understand if we’d used log2 or something without an \\(e\\) in it. This coefficient means that if we multiply an individual’s weight by \\(e\\) (approximately 2.72), we would expect to see a 47 unit increase in their height. So, e.g., if we used log2, we could interpret this as doubling their weight, but we can’t do that now.\n\n# Plot the raw data\nplot(height ~ weight, data = Howell1, col = col.alpha(rangi2, 0.5))\n\n# Get the mean and height samples -- I couldn't get sim to work so I did link\n# and then did the sim part myself\nweight_seq &lt;- seq(from = min(Howell1$weight),\n                                    to = max(Howell1$weight),\n                                    by = 1)\nset.seed(100)\nlink_4h3 &lt;- link(m4h3, data = data.frame(weight = weight_seq))\nsim_4h3  &lt;-  sim(m4h3, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu    &lt;- apply(link_4h3, 2, mean)\nmu_CI &lt;- apply(link_4h3, 2, PI, 0.97)\nmu_PI &lt;- apply( sim_4h3, 2, PI, 0.97)\n\n# Add the predictions to the plot\nlines(weight_seq, y = mu_PI[1, ], lty = 2)\nlines(weight_seq, y = mu_PI[2, ], lty = 2)\nshade(mu_CI, weight_seq, col = col.alpha(\"darkgray\", 0.75))\nlines(weight_seq, y = mu, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWell, it does look like our colleague’s suggestion produced a better model!\n\n\n4H4\nPlot the prior predictive distribution for the parabolic polynomial regression model in the chapter. We want to modify the prior distributions of the parameters so that the prior predictions lie within the reasonable outcome space we defined.\n\n# Data setup since I keep overwriting names\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18, ]\n\n# Standardize weight\nd2$weight_s &lt;- scale(d2$weight)\nd2$weight_s2 &lt;- d2$weight_s ^ 2\n\nxbar &lt;- mean(d2$weight)\nsx &lt;- sd(d2$weight)\n\n# Setup\nlayout(matrix(c(1, 2), ncol = 2))\nset.seed(100)\nN &lt;- 100\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rlnorm(N, 0, 1)\nb2 &lt;- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rnorm(N, 0, 4)\nb2 &lt;- rnorm(N, 0, 4)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\n\n\n\n\nWell, when I looked at the original priors they were all already within the reasonable model space. But since we have a parabola now, we don’t need to constrain the linear part to be positive – we could get an always positive derivative even with a negative linear term (\\(2ax + b &gt; 0\\) for \\(ax^2 + bx + c\\), so e.g. if \\(a = 2, \\ b = -2\\) the derivative will be positive for all \\(x &gt; 1\\), and all of our values are larger than 30). So I changed the priors to get trajectories that cover a bit more of the reasonable space. They approach the edges in some cases, but that’s okay. I wasn’t really sure what else to do with this question once I saw that the parabolas were all already in the reasonable space.\nMaybe I was supposed to NOT standardize the data, even though the chapter did this and the book didn’t explicitly say what model to use. However, assigning a biologically meaningful prior without even centering the data is quite difficult, because in this example it makes sense to constrain the (non-centered) intercept at 0 (i.e. a person with weight zero has height zero).\nHowever, we can quickly see how this model doesn’t make biological sense by thinking about where we should center the data. If we center at the mean, our parabola has to be either concave up or concave down (recall the second derivative of a parabola is constant), which doesn’t make any sense. Why would low weight people be taller, or high weight people be shorter? By centering the data at the mean we force the parabola to take one of these two options. So perhaps standardizing by some reference point would be good, but remember that we don’t want to look at the data to choose where to standardize. So we’re forced to make one of two biologically inaccurate assumptions: either the concavity must be unreasonable or the intercept must be unreasonable. So let’s set an unreasonable intercept and choose not to center the data. That way we can at least constrain the concavity to be positive.\n\n# Setup\nset.seed(100)\nlayout(matrix(c(1, 2), ncol = 2))\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rlnorm(N, 0, 1)\nb2 &lt;- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 90, 20)\nb1 &lt;- rnorm(N, 0, 0.5)\nb2 &lt;- rnorm(N, 0.02, 0.005)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\n\n\n\n\nThere. Now our priors for the non-centered data fit inside the reasonable range. Are you happy now? They don’t really make any sense (why would a person with 0 weight have 90 height? why do we have to restrain the coefficient for the b terms so much?) but they fit inside the line. I think the better answer for this question is to either standardize the data or avoid using a quadratic model for this problem, which was maybe the entire point of the question.\n\n\n4H5\nReturn to the cherry blossoms data and model the association between blossom date (doy) and March temperature (temp). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature trend predict the blossom trend?\nI’m leaning towards splines for this question – I expect this relationship to be nonlinear, and given my frustration with polynomial models in the previous question, I think it’s okay to stick with a spline. So first we need to set up the splines. We haven’t talked about missing data at all yet in this book so I’ll just use the complete cases.\n\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms[ , c(\"temp\", \"doy\")]\nd2 &lt;- d[complete.cases(d), ]\n\nLet’s do a prior predictive simulation. I’ll start with the priors in the book for the previous model, but then adjust if they look unreasonable. I looked at the paper cited in the cherry blossoms data description, and it appears that the temperatures were measured in Kyoto. So I googled “Kyoto march temperature” and I guess we can assume that temperatures should be between 3 and 14 degrees celsius. McElreath says that the flowers can bloom any time from March to May, so as long as our predictions are between day 60 (first day of March in regular year) and day 152 (first day of May in leap year), I’ll consider that to be “reasonable”.\n\nN &lt;- 1000\nset.seed(100)\n\n# Create a range of temperature for the PPS splines\ntemp_range &lt;- seq(2, 14, 0.05)\nnk &lt;- 15\n\n# Create the splines\nB &lt;-\n    splines::bs(\n        x = temp_range,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Simulate a since it is easy\na &lt;- rnorm(N, 100, 10)\n\n# Simulate the weights with a little trickery\nw &lt;- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, 15)))\n\n# Calculate the predicted means\nmu &lt;- a + B %*% w\n\n# Plot the pps\nlayout(1)\nplot(\n    NULL,\n    xlim = c(2, 14), ylim = c(40, 170),\n    xlab = \"temperature (Celsius)\", ylab = \"day of year\",\n    xaxs=\"i\", yaxs=\"i\"\n)\nabline(h =  60, lty = 2, lwd = 0.5)\nabline(h = 152, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    lines(x = temp_range, y = mu[, i], col = col.alpha(\"black\", 0.05))\n}\n\n\n\n\n\n\n\n\nYeah, the default priors look pretty good to me. The only change I made was to increase the width of the prior for \\(w\\), to potentially allow for more wiggliness. There is a bit of wiggling out of the expected day of year range, but it’s so rare that I think it will be fine, especially after the model sees the data. I think it will be harder to set the correct prior on \\(w\\) with a prior predictive simulation, so I think we better go ahead and fit the model.\nNow let’s fit a quap model. Note that I had to turn up the maximum number of iterations run by the underlying optim call to ensure convergence criteria were met.\n\n# Create the real splines\nB_real &lt;-\n    splines::bs(\n        x = d2$temp,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Fit the model\nm4h6 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 15),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B_real),\n        start = list(w = rep(0, ncol(B_real))),\n        control = list(maxit = 200)\n    )\n\nrethinking::precis(m4h6, depth = 2)\n\n             mean         sd       5.5%      94.5%\nw[1]    0.0000000 14.9999514 -23.972819  23.972819\nw[2]    0.0000000 14.9999514 -23.972819  23.972819\nw[3]    0.0000000 14.9999514 -23.972819  23.972819\nw[4]    6.9549283  5.2048778  -1.363472  15.273328\nw[5]    4.7825332  4.8284596  -2.934278  12.499344\nw[6]    5.3724394  4.7117796  -2.157895  12.902773\nw[7]    1.1616676  4.6557218  -6.279075   8.602410\nw[8]   -0.1986208  4.8130958  -7.890877   7.493636\nw[9]   -3.4414421  5.1367824 -11.651012   4.768128\nw[10]  -2.3424900  5.7891404 -11.594655   6.909674\nw[11]  -6.1560988  7.2908763 -17.808327   5.496130\nw[12]  -0.4530242  6.7393765 -11.223849  10.317801\nw[13]   0.0000000 14.9999514 -23.972819  23.972819\nw[14]   0.0000000 14.9999514 -23.972819  23.972819\nw[15]   0.0000000 14.9999514 -23.972819  23.972819\nw[16]   0.0000000 14.9999514 -23.972819  23.972819\nw[17]   0.0000000 14.9999514 -23.972819  23.972819\na     102.5223225  4.5314729  95.280154 109.764491\nsigma   5.8829498  0.1474653   5.647272   6.118628\n\n\nAlright, now we can get the actual predictions and plot them.\n\nplot(d2$doy ~ d2$temp, xlab = \"Temperature (Celsius)\", ylab = \"Day of year\",\n         col = col.alpha(rangi2, 0.4), pch = 16)\n\n# Get the predictions and interval -- have to use the basis over the\n# interpolated temperature data!\nlink_m4h6 &lt;- link(m4h6, data = list(B = B))\nmu &lt;- apply(link_m4h6, 2, mean)\nmu_pi1 &lt;- apply(link_m4h6, 2, PI, 0.97)\nmu_pi2 &lt;- apply(link_m4h6, 2, PI, 0.89)\nmu_pi3 &lt;- apply(link_m4h6, 2, PI, 0.61)\n\n# lines(temp_range, mu_pi3[1, ], lty = 2)\n# lines(temp_range, mu_pi3[2, ], lty = 2)\n# lines(temp_range, mu_pi2[1, ], lty = 2)\n# lines(temp_range, mu_pi2[2, ], lty = 2)\n# lines(temp_range, mu_pi1[1, ], lty = 2)\n# lines(temp_range, mu_pi1[2, ], lty = 2)\nshade(mu_pi1, temp_range, col = col.alpha(\"black\", 0.5))\nshade(mu_pi2, temp_range, col = col.alpha(\"darkgray\", 0.5))\nshade(mu_pi3, temp_range, col = col.alpha(\"gray\", 0.5))\nlines(temp_range, mu, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\nOK, so I had some confusion here with plotting the predictions. I forgot that I need to specify a separate \\(B\\) matrix of basis functions for the model fitting and for this part of the predictions. But now it is working and we can see that there is definitely a decent association between warmer temperatures and earlier blooming. However, we would need more high-temperature data to confirm this trend (and high-temperature data is something we wish we didn’t have more of, really).\n\n\n4H6\nQ. Simulate the prior predictive distirbution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing?\nOk, first we need to simulate the PPD. I’m pretty sure I already have a handle on this from the previous two spline exercises, but it should be fun. (In fact, we really already did this exact exercise, but whatever I guess.)\nLet’s simulate with maybe six different widths: 1, 5, 10 (chosen in the textbook), 15, 20, and 50. I’ll get the predictions from each of these simulations and then plot them.\n\n# Get ready for simulation.\n# Set parameters for sim\nwidths &lt;- c(1, 5, 10, 15, 20, 50)\nN &lt;- 250\nset.seed(100)\nlayout(matrix(c(1, 2, 3, 4, 5, 6), ncol = 3, nrow = 2))\n\n# Create the splits\nnk &lt;- 15\nyear_range &lt;- seq(800, 2010, 1)\nknot_list &lt;- quantile(year_range, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = year_range,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\nsim_res &lt;- vector(mode = \"list\", length = length(widths))\n\ndo_pps &lt;- function(width) {\n    # Simulate a since it is easy\n    a &lt;- rnorm(N, 100, 10)\n    \n    # Simulate the weights with a little trickery\n    w &lt;- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, width)))\n    \n    # Calculate the predicted means\n    mu &lt;- a + B %*% w\n    \n    plot(\n        NULL,\n        xlim = c(800, 2010), ylim = c(40, 170),\n        xlab = \"year\", ylab = \"day of year\",\n        xaxs=\"i\", yaxs=\"i\",\n        main = paste(\"Width:\", width)\n    )\n    \n    for (i in 1:N) {\n        lines(x = year_range, y = mu[, i], col = col.alpha(\"black\", 0.01))\n    }\n        \n    abline(h =  60, lty = 2, lwd = 0.5)\n    abline(h = 152, lty = 2, lwd = 0.5)\n    \n    return(NULL)\n}\n\nout &lt;- sapply(widths, do_pps)\n\n\n\n\n\n\n\n\nYep, just like I said in the other two exercises about this, the width of the spline prior determines the amount of smoothness that the spline should have. If we increase the width, the spline is encouraged to vary more, and will “wiggle” (spike up and down) with larger fluctuations. If we constrain this parameter, the spline is encouraged to vary less, staying closer to the intercept.\n\n\n4H8\nQ. The cherry blossom split in the chapter used an intercept \\(\\alpha\\), but technically it doesn’t require one. The first basis functions could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work.\nThis question is worded a bit confusingly to me. I assume he means the intercept parameter in the model, because the basis splines have a separate intercept that we also set to be true. Either way, let’s go ahead and try refitting a model without the intercept.\n\n# Data import\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- B %*% w,\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\n\nlayout(1)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWell, I’ll be honest. I’m not really sure what else we’re supposed to change here because that model looks perfectly fine to me. It has some curvature at the lowest year values though, which we maybe should have expected since we basically set the intercept to zero. I think we could improve this by adjusting the prior for \\(w\\) to have a mean that was the same mean as the previous intercept prior (that is, 100). Maybe that’s the other thing we need to change. Let’s try it.\n\n# Save the previous results or whatever\nold_mu &lt;- mu\nold_mu_PI &lt;- mu_PI\n\n# Fit the model\nm &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- B %*% w,\n            w ~ dnorm(100, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Model with intercept\nma &lt;-\n        quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\na_mu &lt;- link(m)\na_mu_PI &lt;- apply(mu, 2, PI, 0.97)\n\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\n\n# Plot the results with both lines\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nlines(d2$year, y = apply(a_mu, 2, mean), col = \"green\", lty = 2)\nlines(d2$year, y = apply(old_mu, 2, mean), col = \"blue\", lty = 2)\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\nlegend(\n    x = \"top\",\n    legend = c(\"intercept\", \"no intercept mean 0\", \"no intercept mean 100\"),\n    lty = c(2, 2, 2),\n    col = c(\"green\", \"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\nYes indeed, if you look closely, you can see that the red line and the green line are almost exactly the same. Which means that if we adjust the mean of the spline prior, we can fit the exact same model without an intercept. Yay."
  },
  {
    "objectID": "sr/cp2.html",
    "href": "sr/cp2.html",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "",
    "text": "In this chapter, the basics of Bayesian probability models and updating are described using a motivating example where the percent coverage of water on Earth is estimated by tossing an inflatable globe. This chapter covers the mechanical parts of Bayesian models."
  },
  {
    "objectID": "sr/cp2.html#chapter-notes",
    "href": "sr/cp2.html#chapter-notes",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nDistinction between the small world, the self-contained world of the model, and the large world, the context in which the model is used. Remember that golems can’t see context.\nBayesian Inference Is Just Counting – explanation using tree diagrams. I think this is a good example for simple problems but I don’t think it generalizes well to real-life data analysis examples.\nBayesian updating: if you observe more data, you can use Bayes’ Rule to update your plausibilities for each of the possible results.\n“There is no free lunch…a Bayesian golem must choose an initial plausibility, and a frequentist golem must choose an estimator. Both golems pay for lunch with their assumptions.”\nBayesian inference doesn’t distinguish between data and parameters in the same way that frequentist inference does. Instead, data are observed variable, and parameters are unobserved variables.\nLikelihoods, priors, posteriors, Bayes’ rule, and other things I didn’t take notes on were covered in this section.\nThere are multiple numerical techniques for approximating Bayesian models, different “motors” for the golems. These include grid approximation, quadratic approximation, and Markov Chain Monte Carlo. How you fit the model is part of the model (the engine is part of the golem), and different fitting routines have different compromises and advantages.\nGrid approximation: estimate the posterior probability of several different values of the parameter via brute force. I did a rough version on this in my blog post on Bayesian updating.\n\nDefine the grid of posterior values. You have to choose the set of points for evaluation.\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the unstandardized posterior at each parameter value, by multiplying the prior and the likliehood.\nStandardize the posterior by dividing each value by the sum of all values.\n\nQuadratic approximation: as the number of parameters increases, the number of evaluations becomes \\(\\text{number of points} ^ \\text{number of parameters}\\). So more efficient methods (that make more assumptions are needed.) Quap assumes that the posterior is approximately Gaussian near the peak, essentially representing the log-posterior density as a quadratic function. N.b. quadratic approximation improves with the number of data points.\n\nFind the posterior mode, usually accomplished by some optimization algorithm based on the gradient of the posterior. “The golem does not know where the peak is, but it does know the slope under its feet.”\nEstimate the curvature near the peak, which is sufficient to compute a quadratic approximation of the entire posterior distribution.\n\nSee pp 41–43 for grid and quadratic approximation examples.\nMarkov chain Monte Carlo: useful for computing many models that fail for grid or quadratic approximation, and may have thousands of parameters. The final posterior may not even have a closed form. MCMC techniques rely on sampling from the posterior distribution rather than directly attempting to approximate the posterior. Since McElreath didn’t run his MCMC example in the book, I’ve included it here because I wanted to see the result.\n\n\nset.seed(370)\nn_samples &lt;- 10000\np &lt;- rep(NA, n_samples)\np[1] &lt;- 0.5\nw &lt;- 6\nl &lt;- 3\nfor (i in 2:n_samples) {\n    p_new &lt;- rnorm(1, p[i-1], 0.1)\n    if (p_new &lt; 0) {p_new &lt;- abs(p_new)}\n    if (p_new &gt; 1) {p_new &lt;- 2 - p_new}\n    q0 &lt;- dbinom(w, w + l, p[i-1])\n    q1 &lt;- dbinom(w, w + l, p_new)\n    p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1])\n}\n\nplot(density(p), xlim = c(0, 1))\ncurve(dbeta(x, w + 1, l + 1), lty = 2, add = TRUE)"
  },
  {
    "objectID": "sr/cp2.html#exercises",
    "href": "sr/cp2.html#exercises",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "Exercises",
    "text": "Exercises\n2E1. The expression corresponding to the statement the probability of rain on Monday is \\[\\text{Pr}(\\text{rain} \\mid \\text{Monday}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{Monday})}.\\]\n2E2. The statement corresponding to the expression \\[\\text{Pr}(\\text{Monday} \\mid \\text{rain})\\] is the probability that it is Monday, given that it is raining.\n2E3. The expression correspondonding to the statement the probability that it is Monday, given that it is raining is \\[ \\text{Pr}(\\text{Monday} \\mid \\text{rain}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{rain})} = \\frac{\\text{Pr}(\\text{rain} \\mid \\text{Monday})\\text{Pr}(\\text{Monday})}{\\text{Pr}(\\text{rain})}\\]\n2E4. Based on Bruno de Finetti’s statement “PROBABILITY DOES NOT EXIST,” the statement the probability of water is 0.7 from the earlier example is a statement about our beliefs. We know that there are several other factors underlying the globe-tossing experiment, but we cannot measure all of those factors, but sweeping them under the rug, we believe that about 70% of the time, our result should be water. The frequentist interpretation of this is as a long-run average probability, but in the bayesian interpretation, this is our prior belief for the next time we perform the experiment.\n2M1. Assuming a uniform prior for \\(p\\), compute the grid approximate posterior for each of the following sets of observations: W, W, W, W, W, W, L, and L, W, W, L, W, W, W.\n\n# Define a function that computes the grid-approximate posterior with uniform\n# prior for p given a sampled number of water and land tosses.\nglobe_post &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post(3, 0)\nglobe_post(3, 1)\nglobe_post(5, 2)\n\n\n\n\n\n\n\n\n2M2. Repeat the grid approximate calculations assuming a prior for \\(p\\) of the form \\[f(p) = \\begin{cases} 0, & p &lt; 0.5 \\\\ k, & p \\geq 0.5\\end{cases}.\\] Note that for \\(\\int_0^1 f(p) \\ dp = 1,\\) we must have \\(k = 2\\).\n\nglobe_post_step_prior &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- ifelse(p_grid &lt; 0.5, 0, 1)\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post_step_prior(3, 0)\nglobe_post_step_prior(3, 1)\nglobe_post_step_prior(5, 2)\n\n\n\n\n\n\n\n\n2M3. We want to compute \\(\\text{Pr}(\\text{Earth} \\mid \\text{land})\\) given the following information.\n\n\\(\\text{Pr}(\\text{land} \\mid \\text{Earth}) = 0.3\\)\n\\(\\text{Pr}(\\text{land} \\mid \\text{Mars}) = 1.0\\)\n\\(\\text{Pr}(\\text{Earth}) = \\text{Pr}(\\text{Mars}) = 0.5\\)\n\nWe can deduce that \\[\\begin{align*}\n\\text{Pr}(\\text{land}) &= \\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth}) + \\text{Pr}(\\text{land} \\mid \\text{Mars})\\cdot\\text{Pr}(\\text{Mars}) \\\\\n&= (0.3)(0.5) + (1.0)(0.5) = 0.65.\n\\end{align*}\\]\nSo we compute \\[\\begin{align*}\n\\text{Pr}(\\text{Earth} \\mid \\text{land}) &= \\frac{\\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth})}{\\text{Pr}(\\text{land})} \\\\\n&= \\frac{(0.3)(0.5)}{0.65} \\approx 0.23.\n\\end{align*}\\]\n2M4. We have a deck of three cards: one with two white sides, one with a black side and a white side, and one with two black sides. If we draw one card with the black side up, what is the probability that the other side is also black?\nWe can solve this by directly calculating the conditional probability.\n\\[\\begin{align*}\n\\text{Pr}(\\text{black down} \\mid \\text{black up}) &= \\frac{\\text{Pr}(\\text{black down}, \\text{black up})}{ \\text{Pr}(\\text{black up})} \\\\\n&= \\frac{1 / 3}{1 / 2} = \\frac{2}{3}.\n\\end{align*}\\]\nWe get \\(\\frac{1}{3}\\) for the joint probability since there are three cards, and only one of them has black on both sides. We get the individual probability of one black side being up as \\(\\frac{1}{2}\\) by noticing that there are 6 sides that could be facing up, and 3 of them are black sides.\nThe way that I think scales better to the rest of the problems in this section is by counting the number of ways to get this answer.\n\nIf the card we drew was white on both sides, there are 0 ways we could observe a black side facing up.\nIf the card we drew was white on one side, there is 1 way to observe a black side facing up.\nIf the card we drew was black on both sides, there are 2 ways to observe a black side facing up (it could be either side).\n\nSo out of the three possible ways to generate the situation we observed, two of them have a second black side on the bottom, giving us our \\(\\frac{2}{3}\\) probability.\n2M5. If we add an extra card with two black sides, we update our calculations.\n\nStill 0 ways if we draw the white/white card.\nThere’s still only 1 way to observe a black side facing up with a B/W card.\nHowever, there are now 4 different black sides we could observe facing up with a B/B card.\n\nSo out of 5 ways to observe a black side facing up, four of them have the other side black, giving us a \\(\\frac{4}{5}\\) probability.\n2M6. Now, we suppose that the black ink is heavy. For every one way to pull the B/B card, there are two ways to pull the B/W card and three ways to pull the W/W card.\nThe number of ways to get one black side up has not changed from problem 2M4. There’s still one way with a B/W card and 2 ways with a B/B card. However, now there are \\(2 \\times 1 = 2\\) ways to get the B/W card with the black side up, so the probability of the other side being black is now \\(\\frac{2}{4} = \\frac{1}{2}\\).\n2M7. Now suppose we draw one card and get a black side facing up, then we draw a second card and get a white side facing up. We want to find the probability that the first card was the B/B card.\n\nThere are two ways for a black side to face up if the first card is B/B (either side could be face up). If this is the case, there are three ways for the second card to show white (one way if it is B/W or two ways if it is W/W). So, there are \\(2 \\times 3 = 6\\) ways for us to observe what we did if this is true.\nIf the first card is B/W, there is only one way for a black side to be face up. Then, there are two ways for the second card to face up white (either side of the W/W card), giving \\(1 \\times 2 = 2\\) ways for our data to occur if this is the truth.\nThe W/W card cannot be the first card, the data we observed rules this out.\n\nTherefore, there is a \\(6 / 8 = 3 /4\\) probability that the first card is black on the bottom as well.\n2H1. There are two species of panda, A and B, that are equally likely in the wild. In species A, twins occur 10% of the time. In species B, twins occur 20% of the time. Both species only have twins or singleton infants. If a panda of unknown species gives birth to twins, what is the probability her next birth will also be twins?\nSo, the probability we are interested in is \\(P(\\text{twins} \\mid \\text{twins})\\). This is confusing, so I’ll say \\(P(\\text{twins}^* \\mid \\text{twins})\\).\nWe can calculate that \\[\\begin{align*}\nP(\\text{twins}) &= P(\\text{twins} \\mid A) P(A) + P(\\text{twins} \\mid B) P(B) \\\\\n&= (0.1)(0.5) + (0.2)(0.5) = 0.15.\n\\end{align*}\\]\nFrom the definition of conditional probability, we know that \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{P(\\text{twins}^*, \\text{twins})}{P(\\text{twins})},\\] so it remains to calculate \\(P(\\text{twins}^*, \\text{twins})\\). If we assume that births are independent, we can calculate\n\\[\\begin{align*}\nP(\\text{twins}^*, \\text{twins}) &= P(\\text{twins}^*, \\text{twins} \\mid A)P(A) + P(\\text{twins}^*, \\text{twins} \\mid B)P(B) \\\\\n&= (0.1)^2(0.5) + (0.2)^2(0.5) = 0.025.\n\\end{align*}\\]\nThen, \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{0.025}{0.15} = \\frac{1}{6}.\\]\nSo if a panda of unknown species gives birth to twins, the probability that her next birth will also be twins is \\(1/6\\), given the information we have.\n2H2. Now we want to find the probability that the panda is species A, given that she gave birth to twins, i.e. \\(P(A \\mid \\text{twins})\\). Recall that \\(P(\\text{twins}) = 0.15,\\) \\(P(\\text{twins} \\mid A) = 0.1,\\) and \\(P(A) = 0.5.\\) Then,\n\\[P(A \\mid \\text{twins}) = \\frac{(0.5)(0.1)}{0.15} = \\frac{1}{3}.\\]\n2H3. Suppose the panda has a second birth, a singleton infant. What is \\(P(A)\\) now?\nSince we’ve already estimated \\(P(A)\\) for the first birth, I’ll call this \\(P(A_\\text{prior})\\), instead of working out the entire conditional probability, we can update this estimate with our new information. What we want to calculate is\n\\[P(A_\\text{posterior}) = \\frac{P(A_\\text{prior}) P(\\text{singleton} \\mid A)}{P(\\text{singleton})}.\\]\nNow, the probability of having a singleton is mutually exclusive with the probability of having twins (since we know that these pandas never have more than twins), so \\[P(\\text{singleton} \\mid A) = 1 - 0.1 = 0.9. \\]\nWe get the singleton probability for pandas of species B in the same way. Next, we need to calculate the probability of a singleton, taking our prior probability that the panda is species A into account. We get \\[P(\\text{singleton}) = \\frac{1}{3} (0.9) + \\frac{2}{3} (0.8) = \\frac{5}{6}.\\]\nTherefore, \\[P(A_\\text{posterior}) = \\frac{(1/3)(0.9)}{5/6} = 0.36.\\]\n2H4. The new test for panda species has probabilities \\(P(\\text{test } A \\mid A) = 0.8\\) and \\(P(\\text{test } B \\mid B) = 0.65.\\) We want to know the probability that our panda is species A, given that her test result was A.\nFirst we will ignore our prior probability and calculate the probability that any random panda is species A given that they test A.\nWe calculate that \\(P(\\text{test } A \\mid B) = 1 - 0.65 = 0.35\\) and therefore that \\[\\begin{align*}\nP(\\text{test } A) &= P(A)P(\\text{test } A \\mid A) + P(B)P(\\text{test } B \\mid B) \\\\\n&= (0.5)(0.8) + (0.5)(0.35) = 0.575.\n\\end{align*}\\]\nThen, \\[P(A \\mid \\text{test } A) = \\frac{(0.5)(0.8)}{0.575} \\approx 0.6957.\\]\nNow, if we use our prior probability, \\(P(A_\\text{prior}) = 0.36\\), we instead get that\n\\[\\begin{align*}\nP(\\text{test } A) &= (0.36)(0.8) + (0.64)(0.35) = 0.512 \\quad\\text{and } \\\\\nP(A \\mid \\text{test } A) &= \\frac{(0.36)(0.8)}{0.512} \\approx 0.5612.\n\\end{align*}\\]"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\n\n\n\n\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "LICENSE.html#preamble",
    "href": "LICENSE.html#preamble",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "LICENSE.html#terms-and-conditions",
    "href": "LICENSE.html#terms-and-conditions",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS"
  },
  {
    "objectID": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "bsfw.html",
    "href": "bsfw.html",
    "title": "Bayesian Statistics the Fun Way",
    "section": "",
    "text": "Notes and solutions for “Bayesian Statistics the Fun Way” by Will Kurt, No Starch Press, 2019.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 19: From Hypothesis Testing to Parameter Estimation\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 18: When Data Doesn’t Convince You\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 17: Bayesian Reasoning in the Twilight Zone\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 16: Introduction to the Bayes factor and posterior odds\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 15: From Parameter Estimation to Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 14: Parameter Estimation with Prior Probabilities\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 13: The Tools of Parameter Estimation\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 12: The Normal Distribution\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 11: Measuring the Spread of Our Data\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 10: Introduction to Averaging and Parameter Estimation\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 9: Bayesian Priors and Working with Probability Distributions\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5: the Beta Distribution\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4: Creating a Binomial Probability Distribution\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8: Priors, posteriors, and likelihoods of Bayes’ Theorem\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7: Bayes’s Theorem with LEGO\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: The Logic of Uncertainty\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Measuring Uncertainty\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Bayesian Thinking and Everyday Reasoning\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bsfw/cp18/index.html",
    "href": "bsfw/cp18/index.html",
    "title": "Chapter 18: When Data Doesn’t Convince You",
    "section": "",
    "text": "Code\noptions(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp18/index.html#q1",
    "href": "bsfw/cp18/index.html#q1",
    "title": "Chapter 18: When Data Doesn’t Convince You",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhen two hypothesis explain the data equally well, one way to change our minds is to see if we can attack the prior probability. What are some factors that might increase your prior belief in your friend’s psychic powers?\n\n\n\n\nWe reality shift to a world where psychic powers exist, because they don’t in this world.\nI go to the gas station with my friend and they buy a single lottery ticket for the exact number that wins the jackpot later that night.\nMy friend bends a spoon or lifts an object with their mind.\nMy friend’s psychic powers transfer to other dice or cards instead of just guessing the result of a six sided die roll."
  },
  {
    "objectID": "bsfw/cp18/index.html#q2",
    "href": "bsfw/cp18/index.html#q2",
    "title": "Chapter 18: When Data Doesn’t Convince You",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nAn experiment claims that when people hear the word Florida they think of the elderly and this has an impact on their walking speed. To test this, we have two groups of 15 students walk across a room: one group hears the word Florida and done does not. Assume $H_1 = $ the groups don’t move at different speeds and $H_2 = $ the Florida group is slower because of hearing the word Florida.\nThe experiment shows that the Bayes factor for \\(H_2\\) over \\(H_1\\) is 19. Suppose someone is unconvinced by this epxeriment because \\(H_2\\) had a lower prior odds? What prior odds would explain someone being unconvinced by this experiment and what would the BF need to be to bring the posterior odds to 50 for this unconvinced person?\nNow suppose the prior odds do not change the skeptic’s mind. Think of an alternate \\(H_3\\) that explains the observation that the Florida group is slower. Remember if \\(H_2\\) and \\(H_3\\) both explain the data equally well, only prior odds in facor of \\(H_3\\) would lead someone to claim \\(H_3\\) is true over \\(H_2\\), so we need to rethink the experiment so that these odds are decreased. Come up with an experiment that could change the prior odds in \\(H_3\\) over \\(H_2\\).\n\n\n\nIf someone were unconvinced by this experiment, we can assume that their posterior odds are, say, three or less. Then their prior odds would need to be \\(3/19\\) or less. If they are totally unconvinced, say their posterior odds are \\(1\\) or less, then the prior odds would need to be \\(1/19\\) or lower to cancel out the Bayes factor entirely. If someone’s prior odds were \\(1/19\\), in order for their posterior odds to be \\(50\\), the bayes factor would need to be \\(19 \\times 50 = 950\\). If their prior odds were \\(3/19\\), the bayes factor would only need to be \\(\\lceil 950 / 3 \\rceil = 317.\\)\nOne alternative hypothesis, which we’ll call \\(H_3\\), could be that the two groups of students were measured at different times: the slower group was measured in the morning. We could fix the experiment by ensuring that both groups of students were measured at the same time, or by getting multiple groups of students to repeat the experiments at the same time (adding replicates to our experiment). It could also be possible (say \\(H_4\\)) that the slower group just contained all slower students. We could fix this by randomizing students into groups and by replicating the experiment multiple times. It seems like replicating the experiment might help no matter what."
  },
  {
    "objectID": "bsfw/cp16/index.html",
    "href": "bsfw/cp16/index.html",
    "title": "Chapter 16: Introduction to the Bayes factor and posterior odds",
    "section": "",
    "text": "Code\noptions(\"scipen\" = 9999, \"digits\" = 4)\nRecall that from Bayes’ theorem, \\[P(H \\mid D) \\propto P(H) \\times P(D\\mid H).\\] If we want to compare the relative likelihood of two events, we do not need the normalizing factor \\(P(D)\\), because it will cancel out, so we can use the ratio \\[\\frac{P(H_1) \\times P(D\\mid H_1)}{P(H_2) \\times P(D\\mid H_2)}.\\] The second part, \\[B = \\frac{P(D\\mid H_1)}{P(D\\mid H_2)},\\] is called the Bayes factor and measures the relative likelihood of two hypothesis if our prior belief in both is equal. The first part, \\[O(H_1) = \\frac{P(H_1)}{P(H_2)},\\] is called the prior odds even though it’s a risk ratio (it is only really the “odds” if \\(H_1\\) and \\(H_2\\) are mutually exclusive which is rarely true in real life). When we combine the two, we get the posterior odds: \\[\\tilde{O}(H_1) = O(H_1) \\times B.\\]"
  },
  {
    "objectID": "bsfw/cp16/index.html#q1",
    "href": "bsfw/cp16/index.html#q1",
    "title": "Chapter 16: Introduction to the Bayes factor and posterior odds",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nReturning to the dice problem, assume that your friend made a mistake and suddenly realized that there were, in fact, two loaded dice and only one fair die. How does this change the prior and therefore the posterior odds for our problem? Are you more willing to believe that they die being rolled is the loaded die?\n\n\n\nThe likelihoods of the die rolls are still the same, so we don’t need to edit that. However, we need to update our prior possibilities to \\[P(H_1) = 2/3; \\text{ and } P(H_2) = 1/3,\\] where \\(H_1\\) is the hypothesis that the die is loaded and \\(H_2\\) is the hypothesis that the die is fair.\nWhen we recompute teh posterior odds, we get \\[\\frac{2/3}{1/3} \\times 3.77 = 2 \\times 3.77 = 7.54,\\] so given the data and our prior knowledge, we now believe that the probability that the rolled die was loaded is \\(7.5 \\times\\) higher than the probability that the rolled die was fair."
  },
  {
    "objectID": "bsfw/cp16/index.html#q2",
    "href": "bsfw/cp16/index.html#q2",
    "title": "Chapter 16: Introduction to the Bayes factor and posterior odds",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nReturning to the rare diseases example, suppose you go to the doctor and after having your ears cleaned you notice that your symptoms persist. Even worse, you have a new symptom: vertigo. The doctor proposes another possible explanation, labyrinthitis, which is a viral infection of the inner ear in which 98 percent of cases involve vertigo. However, hearing loss and tinnitus are less common in this disease; hearing loss occurs only 30 percent of the time and tinnitus occurs only 28 percent of the time. Vertigo is also a possible symptom of vestibular schwannoma, but occurs in only 49 percent of cases. In the general population, 35 people per million contract labyrinthitis annually. What is the posterior odds when you compare the hypothesis that you have labyrinthitis against the hypothesis that you have vestibular schwannoma?\n\n\n\nIf you had vestibular schwannoma (\\(H_\\text{VS}\\)), the probability of experiencing hearing loss, tinnitus, and vertigo would be \\[P(\\text{symptoms} \\mid \\text{VS}) = 0.94 \\times 0.83 \\times 0.49 = 0.38.\\]\nIf you had labyrinthitis (\\(H_\\text{L}\\)), the probability of experiencing all three symptoms simultaneously would be \\[P(\\text{symptoms} \\mid \\text{L}) = 0.30 \\times 0.28 \\times 0.98 = 0.08.\\]\nThe bayes factor for the ratio of labyrinthitis to vestibular schwannoma is then \\[B_{\\text{L} / \\text{VS}} = \\frac{0.08}{0.38} = 0.21,\\] indicating that vestibular schwannoma is about 5 times as likely given your currently symptoms, if both diseases were equally likely. Not good!\nHowever, if we compute the prior odds of labyrinthitis relative to vestibular schwannoma, we get \\[O(\\text{L}) = \\frac{35/1000000}{11/1000000} = 3.18.\\]\nThus, the posterior odds of labyrinthitis relative to vestibular schwannoma is \\[3.18 \\times 0.21 = 0.67,\\] so given our data and the prevalence of both conditions, so unfortunately it seems that vestibular schwannoma is a slightly better explanation than labyrinthitis, but the evidence is not extremely strong either way."
  },
  {
    "objectID": "bsfw/cp14/index.html",
    "href": "bsfw/cp14/index.html",
    "title": "Chapter 14: Parameter Estimation with Prior Probabilities",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp14/index.html#q1",
    "href": "bsfw/cp14/index.html#q1",
    "title": "Chapter 14: Parameter Estimation with Prior Probabilities",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nSuppose you’re playing air hockey with some friends and flip a coin to see who starts with the puck. After playing 12 times, you realize that the friend who brings the coin almost always seems to go first: 9 out of 12 times. Some of your other friends start to get suspicious. Define prior probability distributions for the following beliefs:\n\nOne person who weakly believes that the friend is cheating and the true rate of coming up heads is closer to 70 percent.\nOne person who very strongly trusts that the coin is fair and provided a 50 percent chance of coming up heads.\nOne person who strongly believes the coin is biased to come up heads 70 percent of the time.\n\n\n\n\nWe’ll use a beta distribution for all three of the priors. For the first prior, we want a beta distribution with a mean that’s close to 70 percent, but we want the variance to be fairly high. The easiest way to do this, is to use \\(\\alpha = 7\\) and \\(\\beta = 3\\), which we know will have a mean of exactly \\(70\\%\\) but will still be fairly spread out.\nFor the second prior, we’ll use a beta distribution with equal parameters. Since their belief is fairly strong, we’ll say \\(\\alpha = 10\\), \\(\\beta = 10\\). Finally, for the third person, we’ll use \\(\\alpha = 700\\), \\(\\beta = 300\\), which will have a mean of \\(70\\%\\) but with little variation.\nLet’s plot the three priors. The code for this plot is somewhat long, so it has been hidden by default.\n\n\nCode\n# Setup\npal &lt;- gray.colors(3, start = 0, end = 0.75)\nxs &lt;- seq(1e-5, 1 - 1e-5, length.out = 1000)\nlayout(matrix(c(1, 2), nrow = 1))\n\n# PDF plot\np1 &lt;- dbeta(xs, 7, 3)\np2 &lt;- dbeta(xs, 10, 10)\np3 &lt;- dbeta(xs, 700, 300)\nplot(\n    NULL, NULL,\n    xlab = \"Probability of the coin being heads\",\n    ylab = \"Prior density (truncated at 4)\",\n    xlim = c(0, 1), ylim = c(0, 4),\n    axes = FALSE,\n    main = \"Friends' prior PDFs\"\n)\nlines(xs, p1, lwd = 3, col = pal[[1]], lty = 1)\nlines(xs, p2, lwd = 3, col = pal[[2]], lty = 2)\nlines(xs, p3, lwd = 3, col = pal[[3]], lty = 3)\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 4, 1))\nlegend(\n    x = 0, y = 3.9,\n    legend = c(\"Friend 1\", \"Friend 2\", \"Friend 3\"),\n    col = pal,\n    lty = 1:3,\n    lwd = 3\n)\n\n# CDF plot\nc1 &lt;- pbeta(xs, 7, 3)\nc2 &lt;- pbeta(xs, 10, 10)\nc3 &lt;- pbeta(xs, 700, 300)\nplot(\n    NULL, NULL,\n    xlab = \"Probability of the coin being heads\",\n    ylab = \"Cumulative prior probability\",\n    xlim = c(0, 1), ylim = c(0, 1),\n    axes = FALSE,\n    main = \"Friends' prior CDFs\"\n)\nlines(xs, c1, lwd = 3, col = pal[[1]], lty = 1)\nlines(xs, c2, lwd = 3, col = pal[[2]], lty = 2)\nlines(xs, c3, lwd = 3, col = pal[[3]], lty = 3)\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 1, 0.25))\n\n\n\n\n\n\n\n\n\nWe can see that friend 3 is so certain, the PDF plot had to be truncated in order to see the other two curves.\n\n\n\n\n\n\nTo test the coin, you flip it 20 more times and get 9 heads and 11 tails. Using the priors you calculated in the previous question, what are the updated posterior beliefs in the true rate of flipping a heads in terms of the 95 percent confidence interval?\n\n\n\nOK. now we need to compute the posteriors for each of the three friends.\n\npost1 &lt;- dbeta(xs,  7 + 9,  3 + 11)\npost2 &lt;- dbeta(xs, 10 + 9, 10 + 11)\npost3 &lt;- dbeta(xs, 70 + 9, 30 + 11) \n\nNext we need to calculate the posterior credible intervals.\n\nci_levels &lt;- c(0.025, 0.975)\npost_cis &lt;-\n    rbind(\n        qbeta(ci_levels, 07 + 9, 03 + 11),\n        qbeta(ci_levels, 10 + 9, 10 + 11),\n        qbeta(ci_levels, 70 + 9, 30 + 11)\n    ) |&gt;\n    `colnames&lt;-`(c(\"lower\", \"upper\")) |&gt;\n    tibble::as_tibble() |&gt;\n    tibble::add_column(friend = paste(1:3))\nprint(post_cis)\n\n# A tibble: 3 × 3\n  lower upper friend\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 0.357 0.706 1     \n2 0.324 0.628 2     \n3 0.572 0.740 3     \n\n\nFinally, we can visualize the prior and posterior density with the CrI for each of our friends.\n\n\nCode\n# Setup\npal2 &lt;- gray.colors(2, start = 0, end = 0.5)\nxs &lt;- seq(1e-5, 1 - 1e-5, length.out = 1000)\nlayout(matrix(c(1, 2, 3), nrow = 1))\n\n# posterior/prior plots\n# Friend 1\nplot(\n    NULL, NULL,\n    main = \"Friend 1\",\n    xlim = c(0, 1), ylim = c(0, 10),\n    xlab = \"Probability of the coin being heads\",\n    ylab = \"Density\",\n)\nlines(xs, p1,    lwd = 3, col = pal2[[2]], lty = 3)\nlines(xs, post1, lwd = 3, col = pal2[[1]], lty = 1)\nabline(v = post_cis[[1, 1]], lty = 2, lwd = 1, col = \"firebrick2\")\nabline(v = post_cis[[1, 2]], lty = 2, lwd = 1, col = \"firebrick2\")\n\nlegend(\n    x = 0.02, y = 9.75,\n    legend = c(\"Prior\", \"Posterior\", \"95% CrI\"),\n    col = c(rev(pal2), \"firebrick2\"),\n    lty = c(3, 1, 2),\n    lwd = c(3, 3, 1)\n)\n\n# Friend 2\nplot(\n    NULL, NULL,\n    main = \"Friend 2\",\n    xlim = c(0, 1), ylim = c(0, 10),\n    xlab = \"Probability of the coin being heads\",\n    ylab = \"Density\",\n)\nlines(xs, p2,    lwd = 3, col = pal2[[2]], lty = 3)\nlines(xs, post2, lwd = 3, col = pal2[[1]], lty = 1)\nabline(v = post_cis[[2, 1]], lty = 2, lwd = 1, col = \"firebrick2\")\nabline(v = post_cis[[2, 2]], lty = 2, lwd = 1, col = \"firebrick2\")\n\n# Friend 3\nplot(\n    NULL, NULL,\n    main = \"Friend 3\",\n    xlim = c(0, 1), ylim = c(0, 10),\n    xlab = \"Probability of the coin being heads\",\n    ylab = \"Density\",\n)\nlines(xs, p3,    lwd = 3, col = pal2[[2]], lty = 3)\nlines(xs, post3, lwd = 3, col = pal2[[1]], lty = 1)\nabline(v = post_cis[[3, 1]], lty = 2, lwd = 1, col = \"firebrick2\")\nabline(v = post_cis[[3, 2]], lty = 2, lwd = 1, col = \"firebrick2\")\n\n\n\n\n\n\n\n\n\nOverall, we can see the following conclusions for each friend:\n\nFriend 1 did think the coin was unfair before, but is now much more willing to believe the coin is fair given the observed data;\nFriend 2 thought the coin was fair before and is now more confident that the coin is fair given the observed data; and\nFriend 3 still strongly believes the coin is unfair, but the observed data is beginning to change their mind – they are at least now willing to entertain the possibility that the coin is fair."
  },
  {
    "objectID": "bsfw/cp12/index.html",
    "href": "bsfw/cp12/index.html",
    "title": "Chapter 12: The Normal Distribution",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter introduces the normal distribution. There’s not much else to say about it."
  },
  {
    "objectID": "bsfw/cp12/index.html#q1",
    "href": "bsfw/cp12/index.html#q1",
    "title": "Chapter 12: The Normal Distribution",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat is the probability of observing a value five sigma greater than the mean or more?\n\n\n\nSince we know that this property is invariant for all normal family distributions, we can estimate it numerically using a standard normal distribution, for which the standard deviation is 1. Therefore, we can estimate the probability of interest by calculating \\[P(x \\geq 5) = \\int_{5}^{\\infty} \\mathcal{N}(0, 1) \\ \\mathrm{d}x,\\] which is easy to approximate in R.\n\nprob &lt;- integrate(\\(x) dnorm(x), 5, Inf)$value\nprint(prob)\n\n[1] 0.0000002867\n\n\nThe probability is 0.0000002867, which is quite low, less than a hundred-thousandth of a percent."
  },
  {
    "objectID": "bsfw/cp12/index.html#q2",
    "href": "bsfw/cp12/index.html#q2",
    "title": "Chapter 12: The Normal Distribution",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nA fever is any temperature greater than 100.4 degrees Fahrenheit. Given the following measurements, what is the probability that the patient has a fever?\n\n100.0, 98.8, 101.0, 100.5, 99.7\n\n\n\n\n\ntemps &lt;- c(100.0, 98.8, 101.0, 100.5, 99.7)\n(temp_mean &lt;- mean(temps))\n\n[1] 100\n\n(temp_sd &lt;- sd(temps))\n\n[1] 0.8337\n\n\nAssuming the temperature measurements have normally distributed error away from the true underlying temperature value (which we assume is constant), we can estimate the mean as 100 degrees and the standard deviation as 0.83. Then, the probability that the patient has a fever is \\[ P(\\text{temp} \\geq 100.4) = \\int_{100.4}^{\\infty} \\mathcal{N}(100, 0.83) \\ \\mathrm{d} (\\text{temp}).\\]\n\nfever_prob &lt;- integrate(\\(x) dnorm(x, temp_mean, temp_sd), 100.4, Inf)\nprint(fever_prob)\n\n0.3157 with absolute error &lt; 0.0000009\n\n\nGiven all of our assumptions and measurements, the probability that the patient has a fever is about 31.57 percent."
  },
  {
    "objectID": "bsfw/cp12/index.html#q3",
    "href": "bsfw/cp12/index.html#q3",
    "title": "Chapter 12: The Normal Distribution",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nSuppose in Chapter 11 we tried to measure the depth of a well by timing coin drops and we got the following values:\n\n2.5, 3.0, 3.5, 4.0, 2.0.\n\nThe distance an object falls can be calculated (in meters) with the formula \\[\\text{distance} = \\frac{1}{2}\\times G \\times \\text{time}^2\\] where \\(G = 9.8 \\ \\text{m}/\\text{s}^2,\\) the gravitational constant. What is the probability that the well is over 500 meters deep?\n\n\n\nFirst, we need to calculate the fall distances we would have gotten from these times.\n\ntimes &lt;- c(2.5, 3.0, 3.5, 4.0, 2.0)\ndistances &lt;- 0.5 * 9.8 * times ^ 2\nknitr::kable(data.frame(\"t\" = times, \"d\" = distances))\n\n\n\n\nt\nd\n\n\n\n\n2.5\n30.62\n\n\n3.0\n44.10\n\n\n3.5\n60.02\n\n\n4.0\n78.40\n\n\n2.0\n19.60\n\n\n\n\n\nNow, assuming that the errors in measuring the distance are normally distributed, we can calculate the mean and standard deviation.\n\n(dist_mean &lt;- mean(distances))\n\n[1] 46.55\n\n(dist_sd &lt;- sd(distances))\n\n[1] 23.36\n\n\nFinally, we compute the probability by integrating from 500 to infinity.\n\nprob_500_depth &lt;- integrate(\\(x) dnorm(x, dist_mean, dist_sd), 500, Inf)\nprint(prob_500_depth)\n\n0.000000000000000000000000000000000000000000000000000000000000000000000000000000000002872 with absolute error &lt; 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000034\n\n\nThe probability might as well be 0, it’s actually somewhere around \\(10 ^ {-83.54}\\)."
  },
  {
    "objectID": "bsfw/cp12/index.html#q4",
    "href": "bsfw/cp12/index.html#q4",
    "title": "Chapter 12: The Normal Distribution",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nWhat is the probability there is no well (i.e. the well is really 0 meters deep)? You’ll notice that probability is higher than you might expect, given your observation is that there is a well. There are two good explanations for this probability being higher than it should. The first is that the normal distribution is a poor model for our measurements, the second is that, when making up numbers for an example, I chose values that you likely wouldn’t see in real life. Which is more likely to you?\n\n\n\nThe question is ill-posed. The probability that the depth of the well is 0 meters is 0, because the probability of any single point is 0, since we’ve assumed a continuous distribution for the measurements. We have to approximate the probability by finding the probability of a small neighborhood around zero, which is similar but not quite the same thing. If we instead interpret the question to mean the depth of the well is zero or less, we can integrate from -infinity to zero.\n\nprob_0_depth &lt;- integrate(\\(x) dnorm(x, dist_mean, dist_sd), -Inf, 0)\nprint(prob_0_depth)\n\n0.02312 with absolute error &lt; 0.0000028\n\n\nAccording to our model, there is almost a \\(2.31 \\%\\) chance that the well has a zero or negative depth! Which clearly doesn’t make sense. Notably, in the official solution, the author models the times instead of the distances, but also gets a nonsensical result.\nClearly, a normal distribution is not appropriate here. Neither times nor distances can be negative, and yet using a normal distribution for either gives positive probabilities of negative results. Furthermore, the data are incredibly spread out on the distance scale, implying that our measurements are not very good. So both problems are true.\nThe official solution claims that there is “no reason to question the assumptions of the normal distribution here”, but that is an obviously untrue statement."
  },
  {
    "objectID": "bsfw/cp10/index.html",
    "href": "bsfw/cp10/index.html",
    "title": "Chapter 10: Introduction to Averaging and Parameter Estimation",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter covers the basics of parameter estimation, particularly focusing on how to compute unweighted and weighted averages (resulting in the formula for the discrete expected value)."
  },
  {
    "objectID": "bsfw/cp10/index.html#q1",
    "href": "bsfw/cp10/index.html#q1",
    "title": "Chapter 10: Introduction to Averaging and Parameter Estimation",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nIt’s possible to get errors that don’t quite cancel out the way we want. In the Fahrenheit temperature scale, 98.6 degrees is the normal body temperature and 100.4 degrees is the typical threshold for a fever. Say you are taking care of a child that feels warm and seems sick, but you take repeated readings from the thermometer yourself and get several readings between 97.5 and 98. What could be wrong with the thermometer?\n\n\n\nThe therometer might be miscalibrated, giving us errors which are systematically biased to be lower than the true values we are tying to measure. So even if the child has a fever, the therometer might not tell us that."
  },
  {
    "objectID": "bsfw/cp10/index.html#q2",
    "href": "bsfw/cp10/index.html#q2",
    "title": "Chapter 10: Introduction to Averaging and Parameter Estimation",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nGiven that you feel healthy and have traditionally had a very consistently normal temperature, how could you alter the measurements 100, 99.5, 99.6, and 100.2 to estimate if the child has a fever?\n\n\n\nI don’t really think the wording of this question makes any sense, it’s hard to tell what it is actually asking. If we had a consistently normal temperature and weren’t sick, but we got those measurements, we could subtract the bias in the measurement to correct for it. For example, if we subtract 1.5 degrees from all of those measurements, we get temperatures that are all in a normal range, indicating that the thermometer might be miscalibrated.\nHowever, the question asks how we can use this information to measure whether the child has a fever, but in question 1 we were worried about the thermometer giving erroneously low readings. So it’s not really clear what the question wants us to do. Either way, if we have a way to estimate the bias of the thermometer (e.g. by taking temperatures with multiple thermometers or by comparing the current readings with earlier readings when the child was healthy) we could use that bias estimate to correct the measurements we got. However, that would assume that the bias is constant with the temperatures which is not necessarily true, the thermometer might be really bad at reading temperatures above a certain threshold instead. We don’t have nearly enough information in this problem to figure out what is wrong with the child other than they feel warm and seem sick, so we should instead use our best non-thermometer related judgement to decide what to do."
  },
  {
    "objectID": "bsfw/cp08/index.html",
    "href": "bsfw/cp08/index.html",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes’ Theorem",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter is mostly about Bayes’ factors without saying that it’s about Bayes factors, but it also discusses what happens when prior beliefs change (in a particularly simple case). I personally don’t think this chapter generalizes very well to more complex analyses, but I guess the point of this book is to be beginner-friendly. So maybe I’m at the point on my Bayes journey where I am confident beyond what I actually know ¯\\(ツ)/¯."
  },
  {
    "objectID": "bsfw/cp08/index.html#q1",
    "href": "bsfw/cp08/index.html#q1",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes’ Theorem",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nAs mentioned, you mighty disagree with the original probability assigned to the likelihood:\n\\[P(\\text{broken window, open front door, missing laptop} \\mid \\text{robbed}) = \\frac{3}{10}.\\]\nIf we change this probability to \\(1/100\\), how does this change our strength in believing \\(H_1\\) over \\(H_2\\)?\n\n\n\n(I bolded the part that was supposed to be in the question but then wasn’t.) If we do this, it only changes the numerator of our ratio and we get that \\[B = \\frac{3 / 100000}{1 / 21,900,000} = 657. \\] So this decreases our ratio by a factor of 10, the same amount that our beliefs changed."
  },
  {
    "objectID": "bsfw/cp08/index.html#q2",
    "href": "bsfw/cp08/index.html#q2",
    "title": "Chapter 8: Priors, posteriors, and likelihoods of Bayes’ Theorem",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nHow unlikely would you have to believe being robbed is — our prior for \\(H_1\\) — in order for the ratio of \\(H_1\\) to \\(H_2\\) to be one?\n\n\n\nCall our unknown prior probability \\(p\\). Then we solve: \\[\\frac{p \\ \\frac{3}{100}}{1 / 21,900,000} = 1 \\implies p = \\frac{1}{657,000}.\\] So in order for the ratio to be 1, we would need to believe that getting robbed is 657 times less likely than the previous problem – note that we could have gotten the same answer by dividing our prior probability by the Bayes factor from the previous problem."
  },
  {
    "objectID": "bsfw/cp06/index.html",
    "href": "bsfw/cp06/index.html",
    "title": "Chapter 6: Conditional Probability",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 6)\nThis chapter introduces the concept of conditional probability, which, in my opinion, is one of the least intuitive parts of probability. I didn’t take a lot of notes here, but I did at least write down what is covered in this section. For a more detailed explanation of conditional probability, I recommend either Probability for the Enthusiastic Beginner by Morin or Mathematical Statistics with Applications by Wackerly et al.. (I make no money from these links, sales, or promotions.)"
  },
  {
    "objectID": "bsfw/cp06/index.html#q1",
    "href": "bsfw/cp06/index.html#q1",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat piece of information would we need in order to use Bayes’ theorem to determine the probability that someone in 2010 who had GBS also had the flu vaccine that year?\n\n\n\nThis is another question that I have beef with. I think it is reasonable would interpret “the probability that someone in 2010 who had GBS also had the flu vaccine that year” as \\(P(\\text{GBS} \\cap \\text{flu vaccine}).\\) But the book wants us to read this as \\(P(\\text{flu vaccine} \\mid \\text{GBS}),\\) which I would tend to word as (unambiguously) “the probability that someone in 2010 received the flu vaccine, given that they had GBS.” Since conditional/joint probabilities are often vague like this, I think it is best to be specific instead of just making the word also italicized.\nBut anyways. The formula for this calculation is \\[P(\\text{flu vaccine} \\mid \\text{GBS}) = \\frac{P(\\text{flu vaccine}) \\\nP(\\text{GBS} \\mid \\text{flu vaccine})}{P(\\text{GBS})},\\] so the main thing we are missing is \\(P(\\text{GBS})\\), which would allow us to calculate \\(P(\\text{GBS} \\cap \\text{flu vaccine})\\), the only other part we were not given in the chapter."
  },
  {
    "objectID": "bsfw/cp06/index.html#q2",
    "href": "bsfw/cp06/index.html#q2",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nWhat is the probability that a random person picked from the population is female and is not color blind?\n\n\n\nMost of the details for this problem are given in the chapter. Note that for this example, the book is talking about the joint probability, despite what I think is very similar wording to the previous question, but maybe I need to work on my reading comprehension. Anyways, the relevant formula would be\n\\[P(\\text{female} \\cap \\lnot \\text{color blind}) = P(\\text{female}) \\ P(\\lnot\n\\text{color blind} \\mid \\text{female}).\\]\nAssuming male individuals and female individuals are equally likely in the population and are the only two options (neither of which is true, but they are both assumed to be true by the book for this question), we have all of the information that we need to solve this. We get\n\\[P(\\text{female} \\cap \\lnot \\text{color blind}) = (0.5)(1 - 0.005) = 0.4975.\\] So the probability is \\(49.75\\%,\\) since the incidence of color blindness among female members of the population is extremely low."
  },
  {
    "objectID": "bsfw/cp06/index.html#q3",
    "href": "bsfw/cp06/index.html#q3",
    "title": "Chapter 6: Conditional Probability",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nWhat is the probability that a male who received the flu vaccine in 2010 is either color blind or has GBS?\n\n\n\nI’ll steal from the notation that the book uses here, and let \\(A\\) be the event that this individual is colorblind, given that they are male, and let \\(B\\) be the event that this individual has GBS, given that they got the flu vaccine. Note that this makes several independence assumptions (being male or color blind are both independent of the flu vaccine and GBS, and vice versa), because if we don’t make this independence assumptions we would lack the necessary information to solve this problem.\nNow, the probability we want to find is \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B) - P(A) P(B \\mid A).\\]\nIn the chapter, we previously learned the values for \\(P(A)\\) and \\(P(B)\\), but I think this is where the problem starts to break down. The chapter hasn’t dealt with any really complicated events like this at all, and it doesn’t really make sense to say that \\[P(B \\mid A) = P( (\\text{GBS} \\mid \\text{flu vaccine}) \\mid (\\text{color\nblind} \\mid \\text{male}) ).\\]\nSo of course the book does not discuss how to address this problem, as it really isn’t very well-formed, it just says to assume \\(P(B \\mid A) = P(B).\\) This is true if we make all of the independence assumptions above, but again, stating the problem in this way doesn’t really make sense.\nSo, I’ll detour and work through the problem in a way that I think is a bit more logical and arrives at the same answer.\nWhen I read the question, the probability that I think we want to get is \\[P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine}).\\] This is much more complex than any of the examples in the book, but fortunately for us, we can work it out using the laws of set theory that the book has not discussed at all. Note that, by definition of conditional probability,\n\\[\nP(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine})\n= \\frac{P\\left[(\\text{color blind} \\cup \\text{GBS}) \\cap (\\text{male} \\cap \\text{flu vaccine}) \\right]}{P(\\text{male} \\cap \\text{flu vaccine})}.\n\\] Now, the denominator is pretty easy to sort out. Since we don’t have any information on how being male affects flu vaccine coverage, we’ll assume they are independent (although in real life, this is not true – CDC flu vaccine coverage data for the USA suggests that males under 65 are less likely to get flu vaccines than females of the same age, but this different goes away for the elderly). Under this assumption, we get \\[P(\\text{male} \\cap \\text{flu vaccine}) = P(\\text{male}) \\ P(\\text{flu\nvaccine}),\\] which is easy enough to calculate with the data we have.\nSo it remains to deal with the beast of a numerator. For ease-of-use here, let’s define the event \\(C \\equiv \\text{color blind} \\cup \\text{GBS}\\) and the event \\(D \\equiv \\text{male} \\cap \\text{flu vaccine}.\\) Then, we get that\n\\[\nP(C \\cap D) = \\left(\\text{color blind} \\cup \\text{GBS}\\right) \\cap D =\n(\\text{color blind} \\cap D) \\cup (\\text{GBS} \\cap D)\n\\] by using the distributive rules of the \\(cap\\) and \\(cup\\) operators. These are not covered in the book, but one potential resource (in terms of set theory) is here. Many books on probability will also briefly cover this material.\nNow, we have a union, and we can use the addition (or inclusion-exclusion) principle to evaluate the probability. We get that\n\\[\\begin{align*}\nP((\\text{color blind} \\cap D) \\cup (\\text{GBS} \\cap D)) &=\nP(\\text{color blind} \\cap D) + P(\\text{GBS} \\cap D) - P(\\text{color blind}\n\\cap \\text{GBS} \\cap D) \\\\\n&=  P(\\text{color blind} \\cap \\text{male} \\cap \\text{flu vaccine}) + P(\\text{GBS} \\cap \\text{male} \\cap \\text{flu vaccine}) \\\\ &\\quad \\quad - P(\\text{color blind}\n\\cap \\text{GBS} \\cap (\\text{male} \\cap \\text{flu vaccine})).\n\\end{align*}\\]\nUnder the same set of independence assumptions before, we can make the following set of simplifications: \\[\\begin{align*}\nP(\\text{color blind} \\cap \\text{male} \\cap \\text{flu vaccine}) &=\n    P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\nP(\\text{GBS} \\cap \\text{male} \\cap \\text{flu vaccine}) &=\n    P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\nP(\\text{color blind} \\cap \\text{GBS} \\cap\n    (\\text{male} \\cap \\text{flu vaccine})) &=\n    P(\\text{GBS} \\cap \\text{flu vaccine} \\cap \\text{male} \\cap\n    \\text{color blind}) \\\\\n    &= P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine}).\n\\end{align*}\\]\nNow, it would be just about impossible to write all that out in text format and still have it fit into the margins of this page. But nonethless we will try. We get that, overall,\n\n\\[\\begin{equation*}P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine}) =\n\\frac{\n\\left(\\begin{split}\n    &P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\n    &\\quad + P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\n    &\\quad - P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine})\n\\end{split}\\right)\n}{\nP(\\text{male}) \\ P(\\text{flu\nvaccine})\n}.\n\\end{equation*}\\]\n\nI can’t figure out how to make the text of that smaller in this quarto HTML document and it’s too much work to figure out how to size the LaTeX code in a way that’s supported by MathJax so that’s the best that you’re getting from me.\nAnyways, if we want to solve this with the information given in the book, we have to get rid of all of the \\(P(\\text{flu vaccine})\\), since we don’t know that. Fortunately for us, they cancel out. First, let’s plug in the numbers that we do know so this is a bit less cumbersome to type. Let\n\\[\\mathrm{P} = P(\\text{color blind} \\cup \\text{GBS} \\mid \\text{male} \\cap \\text{flu vaccine})\\]\nfor convenience (should’ve said this earlier but it’s too late now). We get \\[\\begin{align*}\n\\mathrm{P} & =\n\\frac{\n\\left(\\begin{split}\n    &P(\\text{color blind} \\cap \\text{male}) \\ P(\\text{flu vaccine}) \\\\\n    &\\quad + P(\\text{GBS} \\cap \\text{flu vaccine}) \\ P(\\text{male}) \\\\\n    &\\quad - P(\\text{male} \\cap \\text{color blind}) \\ P(\\text{GBS} \\cap\n    \\text{flu vaccine})\n\\end{split}\\right)\n}{\nP(\\text{male}) \\ P(\\text{flu\nvaccine})\n} \\\\ \\\\ &= \\frac{\n0.04 \\ P(\\text{FV}) + 0.5 \\ P(\\text{GBS} \\mid \\text{FV}) \\ P(\\text{FV}) -\n    0.04 \\ P(\\text{GBS} \\mid \\text{FV}) \\ P(\\text{FV})\n}{\n0.5 \\ P(\\text{FV})\n} \\\\ &=\n\\frac{P(\\text{FV})\\left( 0.04 + 0.46 \\ P(\\text{GBS} \\mid \\text{FV}) \\right)}{\n0.5 \\ P(\\text{FV})\n} \\\\\n&= \\frac{0.04 + 0.46 (3 / 100,000)}{0.5} \\\\\n&= 0.0800276 = 8.00276\\%.\n\\end{align*}\\]\nThis is the same answer that the book has, so their shortcut that I don’t like apparently works (and saves a lot of time). But I had a good time proving that it was true without shortcuts, so I guess it was worth it.\nWell, that’s all for this chapter!"
  },
  {
    "objectID": "bsfw/cp04/index.html",
    "href": "bsfw/cp04/index.html",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp04/index.html#q1",
    "href": "bsfw/cp04/index.html#q1",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat are the parameters of the binomial distribution for the probability of rolling either a 1 or a 20 on a 20-sided die, if we roll the die 12 times?\n\n\n\nThe parameters are \\(n = 12\\) and \\(p = 2/20 = 1/10 = 0.1\\)."
  },
  {
    "objectID": "bsfw/cp04/index.html#q2",
    "href": "bsfw/cp04/index.html#q2",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nThere are four aces in a deck of 52 cards. If you pull a card, return the card, then reshuffle, and pull a card again, how many ways can you pull just one ace in five pulls?\n\n\n\nSince we’re drawing with replacement, we are doing \\(5\\) Bernoulli trials, each with a probability of \\(4 / 52 = 1 / 13\\). So we can use the Binomial distribution to get this probability. We can get that\n\\[P(1 \\text{ ace in } 5 \\text{ pulls}) = B\\left( x = 1 \\mid n = 5, \\ p = \\frac{1}{13} \\right),\\]\nwhich we would then calculate as\n\\[P(1 \\text{ ace in } 5 \\text{ pulls}) = \\left( 5\\atop{1} \\right) \\left(\\frac{1}{13}\\right)^1\\left(\\frac{12}{13}\\right)^4.\\]\nTo get the answer, we can certainly write out the entire formula in R.\n\nchoose(5, 1) * (1 / 13) ^ 1 * (12 / 13) ^ 4\n\n[1] 0.2792\n\n\nBut here’s a sneaky trick. Since R was designed for statistical computing, a fast version of the binomial distribution is already built in.\n\ndbinom(x = 1, size = 5, prob = 1 / 13)\n\n[1] 0.2792"
  },
  {
    "objectID": "bsfw/cp04/index.html#q3",
    "href": "bsfw/cp04/index.html#q3",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nFor the example in question 2, what is the probability of pulling 5 aces in 10 pulls (remember the card is shuffled back in the deck when it is pulled)?\n\n\n\nThis time I’ll just use the R calculation, since this problem is worked out exactly the same way. The probability we want to estimate is \\[P(5 \\text{ aces in } 10 \\text{ pulls}) = B\\left( x = 5 \\mid n = 10, \\ p = \\frac{1}{13} \\right),\\] which we can approximate in R.\n\ndbinom(x = 5, size = 10, prob = 1 / 13)\n\n[1] 0.0004549\n\n\nNote that this probability is quite small – that is because both drawing this many aces is difficult, and because this is the probability of drawing EXACTLY five aces."
  },
  {
    "objectID": "bsfw/cp04/index.html#q4",
    "href": "bsfw/cp04/index.html#q4",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nWhen you’re searching for a new job, it’s always helpful to have more than one offer on the table so you can use it in negotiations. If you have a 1/5 probability of receiving a job offer when you interview, and you interview with seven companies in a month, what is the probability you’ll have at least two competing offers by the end of that month.\n\n\n\nThis time we have to consider more than just one of these probabilities, because we want the probability of at least two offers. Let \\(O\\) be the number of competing offers we receive. Then we can write \\[P(O \\geq 2) = B\\left(O = 2 \\mid n = 7, p = \\frac{1}{5}\\right) + \\ldots + B\\left(O = 7 \\mid n = 7, p = \\frac{1}{5}\\right),\\] which we could also write more compactly as \\[P(O \\geq 2) = \\sum_{o = 2}^7 B\\left(O = o \\mid n = 7, p = \\frac{1}{5}\\right).\\] Since dbinom() is vectorized, we can calculate this pretty easily in R.\n\nsum( dbinom(x = 2:7, size = 7, prob = 1 / 5) )\n\n[1] 0.4233\n\n\nOf course, as you might have guessed, R has a shortcut for this: the pbinom function. This way is a little bit tricky though, because the way we have to specify the probability we want is a little more complicated than we would hope.\nThe lower.tail parameter specifies whether we want the cumulative probability above (FALSE) or below (TRUE) the argument(s) for the q parameter. For above, the probability is \\(P(X &gt; x)\\), but for below, it is \\(P(X \\leq x)\\). Since we have a positive probability at \\(P(X = x)\\), we can’t just throw in 2 as the boundary, it will not give the correct answer. Instead we can use this function to calculate \\[P(X \\geq 2) = 1 - P(X &lt; 2) = 1 - P(X \\leq 1) = P(X &gt; 1).\\] This last two forms are the two that we can get with pbinom, as I’ll calculate below.\n\n# P(O &gt; 1)\npbinom(q = 1, size = 7, prob =  1 / 5, lower.tail = FALSE)\n\n[1] 0.4233\n\n# 1 - P(O &lt;= 1)\n1 - pbinom(q = 1, size = 7, prob = 1 / 5)\n\n[1] 0.4233\n\n\nOf course all three of these solutions are equivalent, so no matter which way we do it, we get a probability of about \\(42\\%\\)."
  },
  {
    "objectID": "bsfw/cp04/index.html#q5",
    "href": "bsfw/cp04/index.html#q5",
    "title": "Chapter 4: Creating a Binomial Probability Distribution",
    "section": "Q5",
    "text": "Q5\n\n\n\n\n\n\nYou get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know that this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you’re tired. You really don’t want to go on this many interviews unless you are at least twice as likely to get at least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7?\n\n\n\nSo, what we actually want to calculate is the ratio \\[\\frac{P\\left( O \\geq 2 \\mid n =  7, \\ p = \\frac{1}{ 5} \\right)}\n       {P\\left( O \\geq 2 \\mid n = 25, \\ p = \\frac{1}{10} \\right)}.\\] In the previous problem, we calculated the numerator, so now we need to calculate the denominator. Fortunately, we can do this the same way, using the same practical considerations we did less time. Just to make this issue about the boundary more clear, I’ll write out the R code for doing it all three ways.\n\nsum(dbinom(2:25, size = 25, prob = 1 / 10))\n\n[1] 0.7288\n\npbinom(q = 1, size = 25, prob = 1 / 10, lower.tail = FALSE)\n\n[1] 0.7288\n\n1 - pbinom(q = 1, size = 25, prob = 1 / 10)\n\n[1] 0.7288\n\n\nWe can then approximate the ratio we are interested in as \\[\\frac{42\\%}{73\\%} \\approx 0.58 &gt; 0.50,\\] and I just realized that it probably makes more sense to invert this ratio. So let’s do that: \\[\\frac{73\\%}{42\\%} \\approx 1.74 &lt; 2,\\] so we will improve our chances of getting at least two competing offers, but we will not be at least twice as likely, so we want to stick with just \\(7\\) interviews instead of \\(25\\). :::"
  },
  {
    "objectID": "bsfw/cp02/index.html",
    "href": "bsfw/cp02/index.html",
    "title": "Chapter 2: Measuring Uncertainty",
    "section": "",
    "text": "What is the probability of rolling two six-sided dice and getting a value greater than 7?\n\n\n\nIn general, I think it’s worth solving this problem via simulation, because solving via enumeration scales much worse. (We could also solve this problem analytically, but that scales even worse than the enumerative solution.) So first I’ll show how to solve via simulation.\n\n# The number of simulations we will do. As this number gets larger, our\n# simulated probability will approach the theoretical probability.\nN_rolls &lt;- 10000\n\n# Simulate the two die rolls. You could do this with one call of sample but\n# for expository purposes this is easier.\ndie_1 &lt;- sample(1:6, N_rolls, replace = TRUE)\ndie_2 &lt;- sample(1:6, N_rolls, replace = TRUE)\n\n# Calculate the proportion of sums that are greater than 7\nsum &lt;- die_1 + die_2\nmean(sum &gt; 7)\n\n[1] 0.4282\n\n\nWe can also use R to solve this problem by enumeration.\n\npossible_rolls &lt;- expand.grid(die_1 = 1:6, die_2 = 1:6)\npossible_rolls$sum &lt;- possible_rolls$die_1 + possible_rolls$die_2\n\nmean(possible_rolls$sum &gt; 7)\n\n[1] 0.4166667\n\n\nAs you can see, our simulated solution is extremely close to the “true” answer that we get by enumerating the sample space. If you prefer the answer as a fraction, we can also just count.\n\n# Number of possible combinations that sum to &gt; 7\nsum(possible_rolls$sum  &gt; 7)\n\n[1] 15\n\n# Number of possible combinations\nnrow(possible_rolls)\n\n[1] 36\n\n\nSo 15 out of the 36 possible combinations have a sum greater than 7, giving us the observed probability \\(0.41\\bar{6}.\\)"
  },
  {
    "objectID": "bsfw/cp02/index.html#q1",
    "href": "bsfw/cp02/index.html#q1",
    "title": "Chapter 2: Measuring Uncertainty",
    "section": "",
    "text": "What is the probability of rolling two six-sided dice and getting a value greater than 7?\n\n\n\nIn general, I think it’s worth solving this problem via simulation, because solving via enumeration scales much worse. (We could also solve this problem analytically, but that scales even worse than the enumerative solution.) So first I’ll show how to solve via simulation.\n\n# The number of simulations we will do. As this number gets larger, our\n# simulated probability will approach the theoretical probability.\nN_rolls &lt;- 10000\n\n# Simulate the two die rolls. You could do this with one call of sample but\n# for expository purposes this is easier.\ndie_1 &lt;- sample(1:6, N_rolls, replace = TRUE)\ndie_2 &lt;- sample(1:6, N_rolls, replace = TRUE)\n\n# Calculate the proportion of sums that are greater than 7\nsum &lt;- die_1 + die_2\nmean(sum &gt; 7)\n\n[1] 0.4282\n\n\nWe can also use R to solve this problem by enumeration.\n\npossible_rolls &lt;- expand.grid(die_1 = 1:6, die_2 = 1:6)\npossible_rolls$sum &lt;- possible_rolls$die_1 + possible_rolls$die_2\n\nmean(possible_rolls$sum &gt; 7)\n\n[1] 0.4166667\n\n\nAs you can see, our simulated solution is extremely close to the “true” answer that we get by enumerating the sample space. If you prefer the answer as a fraction, we can also just count.\n\n# Number of possible combinations that sum to &gt; 7\nsum(possible_rolls$sum  &gt; 7)\n\n[1] 15\n\n# Number of possible combinations\nnrow(possible_rolls)\n\n[1] 36\n\n\nSo 15 out of the 36 possible combinations have a sum greater than 7, giving us the observed probability \\(0.41\\bar{6}.\\)"
  },
  {
    "objectID": "bsfw/cp02/index.html#q2",
    "href": "bsfw/cp02/index.html#q2",
    "title": "Chapter 2: Measuring Uncertainty",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nWhat is the probability of rolling three six-sided dice and getting a value greater than seven?\n\n\n\nWe can solve this in the same way. We’ll do it just via enumeration this time, since this book is about probability, not about simulations.\n\npossible_rolls &lt;- expand.grid(die_1 = 1:6, die_2 = 1:6, die_3 = 1:6)\npossible_rolls$sum &lt;- possible_rolls$die_1 + possible_rolls$die_2 +\n    possible_rolls$die_3\n\nsum(possible_rolls$sum &gt; 7)\n\n[1] 181\n\nnrow(possible_rolls)\n\n[1] 216\n\nmean(possible_rolls$sum &gt; 7)\n\n[1] 0.837963\n\n\nSo our answer is \\(181 / 216 \\approx 0.84.\\)"
  },
  {
    "objectID": "bsfw/cp02/index.html#q3",
    "href": "bsfw/cp02/index.html#q3",
    "title": "Chapter 2: Measuring Uncertainty",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nThe Yankees are playing the Red Sox. You’re a diehard Sox fan and bet your friend they’ll win the game. You’ll pay your friend $30 if the Sox lose and your friend will have to pay you only $5 if the Sox win. What is the probability you have intuitively assigned to the belief that the Red Sox will win?\n\n\n\nTo solve this problem, we just use the formula that relates the odds to the probability of an event.\n\\[\\begin{align*}\n\\frac{P(x)}{P(\\lnot x)} &= \\frac{30}{5} \\\\\n\\frac{P(x)}{1 - P(x)} &= \\frac{30}{5} \\\\\nP(x) &= \\frac{30}{5} \\left( 1 - P(x) \\right) \\\\\nP(x) &= \\frac{30}{5} - \\frac{30}{5} P(x) \\\\\nP(x) + \\frac{30}{5} P(x) &= \\frac{30}{5} \\\\\n\\frac{35}{5} P(x) &= \\frac{30}{5} \\\\\n7 \\cdot P(x) &= 6 \\\\\nP(x) &= \\frac{6}{7}.\n\\end{align*}\\]"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "Data Analysis Using Regression and Multilevel/Hierarchical Modeling",
    "section": "",
    "text": "Notes and solutions for “Data Analysis Using Regression and Multilevel/Hierarchical Modeling” by Andrew Gelman and Jennifer Hill, Cambridge University Press, 2007.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Linear regression: the basics\n\n\n\n\n\n\n\n\n\n\n\nZane Billings\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2: Concepts and methods from basic probability and statistics\n\n\n\n\n\n\n\n\n\n\n\nZane Billings\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "arm/cp2/index.html",
    "href": "arm/cp2/index.html",
    "title": "Chapter 2: Concepts and methods from basic probability and statistics",
    "section": "",
    "text": "Well, since I finished Bayesian Statistics the Fun Way and I’m still in the middle of Statistical Rethinking, it’s time to start a new book, and this one seemed like a good choice. I use multilevel models so often that learning them a bit more formally will probably be useful. I expect the first part of this book to be pretty fast."
  },
  {
    "objectID": "arm/cp2/index.html#q1",
    "href": "arm/cp2/index.html#q1",
    "title": "Chapter 2: Concepts and methods from basic probability and statistics",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nA test is graded from 0 to 50, with an average score of 35 and a standard deviation of 10. For comparison to other tests, it would be convenient to rescale to a mean of 100 and standard deviation of 15.\n\nHow can the scores be linearly transformed to have this new mean and standard deviation?\nThere is another linear transformation that also rescales the scores to have mean 100 and standard deviation 15. What is it, and why would you not want to use it for this purpose?\n\n\n\n\nIn order to rescale to a new mean and standard deviation, the easiest way for me to do this conceptually is to convert to z-scores, then untransform and that will give us the correct linear transformation. Letting \\(x\\) represent the data vector of test scores, we get \\[\n\\left(\\frac{x - 35}{10}\\right) \\times 15 + 100 = 47.5 + 1.5x.\n\\]\nNotably, the \\(bx\\) part of the transformation transforms the standard deviation to \\(\\left| b \\right|\\sigma_x\\), which implies that the transformation \\(-bx\\) will also give the correct standard deviation. Noting that \\(-1.5x\\) would have a mean of \\(-1.5 \\times 35 = -52.5\\), so the appropriate linear transformation which then sets the mean to 100 would be \\[\n152.5 - 1.5x.\n\\] We do not want to multiply by a negative number in our linear transformation because it will invert the order statistics of the new distribution. That is, calling our first transformation \\(g_1(\\cdot)\\) and the second transformation \\(g_2(\\cdot)\\), if \\(x_1 &lt; x_2\\) we would get that \\(g_1(x_1) &lt; g_1(x_2)\\) but \\(g_2(x_1) &gt; g_2(x_2)\\). For example:\n\\[\n\\begin{aligned}\ng_1(10) = 62.5 ,&\\quad  g_1(15) = 70; \\\\\ng_2(10) = 137.5 ,&\\quad g_2(15) = 130.\n\\end{aligned}\n\\] This is not desirable because it would be very confusing.\n\n\n\n\n\n\nThe girls data contains the proportions of girl births in Vienna for each month in 1908 and 1909 (out of an average of 3900 births per month). von Mises (1957) used these proportions to claim that the sex ratios are less variable than would be expected by chance.\n\nCompute the standard deviation of these proportions and compare to the standard deviation that would be expected if the sexes of babies were independently distributed with a constant probability over the 24-month period.\nThe actual and theoretical standard deviations from (a) differ, of course. Is this difference statistically significant?\n\n\n\n\nFirst we need to load the data – the girls dataset does not seem to exist in the downloable data folder, so luckily it’s easy to type in ourselves. I just typed it into a text file.\n\ngirls &lt;- readLines(here::here(\"arm\", \"data\", \"girls.txt\")) |&gt;\n    as.numeric()\n\n# Calculate the expected standard deviation -- note that we know the number\n# of births was 3900 for each month from the way the proportions are calculated.\nex_sd &lt;- sqrt(mean(girls) * (1 - mean(girls)) / 3900)\n\n# Calculate the observed SD\nobserved_sd &lt;- sd(girls)\n\nThe standard deviation we calculate from the data is 0.0064 while the expected standard deviation is 0.008.\nThese are, of course, not identical, but we can use a chi-square test to determine whether the difference is statistically significant. The test statistic \\[(n-1)\\frac{s^2}{\\sigma^2}\\] follows a chi-square distribution with \\(n-1\\) degrees of freedom where \\(s^2\\) is the observed sample variance and \\(\\sigma^2\\) is the hypothesized population variance. Notably, here, \\(n\\) refers to the number of measurements, i.e., \\(24\\), not the \\(3900\\) individuals used for calculating the proportions. We’ll use the standard \\(\\alpha = 0.05\\) and conduct a two-tailed test.\n\ntest_stat &lt;- (23) * (observed_sd^2) / (ex_sd^2)\n\nrr_upper &lt;- qchisq(0.975, 23)\nrr_lower &lt;- qchisq(0.025, 23)\n\nThe rejection region for the test is \\((0, 11.69) \\cup (38.08, \\infty)\\). The test statistic is \\(14.75\\) which lies outside of the rejection region, and so we do not have enough evidence to reject the null hypothesis at the \\(0.05\\) significance level. The difference between the observed and expected variance is not statistically significant."
  },
  {
    "objectID": "arm/cp2/index.html#q3",
    "href": "arm/cp2/index.html#q3",
    "title": "Chapter 2: Concepts and methods from basic probability and statistics",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nDemonstration of the Central Limit Theorem: let \\(x = x_1 + \\cdots + x_20,\\) the sum of 20 independent \\(\\text{Uniform}(0, 1)\\) random variables. Create 1000 simulations of \\(x\\) and plot their histogram. One the histogram, overlay a graph of the normal density function. COmment on any differences between the histogram and the curve.\n\n\n\nThe plot below shows the simulation results.\n\nset.seed(370)\nq3_sim &lt;- replicate(1e3, sum(runif(20, 0, 1)))\nhist(\n    q3_sim,\n    breaks = seq(floor(min(q3_sim)), ceiling(max(q3_sim)), 0.25),\n    freq = FALSE\n)\n\n# Calculate the normal curve and plot it\nxs &lt;- seq(floor(min(q3_sim)), ceiling(max(q3_sim)), 0.05)\nys &lt;- dnorm(xs, mean(q3_sim), sd(q3_sim))\nlines(xs, ys, lwd = 2)\n\n\n\n\n\n\n\n\nThe normal density curve is the normal distribution implied by the observed mean and variance of the simulated data. While the histogram matches the density curve fairly well, there are a few regions close to the mean which are sparser than we would expect – likely because we just haven’t done enough simulations to see the distribution in full detail."
  },
  {
    "objectID": "arm/cp2/index.html#q4",
    "href": "arm/cp2/index.html#q4",
    "title": "Chapter 2: Concepts and methods from basic probability and statistics",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nDistribution of averages and differences: the heights of men in the United States are approximately normally distributed with mean 69.1 inches and standard deviation 2.9 inches. The heights of women are approximately normally distributed with mean 63.7 inches and standard deviation 2.7 inches. Let \\(x\\) be the average height of 100 randomly sampled men, and \\(y\\) be the average of 100 randomly sampled women. Create 1000 simulations of \\(x-y\\) and plot their histogram. Using the simulations, compute the mean and standard deviation of the distribution of \\(x-y\\) and compare to their exact values.\n\n\n\nThe exact values of the mean and standard deviation of \\(x-y\\) are \\(5.4\\) and \\(0.4\\) respectively (mentioned earlier in the chapter). We can do the simulation as follows.\n\nset.seed(371)\nq4_sim &lt;- replicate(\n    1e3,\n    mean(rnorm(100, 69.1, 2.9)) - mean(rnorm(100, 63.7, 2.7))\n)\nq4_mean &lt;- mean(q4_sim)\nq4_sd &lt;- sd(q4_sim)\nhist(q4_sim, breaks = seq(4, 7, 0.1), freq = FALSE)\n\n\n\n\n\n\n\n\nThe observed mean from the simulation is \\(5.3988\\) and the observed SD is \\(0.3963\\). The simulation results are basically the same as the theoretical values."
  },
  {
    "objectID": "arm/cp2/index.html#q5",
    "href": "arm/cp2/index.html#q5",
    "title": "Chapter 2: Concepts and methods from basic probability and statistics",
    "section": "Q5",
    "text": "Q5\n\n\n\n\n\n\nCorrelated random variables: suppose that the heights of husbands and wives have a correlation of 0.3. Let \\(x\\) and \\(y\\) be the heights of a married couple chosen at random. What are the mean and standard deviation of the average height, \\((x + y)/2\\)?\n\n\n\nLet \\(\\mu_x\\) and \\(\\sigma_x\\) be the mean and standard deviation of \\(X\\) respectively, and let \\(\\mu_y\\) and \\(\\sigma_y\\) be the mean and standard deviation of \\(Y\\) respectively. Then, the mean and standard deviation of \\(x+y\\) are \\[\n\\mu_x + \\mu_y \\quad \\text{and} \\quad \\sqrt{\\sigma^2_x + \\sigma^2_y - 2\\rho\\sigma_x\\sigma_y}\n\\] respectively. Now, the mean of \\(X/2\\) is \\(\\mu_x/2\\) and similar for the mean of \\(Y/2\\). The standard deviation of \\(X/2\\) is \\(\\sigma_x / 2,\\) and similar for the standard deviation of \\(Y/2\\). So then, using the numbers from the previous question text, we get that the mean of \\((x+y)/2\\) is \\[\n\\frac{1}{2}(\\mu_x + \\mu_y) = \\frac{1}{2}(69.1 + 63.7) = 66.4,\n\\] and the standard deviation is \\[\n\\frac{1}{2}\\sqrt{(2.9)^2 + (2.7)^2 + 2(0.3)(2.9)(2.7)} = 2.26.\n\\]\nWe can also test these theoretical averages against an empirical simulation using the mvtnorm package.\n\nset.seed(372)\nmu &lt;- c(69.1, 63.7)\nsigma &lt;- matrix(c(2.9 ^ 2, 0.3 * 2.9 * 2.7, 0.3 * 2.9 * 2.7, 2.7 ^ 2), ncol = 2)\nsim_q5 &lt;- mvtnorm::rmvnorm(1000, mu, sigma)\nsim_avgs &lt;- rowMeans(sim_q5)\nmean(sim_avgs)\n\n[1] 66.37027\n\nsd(sim_avgs)\n\n[1] 2.266538"
  },
  {
    "objectID": "bsfw/cp01/index.html",
    "href": "bsfw/cp01/index.html",
    "title": "Chapter 1: Bayesian Thinking and Everyday Reasoning",
    "section": "",
    "text": "I already have a math degree where I focused on stats, so I kind of rushed through this chapter."
  },
  {
    "objectID": "bsfw/cp01/index.html#q1",
    "href": "bsfw/cp01/index.html#q1",
    "title": "Chapter 1: Bayesian Thinking and Everyday Reasoning",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nRewrite the following statements as equations using the mathematical notation you learned in the chapter:\n\nThe probability of rain is low\nThe probabiliy of rain given that it is cloudy is high\nThe probability of you having an umbrella given it is raining is much greater than the probability of you having an umbrella in general.\n\n\n\n\nNothing too out of the ordinary here!\n\n\\(\\displaystyle P\\left( \\text{rain} \\right) = \\text{low}.\\)\n\\(P \\left( \\text{rain} \\mid \\text{cloudy} \\right) = \\text{high}.\\)\n\\(P \\left( \\text{umbrella} \\mid \\text{raining} \\right) \\gg P \\left(\\text{umbrella}\\right).\\)"
  },
  {
    "objectID": "bsfw/cp01/index.html#q2",
    "href": "bsfw/cp01/index.html#q2",
    "title": "Chapter 1: Bayesian Thinking and Everyday Reasoning",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nOrganize the data you observe in the following scenario into a mathematical notation, using the techniques we’ve covered in this chapter. Then come up with a hypothesis to explain this data:\n\nYou come home from work and notice that your front door is open and the side window is broken. As you walk inside, you immediately notice that your laptop is missing.\n\n\n\n\nLet \\(D\\) represent the data we’ve observed. In order words, \\[D = \\{\\text{door open, window broken, laptop missing}\\}.\\]\nOur hypothesis might be \\[H_1: \\text{My house was robbed and they took my laptop!}\\]\nSo we might conjecture that \\[P(D \\mid H_1) = \\mathrm{high}.\\]"
  },
  {
    "objectID": "bsfw/cp01/index.html#q3",
    "href": "bsfw/cp01/index.html#q3",
    "title": "Chapter 1: Bayesian Thinking and Everyday Reasoning",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nThe following scenario adds data to the previous one. Demonstrate how this new information changes your beliefs and come up with a second hypothesis to explain the data, using the notation you’ve learned in this chapter.\n\nA neighborhood child runs up to you and apologizes profusely for accidentally throwing a rock through your window. They claim that they saw the laptop and didn’t want it stolen so they opened the front door to grab it, and your laptop is safe at their house.\n\n\n\n\nNow we’ll let \\(D_U\\) represent our updated data (augmented with the new information that we learned from this child). Then, \\[D_U = \\{D, \\text{ child's explanation}\\}\\]\nand our new hypothesis could be \\[H_2: \\text{the child has my laptop.}\\]\nIf the child appears to be telling the truth, we could then conjecture that \\[P(D_U \\mid H_2) \\gg P(D_U \\mid H_1),\\] although we shouldn’t rule out the possibility that the child is being coerced or is a part of the robbery."
  },
  {
    "objectID": "bsfw/cp03/index.html",
    "href": "bsfw/cp03/index.html",
    "title": "Chapter 3: The Logic of Uncertainty",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 16)"
  },
  {
    "objectID": "bsfw/cp03/index.html#q1",
    "href": "bsfw/cp03/index.html#q1",
    "title": "Chapter 3: The Logic of Uncertainty",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nWhat is the probability of rolling a 20 three times in a row on a 20-sided die?\n\n\n\nOf course, we could again solve by simulation 😁.\n\nset.seed(375)\nn_sims &lt;- 1000000\nrolls &lt;- tibble::tibble(\n    roll1 = sample(1:20, n_sims, replace = TRUE),\n    roll2 = sample(1:20, n_sims, replace = TRUE),\n    roll3 = sample(1:20, n_sims, replace = TRUE)\n)\n\nwith(rolls, mean(roll1 == 20 & roll2 == 20 & roll3 == 20))\n\n[1] 0.000124\n\n\nBut I don’t want anyone to take away my math degree, so we can show the analytical solution as well. (Besides, this event is quite rare, so the simulation will need a lot of samples, as you can see, to converge.)\nSince the probability of rolling a 20 on one die is \\(\\frac{1}{20}\\), and the rolls are (assumed) independent, we can calculate \\[P(\\text{three 20's}) = \\left( \\frac{1}{20} \\right)^3 = \\frac{1}{8000} = 0.000125.\\]"
  },
  {
    "objectID": "bsfw/cp03/index.html#q2",
    "href": "bsfw/cp03/index.html#q2",
    "title": "Chapter 3: The Logic of Uncertainty",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nThe weather report says there’s a 10 percent chance of rain tomorrow, and you forget your umbrella half the time you go out. What is the probability that you’ll be caught in the rain without an umbrella tomorrow?\n\n\n\nIf these two events are independent (which is kind of weird but seems to be what the question wants), we just multiply the two probabilities to get\n\\[P(\\text{no umbrella} \\cap \\text{rain}) = (10\\%) (50\\%) = 5\\%.\\]"
  },
  {
    "objectID": "bsfw/cp03/index.html#q3",
    "href": "bsfw/cp03/index.html#q3",
    "title": "Chapter 3: The Logic of Uncertainty",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nRaw eggs have a \\(1/20000\\) probability of having salmonella. If you eat two raw eggs, what is the probability you ate a raw egg with salmonella?\n\n\n\nWe have to use the inclusion-exclusion rule here to avoid double-counting, since the two eggs having salmonella are not mutually exclusive (they could both have salmonella). So we get \\[\\begin{align*}\nP(\\text{ate a salmonella egg}) &= P(\\text{egg one had salmonella}) + P(\\text{egg two had salmonella})\n\\\\ &\\quad\\quad -P(\\text{both eggs had salmonella}) \\\\\n&= \\frac{1}{20000} + \\frac{1}{20000} - \\left(\\frac{1}{20000}\\right) ^2 \\\\\n&= \\frac{39,999}{400,000,000} \\approx 0.009\\%.\n\\end{align*}\\]"
  },
  {
    "objectID": "bsfw/cp03/index.html#q4",
    "href": "bsfw/cp03/index.html#q4",
    "title": "Chapter 3: The Logic of Uncertainty",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nNote\n\n\n\nWhat is the probability of either flipping two heads in two coin tosses or rolling three 6s in three six-sided dice rolls?\n\n\nI hate how vaguely worded this question is, so I’ll list the assumptions that I think we are making.\nWe are doing one experiment that involves us flipping a coin twice (or flipping two coins) and also rolling a d6 three times (or rolling 3d6 one time). These two things are all mutually independent from each other, both coin flips are independent, and all three die rolls are independent. The die rolls and coin flips do not affect each other. Once we have done all of these things during the experiment, we want to know the probability that we flipped two heads OR (inclusize) we rolled three sixes.\nSo, we use inclusion-exclusion again. We get that\n\\[P(\\text{two heads}) = \\frac{1}{2}\\cdot \\frac{1}{2} = \\frac{1}{4},\\] \\[P(\\text{three sixes}) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216},\\] and thus \\[P(\\text{two heads} \\cup \\text{three sixes}) = \\frac{1}{4} + \\frac{1}{216} - \\frac{1}{4 \\cdot 216} = \\frac{219}{864} \\approx 25.35\\%.\\]"
  },
  {
    "objectID": "bsfw/cp05/index.html",
    "href": "bsfw/cp05/index.html",
    "title": "Chapter 5: the Beta Distribution",
    "section": "",
    "text": "library(latex2exp)\noptions(\"scipen\" = 9999, \"digits\" = 16)"
  },
  {
    "objectID": "bsfw/cp05/index.html#q1",
    "href": "bsfw/cp05/index.html#q1",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nYou want to use the beta distribution to determine whether or not a coin you have is a fair coin — meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. Using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time?\n\n\n\nThe probability we are interested in is \\[P(p \\geq 0.6) = \\int_0^{0.6} \\mathrm{Beta}(p, 4, 6) \\ \\mathrm{d}p.\\]\nWe could, of course, write out the distribution and integrate it: \\[P(p \\geq 0.6) = \\int_0^{0.6}  \\frac{1}{\\mathrm{B} \\left(4, 6\\right)}p^{4-1}(1-p)^{6-1} \\ \\mathrm{d}p.\\]\nHere, \\(\\mathrm{B}(a, b)\\) is the beta function, not the binomial distribution. In general, we could integrate \\[\\int p^{a - 1}(1-p)^{b - 1} \\ \\mathrm{d}p \\] for real, known constants \\(k, a, b\\), but the general integral in terms of \\(a\\) and \\(b\\) is non-elementary, so it is impractical to do this symbolically. The solution to this integral is called the “regularized incomplete beta function”, \\(I_x(a, b).\\) That means the solution to our original integral is \\[ P(p \\geq 0.6) = I_{0.6}(4, 6) = \\frac{\\mathrm{B}_{0.6}(4, 6)}{\\mathrm{B}(4, 6)}, \\] which we can’t actually calculate in base R (there is no built-in function for the incomplete beta function, although there could be a math identity that I don’t know that lets us calculate it with the tools at our disposal). Apparently this does not come up too often (for reasons we will see shortly), but there are a few packages that offer the incomplete beta function. I found: spsh, UCS, and the one I’ll load, zipfR. These all do other things so it seems potentially worthwhile to me to implement a package that ONLY includes the incomplete beta and gamma functions.\n\nzipfR::Ibeta(0.6, 4, 6) / beta(4, 6)\n\n[1] 0.9006474239999999\n\n\nNote that we could get the same thing by numerical integration.\n\nintegrate(\n    f = \\(p) (1 / beta(4, 6)) * p ^ 3 * (1 - p) ^ 5,\n    lower = 0,\n    upper = 0.6\n)\n\n0.9006474239999995 with absolute error &lt; 0.00000000000001\n\n\nThe two values are the same to like 15 digits of precision. So we really don’t need to go through all of that mess, we can just use numerical integration. As you may have guessed, R has multiple easier ways to do this. First, the beta density is already built into R, so we don’t have to write out the entire thing.\n\nintegrate(\n    f = \\(p) dbeta(p, 4, 6),\n    lower = 0,\n    upper = 0.6\n)\n\n0.900647424 with absolute error &lt; 0.00000000000001\n\n\nAnd of course, R has a built-in, better way to get this probability than using the standard numerical integrator. (And since the beta distribution is continuous, we don’t even have to worry about the boundaries this time.)\n\npbeta(0.6, 4, 6, lower.tail = TRUE)\n\n[1] 0.9006474239999999"
  },
  {
    "objectID": "bsfw/cp05/index.html#q2",
    "href": "bsfw/cp05/index.html#q2",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nYou flip the coin 10 more times and now have 9 heads and 11 tails total. What is the probability that the coin is fair, using our definition of fair, give or take 5%?\n\n\n\nSo, we can either numerically integrate, which allows us to specify whatever bounds we want (since this integrand is pretty well-behaved for this kind of thing).\n\nintegrate(\n    f = \\(p) dbeta(p, 9, 11),\n    lower = 0.45,\n    upper = 0.55\n)\n\n0.3098800156513043 with absolute error &lt; 0.0000000000000034\n\n\nOr we can just subtract.\n\npbeta(0.55, 9, 11) - pbeta(0.45, 9, 11)\n\n[1] 0.3098800156513036\n\n\nSometimes subtraction can be a problem with numerical computing, but if subtraction will cause a problem, so will numerical integration, so this latter method is what I would default to."
  },
  {
    "objectID": "bsfw/cp05/index.html#q3",
    "href": "bsfw/cp05/index.html#q3",
    "title": "Chapter 5: the Beta Distribution",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nData is the best way to become more confident in your assertions. You flip the coin 200 more times and end up with 109 heads and 111 tails. Now what is the probability that the coin is fair, give or take 5%?\n\n\n\nAlright, I’ve already talked enough about how to solve these problems, this is just another of the same thing. So let’s just solve it.\n\npbeta(0.55, 109, 111) - pbeta(0.45, 109, 111)\n\n[1] 0.8589371426532764\n\n\nThat’s a pretty good chance, I think."
  },
  {
    "objectID": "bsfw/cp07/index.html",
    "href": "bsfw/cp07/index.html",
    "title": "Chapter 7: Bayes’s Theorem with LEGO",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 16)\nAgain, no notes. Mostly because this chapter is super short. Just for good measure, I’ll do the mathematician thing and say. Recall Bayes’ Theorem:\n\\[P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.\\]"
  },
  {
    "objectID": "bsfw/cp07/index.html#q1",
    "href": "bsfw/cp07/index.html#q1",
    "title": "Chapter 7: Bayes’s Theorem with LEGO",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nKansas City, despite its name, sits on the border of two US states: Missouri and Kansas. The Kansas City metropolitan area consists of 15 counties, 9 in Missouri, and 6 in Kansas. The entire state of Kansas has 105 counties and Missouri has 114. Use Bayes’ theorem to calculate the probability that a relative who just moved to a country in the KC metropolitan area also lives in a county in Kansas. Make sure to show \\(P(\\text{Kansas})\\) (assuming your relative lives in either Kansas or Missouri), \\(P(\\text{Kansas City Metropolitan Area}),\\) and \\(P(\\text{Kansas City Metropolitan Area} \\mid \\text{Kansas})\\).\n\n\n\nAssuming that our relative is equally likely to have moved to any of the counties in the KC metropolitan area is an interesting choice, but is necessary for doing this problem. So we’ll assume that. Now, assuming that our relative has moved to either Kansas or Missouri, the overall probability that they are in Kansas is \\[P(\\text{Kansas}) = \\frac{105}{105 + 114} = \\frac{105}{219}.\\] The probability that our relative lives in Kansas City, assuming they live in either Kansas or Missouri (and again, that all counties are equally likely) is \\[P(\\text{KC}) = \\frac{15}{219}.\\] The probability that our relative lives in Kansas City, if we already knew that they lived in Kansas, is \\[P(\\text{KC} \\mid \\text{Kansas}) = \\frac{6/219}{105/219}=\\frac{6}{105}.\\] Then, by Bayes’ Theorem, we get that the probability that our relative lives in Kansas, given that they live in Kansas City, is \\[P(\\text{Kansas} \\mid \\text{KC}) = \\frac{P(\\text{KC} \\mid \\text{Kansas})\n\\ P(\\text{Kansas})}{P(\\text{KC})} = \\frac{\\frac{6}{105} \\frac{105}{219}}{\n\\frac{15}{219}} = \\frac{6/219}{15/219} = \\frac{6}{15} = 40\\%.\\]"
  },
  {
    "objectID": "bsfw/cp07/index.html#q2",
    "href": "bsfw/cp07/index.html#q2",
    "title": "Chapter 7: Bayes’s Theorem with LEGO",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nA deck of cards has 52 cards with suits that are either red or black. There are four aces in a deck of cards: two red and two black. You remove a red ace from the deck and shuffle the cards. Your friend pulls a black card. What is the probability that it is an ace?\n\n\n\nI like this problem more because it’s like orbs in an urn. We don’t have to make any other assumptions, this is just good normal probabilities. Anyways.\nThe probability that our friend draws an ace from the remaining 51 cards is \\[P(\\text{ace}) = \\frac{3}{51}.\\] (I am deliberately chosing not to reduce this fraction, because I don’t have to and because the solution is easier to understand if the fractions aren’t reduced.) The probability that our friend draws a black card from the remaining 51 cards (26 of which are black and 25 of which are red) is \\[P(\\text{black}) = \\frac{26}{51}.\\] Now, if our friend draws an ace, the probability that it is black (given that we know it is an ace already) is \\[P(\\text{black} \\mid \\text{ace}) = \\frac{P(\\text{black and ace})}{\nP(\\text{ace})} = \\frac{2 / 51}{3 / 51} = \\frac{2}{3}.\\] Now we can use Bayes’ theorem to get that \\[P(\\text{ace} \\mid \\text{black}) = \\frac{P(\\text{black} \\mid \\text{ace})\nP(\\text{black})}{P(\\text{ace})} = \\frac{\\frac{2}{3} \\ \\frac{3}{51}}{\n\\frac{26}{51}} = \\frac{2 / 51}{26 / 51} = \\frac{2}{26} = \\frac{1}{13}.\\] Since all the black cards are still in the deck, once we know that our friend has drawn a black card, the probability that is an ace is the same as if they had drawn from the full deck (if we hadn’t removed a card). How neat!"
  },
  {
    "objectID": "bsfw/cp09/index.html",
    "href": "bsfw/cp09/index.html",
    "title": "Chapter 9: Bayesian Priors and Working with Probability Distributions",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter introduces the concept of a prior probability distribution, rather than a single point prior (which is also, technically, a prior distribution, but it is the degenerate distribution which is very boring)."
  },
  {
    "objectID": "bsfw/cp09/index.html#q1",
    "href": "bsfw/cp09/index.html#q1",
    "title": "Chapter 9: Bayesian Priors and Working with Probability Distributions",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nA friend finds a coin on the ground, flips it, and gets six heads in a row and then one tails. Give the beta distribution that describes this. Use integration to determine the probability that the true rate of flipping heads is between 0.4 and 0.6, reflecting the belief that the coin is reasonably fair.\n\n\n\nThe beta distribution associated with the data we observed is \\[\\text{Beta}(\\alpha = 6, \\beta = 1),\\] since flipping heads is the outcome of interest (the “success”). Integrating over the specified range, we get that \\[P(0.4 \\leq p \\leq 0.6) = \\int_{0.4}^{0.6} \\text{Beta}(p \\mid 6, 1) \\ \\mathrm{d}p. \\] Approximating the integral in R, we get the following result.\n\n(q1res &lt;- integrate(f = \\(x) dbeta(x, 6, 1), lower = 0.4, upper = 0.6))\n\n0.04256 with absolute error &lt; 0.00000000000000047\n\n\nOur prior probability that the true probability of flipping heads is between 0.4 and 0.6 is approximately \\(4.26\\%\\)."
  },
  {
    "objectID": "bsfw/cp09/index.html#q2",
    "href": "bsfw/cp09/index.html#q2",
    "title": "Chapter 9: Bayesian Priors and Working with Probability Distributions",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nCome up with a prior probability that the coin is fair. Use a beta distribution such that there is at least a 95% chance that the true rate of flipping heads is between 0.4 and 0.6.\n\n\n\nWe want to find a prior probability which guarantees \\[0.95 \\leq \\int_{0.4}^{0.6} \\text{Beta}(p \\mid \\alpha, \\beta) \\ \\mathrm{d}p.\\]\nWe know that \\(\\text{Beta}(1, 1)\\) is uniform, so the probability will be too small in the interval we want. If we increase both numbers at the same time so that \\(\\alpha = \\beta\\), we know that \\(p=0.5\\) will be the most likely value, which is desirable for the prior we want to construct. So, if we instead take \\(\\alpha = 6\\) and \\(\\beta = 6\\), we get the following result.\n\n(q2res1 &lt;- integrate(f = \\(x) dbeta(x, 6, 6), lower = 0.4, upper = 0.6))\n\n0.507 with absolute error &lt; 0.0000000000000056\n\n\nWe see that this prior still isn’t strong enough to give us the coverage we want in the given range. So let’s try doubling both of the parameters.\n\n(q2res2 &lt;- integrate(f = \\(x) dbeta(x, 12, 12), lower = 0.4, upper = 0.6))\n\n0.6727 with absolute error &lt; 0.0000000000000075\n\n\nEven this still doesn’t work – we need an extremely strong beta prior to guarantee the probability we are interested in. So now I’ll experiment with numbers, testing a grid until we get what we want.\n\nparms &lt;- 1:100\nq2res3 &lt;- sapply(\n    parms,\n    \\(ab) integrate(f = \\(x) dbeta(x, ab, ab), lower = 0.4, upper = 0.6)$value\n)\n\n# Get the minimum number where the probability is above the target.\nprior_parm &lt;- parms[[which(q2res3 &gt;= 0.95)[[1]]]]\n\nWe can obtain the correct probability bound by using a beta distribution where both parameters are equal to \\(48\\)."
  },
  {
    "objectID": "bsfw/cp09/index.html#q3",
    "href": "bsfw/cp09/index.html#q3",
    "title": "Chapter 9: Bayesian Priors and Working with Probability Distributions",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nNow see how many more heads (with no more tails) it would take to convince you that there is a reasonable chance that this coin is not fair. In this case, let’s say that this means that our belief in the rate of the coin being between 0.4 and 0.6 drops below 0.5.\n\n\n\nWe know that the posterior distribution after we observe \\(k\\) more heads is equal to \\[\\text{Beta}(\\alpha = 48 + k, \\beta = 48),\\] so now what we want to find is \\[\n\\operatorname*{arg\\,min}_k \\int_{0.4}^{0.6} \\text{Beta}(p \\mid 48\n+ k, 48) \\ \\mathrm{d}p.\n\\] We’ll estimate \\(k\\) using a grid search.\n\nk_vals &lt;- 1L:100L\nintegral_values &lt;- sapply(\n    k_vals,\n    \\(k) integrate(\n        f = \\(x) dbeta(x, prior_parm + k, prior_parm),\n        lower = 0.4,\n        upper = 0.6\n    )$value\n)\n(min_k &lt;- k_vals[which(integral_values &lt; 0.5)[[1]]])\n\n[1] 24\n\n\nSo it would take 24 more heads in a row without any tail flips before our credibility that the coin is unfair is \\(50\\%\\) or higher.\nWe can make a plot of the results as well to see this.\n\nplot(\n    NULL, NULL,\n    xlim = c(0, 50), ylim = c(0, 1),\n    xlab = latex2exp::TeX(r'($k$)'),\n    ylab = latex2exp::TeX(r'($P(0.4 \\leq p \\leq 0.6)$)'),\n    axes = FALSE\n)\naxis(1, at = c(seq(0, 50, 10), min_k))\naxis(2, at = c(seq(0, 1, 0.2), 0.5))\nabline(h = integral_values[min_k], lty = 2)\nabline(v = min_k, lty = 2)\nlines(1:50, integral_values[1:50], type = \"o\", lwd = 1.5)"
  },
  {
    "objectID": "bsfw/cp11/index.html",
    "href": "bsfw/cp11/index.html",
    "title": "Chapter 11: Measuring the Spread of Our Data",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)\nThis chapter defines the mean absolute deviation (MAD), variance, and standard deviation of a set of numbers."
  },
  {
    "objectID": "bsfw/cp11/index.html#q1",
    "href": "bsfw/cp11/index.html#q1",
    "title": "Chapter 11: Measuring the Spread of Our Data",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nOne of the benefits of variance is that squaring the differences makes the penalties exponential. Give some examples of when this would be a useful property.\n\n\n\nThe exponential penalty of the variance can be useful when we expect all of our data points to be somewhat close together, so that outlying data points are penalized more severely the further away they are. If error is expensive, we also want to penalize errors increasingly as they become larger."
  },
  {
    "objectID": "bsfw/cp11/index.html#q2",
    "href": "bsfw/cp11/index.html#q2",
    "title": "Chapter 11: Measuring the Spread of Our Data",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nCalculate the mean, variance, and standard deviation for the following values: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.\n\n\n\n\nvec &lt;- 1:10\ncat(\"Mean:    \", round(mean(vec), 3), \"\\n\")\n\nMean:     5.5 \n\ncat(\"MAD:     \", round(mad(vec, center = mean(vec)), 3), \"\\n\")\n\nMAD:      3.706 \n\ncat(\"Variance:\", round(var(vec), 3), \"\\n\")\n\nVariance: 9.167 \n\ncat(\"SD:      \", round(sd(vec), 3), \"\\n\")\n\nSD:       3.028"
  },
  {
    "objectID": "bsfw/cp13/index.html",
    "href": "bsfw/cp13/index.html",
    "title": "Chapter 13: The Tools of Parameter Estimation",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp13/index.html#q1",
    "href": "bsfw/cp13/index.html#q1",
    "title": "Chapter 13: The Tools of Parameter Estimation",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nUsing the code example for plotting the PDF on page 127, plot the CDF and quantile functions.\n\n\n\n\nxs &lt;- seq(0.005, 0.01, by = 1e-5)\nplot(\n    xs,\n    pbeta(xs, 300, 4e4 - 300),\n    type = \"l\", lwd = 3,\n    xlab = \"Probability of subscription\",\n    ylab = \"Cumulative probability\",\n    main = \"CDF Beta(300, 39700)\"\n)\n\n\n\n\n\n\n\n\n\nxs &lt;- seq(1e-5, 1 - 1e-5, 1e-5)\nplot(\n    xs,\n    qbeta(xs, 300, 4e4 - 300),\n    type = \"l\", lwd = 3,\n    ylab = \"Probability of subscription\",\n    xlab = \"Cumulative probability\",\n    main = \"Quantile function Beta(300, 39700)\"\n)"
  },
  {
    "objectID": "bsfw/cp13/index.html#q2",
    "href": "bsfw/cp13/index.html#q2",
    "title": "Chapter 13: The Tools of Parameter Estimation",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nReturning to the task of measuring snowfall from Chapter 10, say you have the following measurements (in inches) of snowfall:\n\n7.8, 9.4, 10.0, 7.9, 9.4, 7.0, 7.0, 7.1, 8.9, 7.4.\n\nWhat is your 99.9% confidence interval for the true value of snowfall?\n\n\n\nAssuming the snowfall measurements are normally distributed, first we need to calculate the mean and standard deviation of the measurements.\n\nsnow &lt;- c(7.8, 9.4, 10.0, 7.9, 9.4, 7.0, 7.0, 7.1, 8.9, 7.4)\n(snow_mean &lt;- mean(snow))\n\n[1] 8.19\n\n(snow_sd &lt;- sd(snow))\n\n[1] 1.135\n\n\nNow to obtain a \\(99.9\\%\\) credible interval, we need to calculate quantiles where the probabilities are \\(0.0005\\) and \\(0.9995\\).\n\n(snow_ci &lt;- qnorm(c(0.0005, 0.9995), snow_mean, snow_sd))\n\n[1]  4.456 11.924\n\n\nSo our estimate for the actual snowfall is 8.19 inches with a \\(99.9\\%\\) credible interval of \\((4.46, 11.92)\\)."
  },
  {
    "objectID": "bsfw/cp13/index.html#q3",
    "href": "bsfw/cp13/index.html#q3",
    "title": "Chapter 13: The Tools of Parameter Estimation",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nA child is going door to door selling candy bars. So far she has visited 30 houses and sold 10 candy bars. She will visit 40 more houses today. What is the 95% confidence interval for how many candy bars she will sell the rest of the day?\n\n\n\nWe’ll model the probability, say \\(\\pi\\) that a house will buy chocolate bars using a beta distribution. Based on the information we have so far, we should assume that \\[\\pi \\sim \\mathcal{B}(10, 20).\\]\nWe can then calculate the mean and the 95% confidence interval using the distribution.\n\n(mean_pi &lt;- 10 / 30)\n\n[1] 0.3333\n\n(ci_pi &lt;- qbeta(c(0.025, 0.975), 10, 20))\n\n[1] 0.1794 0.5083\n\n\nSo, if she visits 40 more houses today, we can calculate the number of houses we expect to buy chocolate for each of those probability estimates.\n\n(mean_choc &lt;- mean_pi * 40)\n\n[1] 13.33\n\n(ci_choc &lt;- ci_pi * 40)\n\n[1]  7.175 20.333\n\n\nSo we would expect \\(13\\) houses to buy chocolate if the trend stays the same, with a \\(95\\%\\) credible interval of \\((7,\n21)\\). (Note that the interval is not exact due to rounding, but it is the closest we can get with integer boundaries.)\nWe can also make a plot of the expected number of chocolate bars.\n\nxs &lt;- seq(1e-5, 1 - 1e-5, 1e-5)\nplot(\n    NULL, NULL,\n    ylab = \"Probability of selling at least this many chocolate bars\",\n    xlab = \"Number of chocolate bars\",\n    main = \"CCDF Beta(10, 20) * 40\",\n    xlim = c(0, 40),\n    ylim = c(0, 1),\n    axes = FALSE\n)\n\nabline(v = floor(mean_choc), lty = 2, lwd = 2)\nabline(v = floor(ci_choc[[1]]), lty = 3, lwd = 1.5)\nabline(v = ceiling(ci_choc[[2]]), lty = 3, lwd = 1.5)\nlines(\n    xs * 40,\n    1 - pbeta(xs, 10, 20),\n    type = \"l\", lwd = 3\n)\naxis(\n    1,\n    at = c(seq(0, 40, 10), floor(mean_choc), floor(ci_choc[[1]]),\n                 ceiling(ci_choc[[2]]))\n)\naxis(2, at = seq(0, 1, 0.25))"
  },
  {
    "objectID": "bsfw/cp15/index.html",
    "href": "bsfw/cp15/index.html",
    "title": "Chapter 15: From Parameter Estimation to Hypothesis Testing",
    "section": "",
    "text": "options(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp15/index.html#q1",
    "href": "bsfw/cp15/index.html#q1",
    "title": "Chapter 15: From Parameter Estimation to Hypothesis Testing",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nSuppose a director of marketing with many years of experience tells you he believes very strongly that the variant without images (B) won’t perform any differently than the original variant. How could you account for this in our model? Implement this change and see how your final conclusions change as well.\n\n\n\nWe can account for the director of marketing’s experience by adjusting our priors. To make our prior belief that the conversion rates are the same more stringent, we could use, say, a \\(\\text{Beta}(300, 700)\\) prior.\nThat means the posterior distribution for variant A is \\(\\text{Beta}(336, 814)\\) and the posterior distribution for variant B is \\(\\text{Beta}(350, 800)\\). We can repeat the Monte Carlo simulation for these priors and draw \\(100,000\\) samples for each variant.\n\n\nCode\nset.seed(370)\nn_sims &lt;- 1e5\na_sims &lt;- rbeta(n_sims, 336, 814)\nb_sims &lt;- rbeta(n_sims, 350, 800)\n\na_density &lt;- density(a_sims, kernel = \"epanechnikov\")\nb_density &lt;- density(b_sims, kernel = \"epanechnikov\")\n\nplot(\n    NULL, NULL,\n    xlim = c(0.1, 0.5),\n    ylim = c(0, 30),\n    xlab = \"Conversion rate\",\n    ylab = \"Density\",\n    main = \"Simulated conversion rate distributions for variants A and B\",\n    axes = FALSE\n)\nlines(a_density, lwd = 3, col = \"dodgerblue2\")\nlines(b_density, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0.1, 0.5, 0.1))\naxis(2, seq(0, 30, 10))\nlegend(\n    0.1, 30, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n    lty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n\n\n\n\n\n\n\n\n\nWe can see that the posterior distributions for variants A and B are much closer together. In total, \\(73.81\\%\\) of the simulated conversation rates for variant B were greater than the simulated conversation rate for variant A. The empirical CDF for relative improvements is shown below.\n\n\nCode\nimp &lt;- b_sims / a_sims\nimp_ecdf &lt;- ecdf(imp)\nxs &lt;- seq(min(imp), max(imp), length.out = 1e3)\nys &lt;- imp_ecdf(xs)\nplot(\n    NULL, NULL,\n    main = \"Emperical cdf of simulated relative improvements of B over A\",\n    xlab = \"Relative improvement of B over A\",\n    ylab = \"Empirical cumulative probability\",\n    axes = FALSE,\n    xlim = c(0.68, 1.42),\n    ylim = c(-0.02, 1.02),\n    xaxs = \"i\", yaxs = \"i\"\n)\nabline(h = 0.25, lty = 2, col = \"gray\")\nabline(h = 0.50, lty = 2, col = \"gray\")\nabline(h = 0.75, lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.25))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.50))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.75))], lty = 2, col = \"gray\")\nlines(\n    xs, ys,\n    lwd = 3, col = \"firebrick4\", type = \"s\",\n)\naxis(1, seq(0.7, 1.4, 0.1))\naxis(2, seq(0, 1, 0.25))\n\n\n\n\n\n\n\n\n\nUnder the new more stringent prior, we can see that nearly a quarter of all samples showed a higher conversion rate for variant A than variant B, and very few samples showed a relative improvement of variant B more than 1.2. So while our simulation suggests that B is most likely better than A, we are less confident in that assertion, and the magnitude of improvement is estimated to be smaller on average."
  },
  {
    "objectID": "bsfw/cp15/index.html#q2",
    "href": "bsfw/cp15/index.html#q2",
    "title": "Chapter 15: From Parameter Estimation to Hypothesis Testing",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nThe lead designer sees your results and insists there’s no way that variant B should perform better with no images. She feels that you should assume the conversion rate for variant B is closer to \\(20\\%\\) than \\(30\\%\\). Implement a solution for this and again review the results of our analysis.\n\n\n\nTo accomodate the lead designer’s prior beliefs, we’ll return to our original prior of \\(\\text{Beta}(3, 7)\\). However, we’ll only use this prior for variant A. For variant B, we want a prior that has a mean conversion rate closer to \\(20\\%\\), so we’ll use \\(\\text{Beta}(2, 8)\\). Let’s take a look at the priors.\n\n\nCode\nxs &lt;- seq(1e-4, 1 - 1e-4, 1e-4)\na_prior_q2 &lt;- dbeta(xs, 3, 7)\nb_prior_q2 &lt;- dbeta(xs, 2, 8)\n\nplot(\n    NULL, NULL,\n    xlim = c(0, 1),\n    ylim = c(0, 3.6),\n    xlab = \"Conversion rate\",\n    ylab = \"Density\",\n    main = \"Prior conversion rate distributions for variants A and B\",\n    axes = FALSE\n)\nlines(xs, a_prior_q2, lwd = 3, col = \"dodgerblue2\")\nlines(xs, b_prior_q2, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 3.6, 0.5))\nlegend(\n    0.6, 3.2, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n    lty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n\n\n\n\n\n\n\n\n\nWe can see that our priors now assume the average conversion rate for variant B is around \\(20\\%\\) as desired, although both priors are not too strict. So now our posterior distributions given the observed data are \\(\\text{Beta}(39, 121)\\) for variant A and \\(\\text{Beta}(52, 108)\\) for variant B. Let’s take a look at the posterior distributions.\n\n\nCode\nxs &lt;- seq(1e-4, 1 - 1e-4, 1e-4)\na_post_q2 &lt;- dbeta(xs, 39, 121)\nb_post_q2 &lt;- dbeta(xs, 52, 108)\n\nplot(\n    NULL, NULL,\n    xlim = c(0, 1),\n    ylim = c(0, 12),\n    xlab = \"Conversion rate\",\n    ylab = \"Density\",\n    main = \"Posterior conversion rate distributions for variants A and B\",\n    axes = FALSE\n)\nlines(xs, a_post_q2, lwd = 3, col = \"dodgerblue2\")\nlines(xs, b_post_q2, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 12, 2))\nlegend(\n    0.6, 10, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n    lty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n\n\n\n\n\n\n\n\n\nOur posterior distributions again seem to show that the conversion rate for B is, on average, higher than the conversion rate for A. However, the spread of the two posterior distributions is now different and we are less certain in the magnitude of the conversion rate for B than we are for B. Let’s repeat the Monte Carlo simulation with these new priors and look at the ECDF of improvements.\n\n\nCode\na_sims_q2 &lt;- rbeta(n_sims, 39, 121)\nb_sims_q2 &lt;- rbeta(n_sims, 52, 108)\nimp_q2 &lt;- b_sims_q2 / a_sims_q2\nimp_ecdf_q2 &lt;- ecdf(imp_q2)\nxs &lt;- seq(min(imp_q2), max(imp_q2), length.out = 1e3)\nys &lt;- imp_ecdf(xs)\nplot(\n    NULL, NULL,\n    main = \"Emperical cdf of simulated relative improvements of B over A\",\n    xlab = \"Relative improvement of B over A\",\n    ylab = \"Empirical cumulative probability\",\n    axes = FALSE,\n    xlim = c(0.48, 3),\n    ylim = c(-0.02, 1.02),\n    xaxs = \"i\", yaxs = \"i\"\n)\nabline(h = 0.25, lty = 2, col = \"gray\")\nabline(h = 0.50, lty = 2, col = \"gray\")\nabline(h = 0.75, lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.25))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.50))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.75))], lty = 2, col = \"gray\")\nlines(\n    xs, ys,\n    lwd = 3, col = \"firebrick4\", type = \"s\",\n)\naxis(1, seq(0.5, 3, 0.5))\naxis(2, seq(0, 1, 0.25))\n\n\n\n\n\n\n\n\n\nUnder these priors, \\(94.78\\%\\) of sampled conversation rates for variant B were higher than the sampled conversion rates for variant A. So despite the lowered probability for variant B, we are still very confident that B outperforms A, although we are slightly less confident than the example in the book using the same priors for B and A."
  },
  {
    "objectID": "bsfw/cp15/index.html#q3",
    "href": "bsfw/cp15/index.html#q3",
    "title": "Chapter 15: From Parameter Estimation to Hypothesis Testing",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nAssume that being 95% certain means that you’re more or less “convinced” of a hypothesis. Also assume that there’s no longer any limit to the number of emails you can send in your test. If the true conversion rate for A is 0.25 and the true conversion rate for B is 0.3, explore how many samples it would take to convince the director of marketing that B was in fact superior. Explore the same for the lead designer.\n\n\n\nI think this question, like many others in this book, is quite poorly worded. But I’ll assume that what we actually want to know is how many samples we would need to convince either person that the \\(P(B &gt; A) = 0.95\\).\nSo, for a number of different sample sizes, we’ll simulate conversions. Then, from those simulated conversion trials (pretend they are real data), we’ll run our original Monte Carlo simulation using the updated prior based on the number of conversion events we observed for variants A and B. We’ll repeat this entire scheme 100 times for both the director and the lead designer in order to make sure our results aren’t due to variance. Then we can plot the results.\n\n\nCode\n# This function runs a single round of our simulation procedure. That is,\n# we generate n_samples number of trials, where a trial is defined as one\n# variant A email being sent and one variant B email being sent. We simulate\n# from the true conversion rate whether those are conversions, and then create\n# the posterior and repeat our MC simulation as before.\nsimulate_probability &lt;- function (\n        n_samples, true_a, true_b,\n        prior_alpha_a, prior_alpha_b,\n        prior_beta_a, prior_beta_b\n    ) {\n        # Generate the number of conversions in this simulation\n        events_a &lt;- runif(n_samples) &lt;= true_a\n        events_b &lt;- runif(n_samples) &lt;= true_b\n        \n        # Calculate the parameters for the posterior distributions\n            post_alpha_a &lt;- prior_alpha_a + sum(events_a)\n        post_beta_a &lt;- prior_beta_a + sum(!events_a)\n        post_alpha_b &lt;- prior_alpha_b + sum(events_b)\n        post_beta_b &lt;- prior_beta_b + sum(!events_b)\n        \n        # Now run the Monte Carlo simulation from the implied posterior\n        samples_a &lt;- rbeta(1e5, post_alpha_a, post_beta_a)\n        samples_b &lt;- rbeta(1e5, post_alpha_b, post_beta_b)\n        \n        # Get the probability b &gt; a\n        prob_bga &lt;- mean(samples_b &gt; samples_a)\n        return(prob_bga)\n    }\n\n# This function runs the simulate_probability() function multiple times, each\n# time using a different number of samples defined by the samples_seq argument.\n# The result is given as a matrix where each column is the output of one\n# run of simulate_probability().\nrun_sample_seq &lt;- function(\n        samples_seq = seq(25, 1000, 25),\n        ...\n    ) {\n    sapply(\n        samples_seq,\n        \\(n) simulate_probability(n, ...)\n    )\n}\n\n# This function will take in our results and return a data frame.\nsummarize_replicate_results &lt;- function(res, alpha = 0.1) {\n    res_mean &lt;- rowMeans(res)\n    res_PI &lt;- apply(res, 1, quantile, probs = c(alpha / 2, 1 - alpha / 2))\n    out &lt;- data.frame(\n        est = res_mean,\n        lwr = res_PI[1, ],\n        upr = res_PI[2, ]\n    )\n    return(out)\n}\n\nsamples_vector &lt;- seq(25, 1000, 25)\n\n# Run the simulation for the director\nsims_director &lt;- replicate(\n    100,\n    run_sample_seq(\n        samples_seq = samples_vector,\n        true_a = 0.25,\n        true_b = 0.35,\n        prior_alpha_a = 300,\n        prior_alpha_b = 300,\n        prior_beta_a = 700,\n        prior_beta_b = 700\n    )\n)\nres_director &lt;- cbind(\n    n = samples_vector,\n    person = 1,\n    summarize_replicate_results(sims_director)\n)\n\n# And for the designer\nsims_designer &lt;- replicate(\n    100,\n    run_sample_seq(\n        samples_seq = samples_vector,\n        true_a = 0.25,\n        true_b = 0.35,\n        prior_alpha_a = 3,\n        prior_alpha_b = 7,\n        prior_beta_a = 2,\n        prior_beta_b = 8\n    )\n)\nres_designer &lt;- cbind(\n    n = samples_vector,\n    person = 2,\n    summarize_replicate_results(sims_designer)\n)\n\n# Data cleaning\nres_combined &lt;- rbind(res_director, res_designer)\nres_combined$person &lt;- factor(\n    res_combined$person,\n    levels = c(1, 2),\n    labels = c(\"Director of Marketing\", \"Lead Designer\")\n)\n\n# Get the intercepts for the number of samples closest to 95% probability\ndirector_n &lt;- res_director$n[which.min(abs(res_director$est - 0.95))]\ndesigner_n &lt;- res_director$n[which.min(abs(res_designer$est - 0.95))]\n\n# Plot the results\nlibrary(ggplot2)\nres_combined |&gt;\n    ggplot() +\n    aes(\n        x = n, y = est, ymin = lwr, ymax = upr, group = person,\n        color = person, fill = person, shape = person\n    ) +\n    geom_ribbon(alpha = 0.3, color = \"transparent\") +\n    geom_hline(yintercept = 0.95, linetype = 2, linewidth = 1) +\n    geom_vline(\n        xintercept = c(director_n, designer_n),\n        linetype = 2, linewidth = 1\n    ) +\n    geom_line(linewidth = 1.5, alpha = 0.8) +\n    geom_point(size = 3, stroke = 2, fill = \"white\", alpha = 0.8) +\n    scale_color_manual(values = c(\"dodgerblue2\", \"firebrick3\")) +\n    scale_fill_manual(values = c(\"dodgerblue2\", \"firebrick3\")) +\n    scale_shape_manual(values = c(21, 22)) +\n    hgp::theme_ms() +\n    labs(\n        x = \"Number of samples\",\n        y = \"Probability of B &gt; A\",\n        color = NULL,\n        fill = NULL,\n        shape = NULL\n    )\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see that we would need about 250 samples to convince the lead designer, and around 475 samples to convince the director of marketing. However, if we wanted to make sure we take the sampling variability into account, (instead assuming that our lower bound for the probability over all of our sampling replications is 95% or higher), we would actually need around 475 samples to convince the lead designer and 750 samples to convince the director of marketing."
  },
  {
    "objectID": "bsfw/cp17/index.html",
    "href": "bsfw/cp17/index.html",
    "title": "Chapter 17: Bayesian Reasoning in the Twilight Zone",
    "section": "",
    "text": "Code\noptions(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp17/index.html#q1",
    "href": "bsfw/cp17/index.html#q1",
    "title": "Chapter 17: Bayesian Reasoning in the Twilight Zone",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nEvery time you and your friend get together to watch movies, you flip a coin to determine who gets to choose the movie. Your friend always picks heads, and every Friday for 10 weeks, the coin lands on heads. You develop a hypothesis that the coin has two heads sides, rather than both a heads side and a tails side. Set up a Bayes factor for the hypothesis that the coin is a trick coin over the hypothesis that the coin is fair. What does this ratio alone suggest about whether or not your friend is cheating you?\n\n\n\nLet \\(H_1\\) be the hypothesis that our friend’s coin has heads on both sides and let \\(H_2\\) be the hypothesis that our friend’s coin is fair. If \\(H_1\\) were true, then the probability of every flip being heads would be \\(1\\) no matter how many flips we do. However, under \\(H_2\\), the probability of every coin being heads would be \\[P(D \\mid H_2) = 0.5^{10} = \\frac{1}{1024}.\\]\nSo our bayes factor is then \\[B = \\frac{1}{1 / 1024} = 1024.\\]\nIf we believed that both scenarios were equally likely, we would have incredibly strong evidence that our friend were cheating."
  },
  {
    "objectID": "bsfw/cp17/index.html#q2",
    "href": "bsfw/cp17/index.html#q2",
    "title": "Chapter 17: Bayesian Reasoning in the Twilight Zone",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nNow imagine three cases: that your friend is a bit of a prankster, that your friend is honest most of the time but can occasionally be sneaky, and that your friend is very trustworthy. In each case, estimate some prior odds ratios for your hypothesis and compute the posterior odds.\n\n\n\nFor the first case, that our friend is a bit of a prankster, we’ll say our posterior odds of being tricked are somewhat high, maybe \\(0.8\\) – that is, out of every 5 times we interact with our friend, we expect one prank. The posterior odds would then be \\[0.8 \\times 1024 = 819.2,\\] and we would still be quite sure that our friend was tricking us.\nIf our friend is honest most of the time, say we believe that our prior odds of being tricked are \\(1/100\\). Then our posterior odds would be \\[0.01 \\times 1024 = 10.24,\\] so we should at least be amenable to the idea that our friend is tricking us, although the evidence is not quite conclusive.\nFinally, if our friend is really trustworthy, let’s say our prior odds are \\(1/10000\\). Then we would get a posterior odds of \\[0.0001 \\times 1024 = 0.1024,\\] and we would actually be somewhat confident that our friend is not cheating (note that this corresponds to a Bayes factor of about \\(10\\) for the relative odds of a fair coin)."
  },
  {
    "objectID": "bsfw/cp17/index.html#q3",
    "href": "bsfw/cp17/index.html#q3",
    "title": "Chapter 17: Bayesian Reasoning in the Twilight Zone",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nSuppose you trust this friend deeply. Make the prior odds of them cheating \\(1/10000\\). How many times would the coin have to land on heads before you feel unsure about their innocence–say a posterior odds of 1?\n\n\n\nWe need to solve the equation \\[1 \\leq \\frac{1}{10000}\\frac{1}{0.5^x}\\] for \\(x\\), which yields \\[x \\approx 14.\\]\nSo we would need to see 14 heads in a row before we would entertain the notion that our friend is cheating."
  },
  {
    "objectID": "bsfw/cp17/index.html#q4",
    "href": "bsfw/cp17/index.html#q4",
    "title": "Chapter 17: Bayesian Reasoning in the Twilight Zone",
    "section": "Q4",
    "text": "Q4\n\n\n\n\n\n\nAnother friend of yours also hangs out with this same friend and, after only four weeks of the coin landing on heads, feels certain you’re both being cheated. This confidence implies a posterior odds of about 100. What value would you assign to this other friend’s prior belief that the first friend is a cheater?\n\n\n\nWe can solve for the prior odds in the equation \\[100 = O(H_1)\\times \\frac{1}{0.5^4},\\] which gives \\[O(H_1) = 6.25.\\]"
  },
  {
    "objectID": "bsfw/cp19/index.html",
    "href": "bsfw/cp19/index.html",
    "title": "Chapter 19: From Hypothesis Testing to Parameter Estimation",
    "section": "",
    "text": "Code\noptions(\"scipen\" = 9999, \"digits\" = 4)"
  },
  {
    "objectID": "bsfw/cp19/index.html#q1",
    "href": "bsfw/cp19/index.html#q1",
    "title": "Chapter 19: From Hypothesis Testing to Parameter Estimation",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nOur Bayes factor assumed that we were looking at \\(H_1: P(\\text{prize} = 0.5)\\). THis allowed us to derive a version of the beta distribution with an alpha of 1 and a beta of 1. Would it matter if we chose a different probability for \\(H_1\\)? Assume \\(H_1: P(\\text{prize} = 0.24)\\), then see if the resulting distribution, once normalized to sum to 1, is any different from the original hypothesis.\n\n\n\nOK, so what we need to do is calculate the new distribution and the old distribution so we can compare them.\n\n\nCode\nhypotheses &lt;- seq(0, 1, by = 0.01)\nnumerator &lt;- hypotheses ^ 24 * (1 - hypotheses) ^ 76\nbf_0.50 &lt;- numerator / (0.50 ^ 24 * (1 - 0.50) ^ 76)\nbf_0.24 &lt;- numerator / (0.24 ^ 24 * (1 - 0.24) ^ 76)\n\n# Now normalize the distributions\nbf_0.50_norm &lt;- bf_0.50 / sum(bf_0.50)\nbf_0.24_norm &lt;- bf_0.24 / sum(bf_0.24)\n\nplot(\n    bf_0.24_norm, bf_0.50_norm,\n    xlab = \"Normalized Bayes factors when assumed probability = 0.24\",\n    ylab = \"Normalized Bayes factors when assumed probability = 0.50\",\n    pch = 3, cex = 1.1\n)\nabline(a = 0, b = 1, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\n\nYou can see that the points are exactly correlated, i.e., the normalized distributions are exactly the same."
  },
  {
    "objectID": "bsfw/cp19/index.html#q2",
    "href": "bsfw/cp19/index.html#q2",
    "title": "Chapter 19: From Hypothesis Testing to Parameter Estimation",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nWrite a prior for the distribution in which each hypothesis is 1.05 times more likely than the previous hypothesis (assume our dx remains the same).\n\n\n\nFirst we’ll create the prior.\n\n\nCode\nn &lt;- length(hypotheses)\nprior &lt;- 1.05 ^ (0:(n-1))\n\n\nTo obtain the posterior distribution, we multiply the priors by their corresponding Bayes factors and normalize.\n\n\nCode\npost_raw &lt;- prior * bf_0.50_norm\npost &lt;- post_raw / sum(post_raw)\nplot(\n    hypotheses, post, lwd = 3, type = \"l\",\n    xlab = \"Hypothesized prize probability\",\n    ylab = \"Normalized posterior density of result\"\n)\n\n\n\n\n\n\n\n\n\nWe can see that the best estimate is still around \\(0.24\\). Even though the extreme values have higher prior likelihood, the evidence is strong enough to ignore them."
  },
  {
    "objectID": "bsfw/cp19/index.html#q3",
    "href": "bsfw/cp19/index.html#q3",
    "title": "Chapter 19: From Hypothesis Testing to Parameter Estimation",
    "section": "Q3",
    "text": "Q3\n\n\n\n\n\n\nSuppose you observed another duck game that included 34 ducks with prizes and 66 ducks without prizes. How would you set up a test to answer “What is the probability that you have a better chance of winning a prize in this game than in the game we used in our example?”\n\n\n\nIf we assume a \\(\\text{Beta}(1, 1)\\) prior for both probabilities, then the posterior distribution for the probability of a prize in the first game is \\(\\text{Beta}(25, 77)\\) and the posterior distribution for the probability of a prize in the second game is \\(\\text{Beta}(35, 67)\\). We can conduct a Monte Carlo simulation with, say, \\(100,000\\) draws from both distributions as we did previously.\n\n\nCode\nset.seed(132)\nn_sims &lt;- 1e5\ngame_1_sim &lt;- rbeta(n_sims, 25, 77)\ngame_2_sim &lt;- rbeta(n_sims, 35, 67)\nprob &lt;- mean(game_2_sim &gt; game_1_sim)\n\n\nFrom this simple simulation, the probability that we have a better chance of winning a prize was about \\(93.9\\%\\).\nApparently the solutions in the book wanted us to use arbitrary priors and use a numerical algorithm to generate samples, but for a problem this simple I doubt we will get different results unless we change the priors a lot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bookwork Blog",
    "section": "",
    "text": "This is my blog for storing notes and work from books that I read, so I could consolidate them all into one web page.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStatistical Rethinking\n\n\n\nStatistics\n\n\nBayesian\n\n\nCausal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis Using Regression and Multilevel/Hierarchical Modeling\n\n\n\nStatistics\n\n\nMultilevel modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics the Fun Way\n\n\n\nStatistics\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sr/cp1.html",
    "href": "sr/cp1.html",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "This chapter covers McElreath’s perspectives on statistical modeling and causal inference, and gives a description of how the book will address issues with current statistical practice and the tools that will be used."
  },
  {
    "objectID": "sr/cp1.html#chapter-notes",
    "href": "sr/cp1.html#chapter-notes",
    "title": "Chapter 1: The Golem of Prague",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nThe statistical model as a golem: models cannot think or see context. They are designed to do specific jobs and carry out their job exactly as they are instructed to, without regard for the consequences. Golems can make numbers, but we have to make golems and interpret the results.\nStatistics often lacks a coherent epistemiology. We need to understand how statistical models related to causal models, and how causal models relate to scientific hypotheses.\n“Folk Popperism:” many scientists believe that null hypothesis significance testing reflects Popper’s belief that scientific hypotheses must be falsifiable. But really, one statistical model is related to multiple process models, and one process model is related to many scientific hypotheses.\nAdditionally, the NHST paradigm often ignores the fallibility of measurements and the idea that falsification can be spurious. And continuous hypotheses cannot simply be falsified.\nMy favorite quote from this chapter (and it is full of great quotes): “So, if attempting to mimic falsification is not a genreally useful approach to statistical methods, what are we to do? We are to model.”\nThe rest of the chapter details the approaches that the book will take with respect to causal modeling and bayesian analysis methods.\nAn equally good title for this chapter might be “the statistical nihilist’s manifesto.”"
  },
  {
    "objectID": "sr/cp1.html#exercises",
    "href": "sr/cp1.html#exercises",
    "title": "Chapter 1: The Golem of Prague",
    "section": "Exercises",
    "text": "Exercises\n(This space intentionally left blank.)"
  },
  {
    "objectID": "sr/cp3.html",
    "href": "sr/cp3.html",
    "title": "Chapter 3: Sampling the Imaginary",
    "section": "",
    "text": "This chapter discusses the basics of sampling – instead of directly approximate the density of the posterior distribution, we can draw samples from it. This seems silly for simple distributions, but scales to otherwise intractable problems. Once we have the samples, we can use those to estimate the posterior density."
  },
  {
    "objectID": "sr/cp3.html#chapter-notes",
    "href": "sr/cp3.html#chapter-notes",
    "title": "Chapter 3: Sampling the Imaginary",
    "section": "Chapter notes",
    "text": "Chapter notes\n\n“Fetishizing precision to the fifth decimal place will not improve your science.”\nThe HDPI and PI methods for constructing credible intervals are similar for bell-shaped curves, but will be different for highly skewed curves where the mode and mean are different. “If the choice of interval affects the inference, you are better off plotting the entire posterior distribution.”\nThe HDPI also has higher simulation variance, that is, it needs more samples than the PI to arrive at a stable result.\nChoosing a point estimate, such as the mean, median, or mode (maximum a posteriori value) can be difficult.\nImportantly, different loss functions imply different estimates. The absolute loss function, \\(L(\\theta, \\hat{\\theta}) = \\left| \\theta - \\hat{\\theta} \\right|\\) is minimized by the median; the quadratic loss function, \\(L(\\theta, \\hat{\\theta}) = \\left( \\theta - \\hat{\\theta} \\right)^2\\), is minimized by the mean; and the 0-1 loss function (different for discrete and continuous problems) is minimized by the mode and corresponds to maximizing the posterior likelihood.\nWhile frequentist methods rely on sampling distributions and the (theoretical) physical act of random sampling, Bayesian models do not! The “sampling” we are doing here is small world sampling – our samples are from the model, we don’t expect them to be “real.”\nDummy data generated by the prior predictive distribution, the distribution of the parameters of interest using only the priors and not the data, help us build models. These simulations can tell us whether the priors are reasonable or not.\nOnce we update the model with the data, we can generate samples from the posterior predictive distribution. These samples can help us check how accurate the model is or how well it fit. This distribution is “honest” because it propagates the uncertainty embodied in the posterior distribution of the parameter of interest."
  },
  {
    "objectID": "sr/cp3.html#exercises",
    "href": "sr/cp3.html#exercises",
    "title": "Chapter 3: Sampling the Imaginary",
    "section": "Exercises",
    "text": "Exercises\nFor the easy exercises, we need the following code given in the book.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000)\nlikelihood &lt;- dbinom(6, size = 9, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\n3E1. How much posterior probability lies below \\(p = 0.2\\)?\n\nmean(samples &lt;= 0.2)\n\n[1] 4e-04\n\n\n3E2. How much posterior probability lies below \\(p = 0.8\\)?\n\nmean(samples &lt;= 0.8)\n\n[1] 0.8884\n\n\n3E3. How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\).\nWe could calculate this directly.\n\nmean((samples &gt;= 0.2) & (samples &lt;= 0.8))\n\n[1] 0.888\n\n\nOr if we had stored the previous calculations, we could have used those instead.\n\nmean(samples &lt;= 0.8) - mean(samples &lt;= 0.2)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of \\(p\\)?\n\nquantile(samples, probs = c(0.2))\n\n      20% \n0.5185185 \n\n\n3E5. 20% of the posterior probability lies above which value of \\(p\\)?\n\nquantile(samples, probs = c(0.8))\n\n      80% \n0.7557558 \n\n\n3E6. Which values of \\(p\\) contain the narrowest interval equal to 66% of the posterior probability?\n\nrethinking::HPDI(samples, prob = 0.66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\n3E7. Which values of \\(p\\) contain 66% of the posterior probability, assuming equal posterior probability both above and below the interval?\n\nrethinking::PI(samples, prob = 0.66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\n3M2. Draw 10000 samples from the grid approximate prior abnove. Then use the samples to calculate the 90% HDPI for \\(p\\).\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = 0.90)\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\n3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses?\n\nppc &lt;- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.1499\n\n\n3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.\n\nppc2 &lt;- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.1842\n\n\n3M5. Start over at 3M1, this time using a prior that is zero below \\(p = 0.5\\) and a constant above \\(p = 0.5\\).\nFirst we approximate the posterior and take samples.\n\n# I should name these different things but I am not going to.\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- ifelse(p_grid &lt; 0.5, 0, 2)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nNow we’ll do the first posterior predictive check and estimate the probability. Note that the true probability (if \\(p = 0.7\\)) is \\(0.08113\\).\n\nppc &lt;- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.163\n\n\nIt looks like this estimate is actually slightly worse with this prior than it was with the uniform prior. However, they are fairly similar.\nAnd the second check. Note that the true probability is \\(0.2668279\\)\n\nppc2 &lt;- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.2353\n\n\nThis estimate is much closer to the true value than the previous estimate was. It seems that this prior allows us to more accurate estimate probabilities close to the true value (\\(p = 0.7\\)), but not near the lower boundary for the prior. We can examine the histogram.\n\nrethinking::simplehist(ppc)\n\n\n\n\n\n\n\n\nWe can see that the low values are extremely low, but so are the high values. We would expect the mode to be around 10 or 11, but since we observed 8 / 15, it makes sense that we get a higher estimate of the probability of this occurring than what we “know” is true.\n3M6. We want to construct a 99% percentile interval of the posterior distribution of \\(p\\) that is only 0.05 wide. How many times will we have to toss the globe to do this?\nTo me, this question seems phrased in the general, but I think it is impossible to answer in general. So we’ll do our best. First let’s look at the width of the current PI.\n\nrethinking::PI(samples, prob = 0.99) |&gt; diff()\n\n     100% \n0.3243243 \n\n\nThat’s much larger than what we want, but we only tossed the ball 15 times. So we’ll need to do some simulating to solve this problem. I know this is not the “true” probably, but for the sake of keeping with this model, I’ll make sure all of our larger samples have (approximately) the same \\(8/15\\) probability of water.\nI’ll also continue using the flat prior. The answer to this question depends on both the “true” value of \\(p\\) and the prior that we used.\n\none_sim &lt;- function(N) {\n    likelihood &lt;- dbinom(floor(N * (8/15)), size = N, prob = p_grid)\n    posterior &lt;- likelihood * prior\n    posterior &lt;- posterior / sum(posterior)\n\n    set.seed(100)\n    samples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n    \n    out &lt;- rethinking::PI(samples, prob = 0.99) |&gt; diff()\n    return(out)\n}\n\nmy_n &lt;- seq(from = 10, to = 5000, by = 10)\nsim_res &lt;- purrr::map_dbl(my_n, one_sim)\nplot(sim_res ~ my_n, type = \"l\")\nabline(h = 0.05, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nVisually, we can see that around 3000 samples are necessary, let’s get the exact estimate.\n\nindex &lt;- min(which(sim_res &lt; 0.05))\ncat(\"n: \", my_n[index], \"; width: \", sim_res[index], sep = \"\")\n\nn: 2740; width: 0.04905405"
  },
  {
    "objectID": "sr/cp3.html#hard-problems",
    "href": "sr/cp3.html#hard-problems",
    "title": "Chapter 3: Sampling the Imaginary",
    "section": "Hard Problems",
    "text": "Hard Problems\nFor the hard problems, we need to load the indicated data set.\n\ndata(homeworkch3, package = \"rethinking\")\ncombined &lt;- c(birth1, birth2)\n\n3H1. Use grid approximation to compute the posterior distribution for the probability of a birth being a boy.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000) # Uniform prior\nlikelihood &lt;- dbinom(sum(combined), size = length(combined), prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nplot(posterior ~ p_grid, type = \"l\")\n\n\n\n\n\n\n\n\n3H2. Draw 10000 random samples from the posterior, and use these to estimate the 50, 89, and 97 percent HDPIs.\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = c(0.5, 0.89, 0.97))\n\n    |0.97     |0.89      |0.5      0.5|     0.89|     0.97| \n0.4824825 0.4994995 0.5265265 0.5725726 0.6076076 0.6296296 \n\n\n3H3. Simulate 10,000 replicates of 200 births. Compare the distribution of predicted counts to the actual count. Does it look like the model fits the data well?\n\nset.seed(100)\nppc &lt;- rbinom(1e4, size = 200, prob = samples)\nrethinking::simplehist(ppc)\nabline(v = sum(combined), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nIn this particular simulation, the observed value (111 boys) is a central, likely outcome of the posterior predictive distribution. The model seems to fit the data well, although there is a fairly large amount of spread.\n3H4. Now compare 10,000 counts of boys from 100 simulated firstborns only to the number of boys in the first births.\n\nset.seed(100)\nb1_samp &lt;- rbinom(10000, size = 100, prob = samples)\nrethinking::simplehist(b1_samp)\nabline(v = sum(birth1), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThe model seems to overestimate the number of firstborn boys. This could potentially be because our observed count of boys is slightly higher than 50% (\\(0.56 \\%\\)) and this overestimation becomes more prominent in the smaller sample size. However, the true value is not in the tails of our distribution, so we would probably capture it in a CI.\n3H5. Our model assumes that sex of first and second births are independent. We can check this assumption by focusing on second births that followed female firstborns. Compare 10,000 simulated counts of boys to only those second births that followed girls.\n\n# Get the correct count\nn &lt;- sum(birth2[birth1 == 0])\n\n# Run the simulation\n\nb2_samp &lt;- rbinom(1e4, size = n, prob = samples)\n\n# Plot the results\nrethinking::simplehist(b2_samp, xlim = c(min(b2_samp), max(n, max(b2_samp))))\nabline(v = n, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWow, the number of boys who follow girls is much larger than our model predicts. Either our sample is far away from the “real” value (although this is really more of a frequentist notion), or more likely, the assumption of our model is wrong."
  },
  {
    "objectID": "sr/cp5.html",
    "href": "sr/cp5.html",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "",
    "text": "This chapter discusses the “causal salad” issue that is really prevelant in epidemiology (and other sciences) right now. When you “adjust” for variables in models, what are you actually doing? What answers can you get from adjusting? How do you decide what variables should go into a model? We get to talk about confounding, which is one of my favorite subjects, and more generally, other types of biases. These are presented in the framework of graphical causal models using directed acyclic graphs (DAGs)."
  },
  {
    "objectID": "sr/cp5.html#chapter-notes",
    "href": "sr/cp5.html#chapter-notes",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nThis chapter is mainly concerned with the issue of confounding, although it doesn’t used the same technical terms that I learned in my epidemiology classes.\nSpecifically, we are concerned with spurious associations (positive confounding) where a third variable causes the relationship between two variables to appear stronger than it is;\nAnd with masked relationships (negative confounding), where a third variable causes the relationship between two variables to appear weaker than it is.\nThis chapter also introduces Directed Acyclic Graphs (DAGs) as heuristic graphical causal models for understanding the causal relationships between variables.\nThe conditional independencies of DAGs are disccused, which are statements of which variables should be associated with each other in the data or not, given that the DAG is an accurate causal model. DAGs with the same variables and implied conditional independencies are called Markov Equivalent.\nThis chapter also discusses three ways to visualize the results of a regression model: predictor residual plots, posterior prediction plots, and counterfactual plots. In this case, the counterfactual plot does not necessarily mean what I am used to it meaning, it just means we are predicting values which may not have been observed using the model (so in some sense they are counterfactual to our observed data).\nFinally, this chapter also discusses categorical variables and index coding, which is used by the rethinking package (and later by Stan) rather than the dummy coding used by most R models."
  },
  {
    "objectID": "sr/cp5.html#exercises",
    "href": "sr/cp5.html#exercises",
    "title": "Chapter 5: The Many Variables and the Spurious Waffles",
    "section": "Exercises",
    "text": "Exercises\n\n5E1\nThe linear models\n\\[\\mu_i = \\beta_x x_i + \\beta_z z_i\\] and \\[\\mu_i = \\alpha + \\beta_x x_i + \\beta_z z_i\\]\nare both multiple regression models. The first model \\[\\mu_i = \\alpha + \\beta x_i\\] is a simple linear regression model and while the third model \\[\\left( \\mu_i = \\alpha + \\beta (x_i - z_i) \\right)\\] involves both \\(x\\) and \\(z\\), the model only has one coefficient and treats their difference as a single explanatory variable.\n\n\n5E2\nWe could evaluate the claim animal diversity is linearly related to latitude, but only after controlling for plant diversity using the linear model \\[\\begin{align*}\n\\text{animal diversity}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\mathrm{latitude} \\right) + \\beta_2 \\left( \\text{plant diversity} \\right)\n\\end{align*}\\]\nwhere suitable priors are assigned and other appropriate parameters are given for the likelihood function.\n\n\n5E3\nWe could evaluate the claim neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree using the multiple linear regression \\[\\begin{align*}\n\\text{time to PhD}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\text{amount of funding} \\right) + \\beta_2 \\left( \\text{size of laboratory} \\right)\n\\end{align*}\\]\nwith suitable priors, etc. The slope of both \\(\\beta_j\\) should be positive. Classically, I would probably be inclined to include an interaction term in this model, but we haven’t talked about that yet in the book so I didn’t.\n\n\n5E4\nIf we have a single categorical variable with levels \\(A,\\) \\(B,\\) \\(C,\\) and \\(D,\\) (represented as indicator variables), the following linear models are inferentially equivalent: \\[\\begin{align*}\n\\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i, \\quad \\text{ and }\\\\\n\\mu_i &= \\alpha_A \\left( 1 - B_i - C_i - D_i \\right) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i.\n\\end{align*}\\]\n\n\n5M1\nAn example of a spurious correlation: I am happy on days when it is sunny outside, and when I get to leave work early. Both of these things individually make me happy, but the weather doesn’t determine whether I get to leave work early. (At least not fully anyways. The weather definitely determines how much work I get done, but other external factors control the amount of work I have and the deadlines I need to meet.)\n\n\n5M2\nAn example of a masked relationship that I like: supposed you have multiple measurements of how far the accelerator is pressed in a car and the car’s speed (taken simultaneously) at multiple time points, and you see no correlation. However, you are then given the measurements of the slope of the road at each of those time points, and you see that, when the slope of the road is taken into account, the two variables are correlated. When the car is going uphill, the accelerator is always pressed further and the speed is always lower, but pressing the accelerator still increases the speed.\n\n\n5M3\nI guess a higher divorce rate could cause a higher marriage rate by making more people available to be married. If divorced people tend to get remarried (potentially to non-divorced people), then the overall marriage rate could go up. Addressing this using a multiple linear regression model would be quite difficult, as you would need more data on remarriage and divorce status. It could be difficult to incorporate remarriages into the regression model, maybe an agent-based model would be more intuitive for this.\n\n\n5M4\nI found this list of percent LDS population by state. So if we want to add this as a predictor to the divorce rate model, first I’ll join these data to the WaffleDivorce data from the rethinking package.\n\npct_lds &lt;- readr::read_csv(here::here(\"static/pct_lds.csv\"))\n\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (3): mormonPop, mormonRate, Pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\nThis is cmdstanr version 0.8.1.9000\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n- CmdStan version: 2.34.1\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\nLoading required package: posterior\nThis is posterior version 1.6.0\n\nAttaching package: 'posterior'\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nLoading required package: parallel\nrethinking (Version 2.40)\n\nAttaching package: 'rethinking'\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(\"WaffleDivorce\")\n\ndat_5m4 &lt;-\n    dplyr::left_join(\n        WaffleDivorce,\n        pct_lds,\n        by = c(\"Location\" = \"State\")\n    )\n\ndat_5m4[\n    which(dat_5m4$Location == \"District of Columbia\"),\n    \"mormonRate\"\n] &lt;-0.0038\n\ndat_5m4_l &lt;-\n    dat_5m4 |&gt;\n    dplyr::transmute(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage,\n        L = mormonRate * 100\n    ) |&gt;\n    as.list()\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\npurrr::walk2(dat_5m4_l, names(dat_5m4_l), ~hist(.x, main = .y, breaks = \"FD\"))\n\n\n\n\n\n\n\nlayout(1)\n\nWhen I tried to do this the first time, it turns out that the WaffleDivorce dataset has D.C. in it, but not the state of Nevada? And the table of percent LDS populations I found has Nevada, but not D.C. So I just googled it, and on the Wikipedia page I saw that the percentage was 0.38% in 2014, which is good enough for government work, so I filled it in manually. I didn’t transform any of the predictors, but standardizing and transforming them would probably be a good idea. I think a logit transformation would probably be suitable for the percent LDS population but as long as quap() converges I won’t worry about it too much.\n\\[\\begin{align*}\n\\text{Divorce rate}_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot \\text{Marriage rate}_i + \\beta_2 \\cdot \\text{Median age at marriage}_i \\\\\n&\\quad\\quad + \\beta_3 \\cdot \\text{Percent LDS}_i \\\\\n\\beta_0 &\\sim \\mathrm{Unif}(0, 1000); \\\\\n\\beta_j &\\sim \\mathrm{Normal}(0, 10); \\quad j = \\{0, \\ldots, 3\\} \\\\\n\\sigma &\\sim \\mathrm{Exp}(0.5)\n\\end{align*}\\]\nwhere \\(i\\) indexes the states. Since the outcome is in units of divorces per 1000 people, I decided to let the intercept be anything from 0 to 1000 people. It will probably be quite small but that shouldn’t be too much of a problem I think. Then, I assigned weakly uninformative priors to the slope coefficients and a simple positive prior to the standard deviation. It would be better to do a prior predictive simulation and figure out some better assumptions. Now we’ll fit th model with quap (quadratic approximation).\n\nset.seed(100)\nfit_5m4 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            # We could rewrite this so we didn't have to write out all the identical\n            # priors but this is easier and I am lazy\n            mu &lt;- b0 + bM * M + bA * A + bL * L,\n            b0 ~ dunif(0, 100),\n            bM ~ dnorm(0, 10),\n            bA ~ dnorm(0, 10),\n            bL ~ dnorm(0, 10),\n            sigma ~ dexp(0.5)\n        ),\n        data = dat_5m4_l\n    )\n\nrethinking::precis(fit_5m4)\n\n              mean         sd       5.5%       94.5%\nb0    38.634820524 6.90452240 27.6000602 49.66958086\nbM     0.003608067 0.07550132 -0.1170576  0.12427376\nbA    -1.105494100 0.22397859 -1.4634551 -0.74753306\nbL    -0.066488235 0.02396230 -0.1047846 -0.02819184\nsigma  1.331562503 0.13193156  1.1207104  1.54241461\n\n\nWe can see that for every one percentage increase in the LDS population of a state, the model predicts that the divorce rate will decrease by -0.07 units. Since the divorce rate is in percentage units, this means we would need slightly less than a 15% increase in LDS population for a state’s divorce rate to decrease by 1%.\nThis estimate is probably biased by Utah, which is a strong outlier with 63% LDS population (far more than the second highest state, Idaho, with 24% of the population identifying as LDS).\n\n\n5M5\nFor this exercise, I’ll call the price of gas \\(G\\), obesity \\(O\\), exercise \\(E\\), and eating at restaurants \\(R\\). The dag for this hypothesis looks like this.\n\ndag &lt;- dagitty::dagitty(\n    \"dag {\n        G -&gt; E\n        G -&gt; R\n        G -&gt; O\n        E -&gt; O\n        R -&gt; O\n    }\"\n)\n\nplot(dagitty::graphLayout(dag))\n\n\n\n\n\n\n\n\nTherefore, \\(G\\) confounds the relationship of \\(E\\) on \\(O\\) and also confounds the relationship of \\(R\\) on \\(O\\). (The direct causal effect of \\(G\\) on \\(O\\) represents the total impact of any other pathways that we have not measured.) We could capture both of these pathways simultaneously with the multiple regression \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_R R_i + \\beta_G G_i,\\] where \\(\\mu_i\\) is the conditional mean of \\(O_i\\). This model “controls for” the price of gas, and additionally controls for the effect of \\(E\\) and \\(R\\) on each other. If we are certain that \\(E\\) and \\(R\\) are related only though \\(G\\), we could fit the two regression models \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_G G_i\\] and \\[\\mu_i = \\alpha + \\beta_R R_i + \\beta_G G_i.\\] Using these two models, we control for the effect of gas price and obtain the direct causal effect of \\(E\\) and \\(R\\), assuming that \\(E\\) and \\(R\\) do not affect each other at all.\n\n\n5H1\nAssuming the DAG for the divorce problem is \\(M \\to A \\to D\\). The DAG only has one conditional independency: \\(D\\) and \\(M\\) are uncorrelated if we condition on \\(A\\). We can check this using dagitty as well.\n\ndag2 &lt;- dagitty::dagitty(\"dag {M -&gt; A -&gt; D}\")\ndagitty::impliedConditionalIndependencies(dag2)\n\nD _||_ M | A\n\n\nNow we can check if the data are consistent with this DAG. (We already know that it is, because this is the same conditional independency set as one of the previous example DAGs).\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::select(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage\n    ) |&gt;\n    dplyr::mutate(dplyr::across(tidyselect::everything(), standardize))\n\n# Fit the model without age so we can see the unconditional estimate\nmodel_noage &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age only\nmodel_ageonly &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age to see the estimate after conditioning\nmodel_5h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\ncoeftab_plot(\n    coeftab(model_noage, model_ageonly, model_5h1),\n    par = c(\"bA\", \"bM\")\n)\n\n\n\n\n\n\n\n\nWe can see that before we add age to the model, the effect of marriage is quit large. But then when we condition on age, the effect is close to zero with a large amount of uncertainty. So it appears that our data are consistent with the conditional independencies of the model. The coefficient for age does not change when we condition it on marriage rate, so this supports our conclusion.\n\n\n5H2\nAssuming that this is the true DAG for the divorce example, we want to fit a new model and estimate the counterfactual effect of halving a state’s marriage rate \\(M\\).\n\nset.seed(100)\nm_5h2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            ## M -&gt; A\n            A ~ dnorm(mu_A, sigma_A),\n            mu_A &lt;- b0_A + bM * M,\n            b0_A ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma_A ~ dexp(1),\n            \n            ## A -&gt; D\n            D ~ dnorm(mu_D, sigma_D),\n            mu_D &lt;- b0_D + bA * A,\n            b0_D ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma_D ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_5h2)\n\n                 mean         sd       5.5%      94.5%\nb0_A    -6.922370e-08 0.08684788 -0.1387998  0.1387996\nbM      -6.947376e-01 0.09572699 -0.8477278 -0.5417474\nsigma_A  6.817373e-01 0.06758016  0.5737312  0.7897434\nb0_D    -4.353665e-07 0.09737877 -0.1556305  0.1556296\nbA      -5.684028e-01 0.10999981 -0.7442038 -0.3926019\nsigma_D  7.883257e-01 0.07801134  0.6636485  0.9130029\n\n# Get the halved marriage rate for each state and standardize to model units\nwith(\n    WaffleDivorce,\n    M_seq &lt;&lt;- c(\n        ((Divorce / 2) - mean(Divorce)) / sd(Divorce),\n        d$D\n    )\n)\n\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\nres &lt;-\n    s |&gt;\n    # stack the matrix columns on top of each other into one vector\n    lapply(as.vector) |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::mutate(\n        M = rep(M_seq, each = nrow(s$A))\n    )\n    \nres_diff &lt;-\n    tibble::tibble(\n        A = s$A[, 2] - s$A[, 1],\n        D = s$D[, 2] - s$D[, 1]\n    )\n\ntest &lt;-\n    lapply(s, \\(x) tibble::tibble(\n        mean = colMeans(x[, 51:100] - x[, 1:50]),\n        lwr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[1, ],\n        upr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[2, ]\n    ))\n\ntest2 &lt;- test$D\ntest2 &lt;- cbind(test2, WaffleDivorce)\n\nlibrary(ggplot2)\nggplot(test2, aes(y = forcats::fct_reorder(Location, mean),\n                                    x = mean, xmin = lwr, xmax = upr)) +\n    geom_pointrange() +\n    scale_x_continuous(\n        labels = function(x) scales::label_number()((x * sd(WaffleDivorce$Divorce)))\n    ) +\n    labs(\n        x = \"Counterfactual effect on divorce rate of halving marriage rate (mean, 89% CI)\",\n        y = NULL\n    ) +\n    hgp::theme_ms()\n\n\n\n\n\n\n\n\nFrom the precis, we can derive that for every 1 unit increase in the marriage rate, we expect \\(\\beta_M\\) units of change in the median age of marriage, and thus \\(\\beta_A \\beta_M\\) units of change in the divorce rate, which works out to approximately \\((-0.69)(-0.57) = 0.3933\\). So if the marriage rate increases by 1 standard deviation, we expect the divorce rate to increase by about 0.4 standard deviations.\nWe could also (much more easily, in fact, I just didn’t think about it until after I did this the hard way) compute the counterfactual effect for a range of values, and then look up whatever we wanted. The divorce rate measurements in the original data are only measured to the nearest tenth, so we just need to simulate the counterfactual effect of every one-tenth unit change in divorce rate over the range of observed rates.\n\n# Generate the sequence\nM_seq &lt;-\n    seq(\n        from = 4,\n        to = 16,\n        by = 0.01\n    )\n# Restandardize it using the original values for model units\nM_seq &lt;- (M_seq - mean(WaffleDivorce$Marriage)) /\n    sd(WaffleDivorce$Marriage)\n\n# Do the simulation\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\n\n# Clean up the results\nD_res &lt;-\n    tibble::tibble(\n        M = round(M_seq * sd(WaffleDivorce$Marriage) + mean(WaffleDivorce$Marriage),\n                            digits = 2),\n        mean = colMeans(s$D),\n        lwr = apply(s$D, 2, rethinking::PI)[1, ],\n        upr = apply(s$D, 2, rethinking::PI)[2, ]\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(c(mean, lwr, upr), \\(x) x * sd(WaffleDivorce$Divorce) +\n                                        mean(WaffleDivorce$Divorce) |&gt; round(digits = 2))\n    )\n\nmanipulated &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        Location, Loc, M = Marriage / 2, orig = Marriage, out = Divorce\n    ) |&gt;\n    dplyr::left_join(D_res, by = \"M\")\n\n# Make the plot\nD_res |&gt;\n    ggplot(aes(x = M, y = mean, ymin = lwr, ymax = upr)) +\n    geom_ribbon(fill = \"gray\") +\n    geom_line(size = 0.75) +\n    geom_point(\n        data = manipulated,\n        fill = \"white\",\n        color = \"black\",\n        shape = 21,\n        stroke = 1.5,\n        size = 3\n    ) +\n    labs(\n        x = \"Manipulated marriage rate\",\n        y = \"Counterfactual divorce rate\"\n    ) +\n    hgp::theme_ms()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# manipulated |&gt;\n#   dplyr::mutate(id = factor(dplyr::row_number())) |&gt;\n#   ggplot() +\n#   geom_segment(\n#       aes(x = orig, xend = M, y = out, yend = mean, color = id),\n#       show.legend = FALSE,\n#       alpha = 0.5\n#   ) +\n#   geom_point(\n#       aes(x = M, y = mean, shape = \"Counterfactual\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   geom_point(\n#       aes(x = orig, y = out, shape = \"Observed\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   guides(\n#       fill = guide_none()\n#   ) +\n#   scale_shape_manual(values = c(21, 22)) +\n#   labs(\n#       x = \"Marriage rate\",\n#       y = \"Divorce rate\"\n#   ) +\n#   hgp::theme_ms()\n\nThere we go. The white points here show each of the states if their divorce rate were halved (and the model is true). I didn’t label them because I had already spend too much time on this project. There’s a lot more I could think of to do on this problem, but instead I decided to move on to the next one instead.\n\n\n5H3\nWe are given the following DAG for the milk energy problem.\n\ndag3 &lt;- dagitty::dagitty(\"dag {K &lt;- M -&gt; N -&gt; K}\")\n\nWe want to compute the counterfactual effect on K of doubling M, accounting for both the direct and indirect paths of causation.\n\ndata(\"milk\")\nm &lt;-\n    milk |&gt;\n    dplyr::transmute(\n        N = standardize(neocortex.perc),\n        M = standardize(log(mass)),\n        K = standardize(kcal.per.g)\n    ) |&gt;\n    tidyr::drop_na()\n\nm_5h3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            # M -&gt; N\n            N ~ dnorm(mu_n, sigma_n),\n            mu_n &lt;- a_n + b_m * M,\n            a_n ~ dnorm(0, 0.2),\n            b_m ~ dnorm(0, 0.5),\n            sigma_n ~ dexp(1),\n            # M -&gt; K &lt;- N\n            K ~ dnorm(mu_k, sigma_k),\n            mu_k &lt;- a_k + b_m * M + b_n * N,\n            a_k ~ dnorm(0, 0.2),\n            b_n ~ dnorm(0, 0.5),\n            sigma_k ~ dexp(1)\n        ),\n        data = m\n    )\n\nrethinking::precis(m_5h3)\n\n                mean        sd       5.5%     94.5%\na_n     -0.008405637 0.1274276 -0.2120595 0.1952482\nb_m      0.417649823 0.1672193  0.1504011 0.6848986\nsigma_n  0.681045996 0.1323715  0.4694908 0.8926012\na_k      0.026405034 0.1660113 -0.2389131 0.2917232\nb_n     -0.138437700 0.2789888 -0.5843156 0.3074402\nsigma_k  1.223352397 0.2214232  0.8694753 1.5772295\n\n\nIn this case, the predictor we want the counterfactual effect of (\\(M\\)) is on a log scale, so we should be able to get the effect of halving it (since changes will be proportional on a multiplicative scale). But I’m not sure I formally understand what the “total counterfactual effect” is well enough to do that. So I’ll simulate instead.\n\n# Sequence in log units\nM_seq &lt;- seq(from = -3, to = 5, by = 0.01)\n\n# Standardize with original values to model units\nM_seq &lt;- (M_seq - mean(log(milk$mass), na.rm = TRUE)) /\n    sd(log(milk$mass), na.rm = TRUE)\n\n# Simulate the predictions\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h3, data = sim_dat, vars = c(\"N\", \"K\"))\n\nplot_data &lt;-\n    tibble::tibble(\n        M = exp(sim_dat$M * attr(m$M, \"scaled:scale\") +\n            attr(m$M, \"scaled:center\")),\n        K = colMeans(s$K) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n    )\nplot_PI &lt;- apply(s$K, 2, PI) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n\n# Plot the counterfactual effect\nplot(\n    plot_data$M, plot_data$K,\n    type = \"l\",\n    xlab = \"manipulated mass (kg)\", ylab = \"counterfactual kilocalories per gram of milk\",\n    xlim = exp(c(-3, 5)),\n    ylim = c(0.1, 1.1)\n)\nshade(plot_PI, plot_data$M)\nmtext(\"Total counterfactual effect of M on K\")\n\n\n\n\n\n\n\n\nAgain, the question didn’t say what number to double, but you can get any of them from a simulation like this.\n\n\n5H4\nThis is an open-ended problem where we will consider how to add the indicator of being in the South to the marriage problem. There are a few possibilities for the causal implications of this.\n\nSomething about Southerness directly affects age at marriage, marriage rate, and divorce rate all at the same time. (Or divorce rate, and one of the two predictors) That is, Southerness has both direct and indirect effects on divorce rate.\nSomething about Souttherness directly affects age at marriage and marriage rate, having only indirect effects on divorce rate. Alternatively, Southerness only impacts one of these two variables.\nSomething about Southernness directly affects divorce rate, with no indirect effects.\n\nMy first instinct is to say that age at marriage and marriage rate are primarily influenced by socioeconomic factors. Due to multiple historical factors (including slavery and an enduring legacy of systemic racism), the southern US, on average, has lower education rates and higher poverty rates as a region. Increased socioeconomic status tends to be associated with higher age of marriage, but a higher marriage rate and lower divorce rate (according to what I read while googling this).\nHowever, the southern U.S. also has a unique subculture (which varies widely across regions of the south), which could be a sociological cause of differences in some of these variables. For example, I think it is reasonable to say that traditional Southern culture encourages women to marry young, and also encourages women to get married in general – there is definitely a stereotype about older, unmarried women in traditional Southern culture.\nSo, based on both the socioeconomic reasons and the “culture” argument, there are two models I would like to examine. First, the model that posits a direct effect of Southerness on divorce rate, as well as indirect effects on both age at marriage and marriage rate. Then, I’d also like to examine the model where there is no direct effect of Southerness on divorce rate, and Southerness acts on divorce rate via age at marriage and marriage rate.\nLet’s examine the model with no direct effects first. We’ll consider the conditional independencies of this DAG.\n\ndag_indirect &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D}\")\nplot(dag_indirect)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_indirect)\n\nD _||_ S | A, M\n\n\nSo, if the data are consistent with this DAG, then the divorce rate should be independent of being in the South after we control for age at marriage and marriage rate. So let’s fit a model that does that. Since Southern status is an indicator variable, I wasn’t quite sure how to handle it in this model. In a frequentist framework, I would probably want to include an interaction term – this was briefly mentioned at the beginning of this chapter, but has not been covered in detail so I’ll just use a simple additive-effects-only model for now.\nBut first let’s consider the conditional independencies of the DAG with direct effects.\n\ndag_direct &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D; S -&gt; D}\")\nplot(dag_direct)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_direct)\n\nThere are no conditional independencies for this graph! So we should expect to see a relationship for all three parameters in the model that conditions on all three of our predictor variables.\nFortunately for us, we can use the same model to evaluate both of these DAGs – we just need to see which of the conditional independencies are supported by the result of predicting \\(D\\) using all three of them.\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        D = standardize(Divorce),\n        M = standardize(Marriage),\n        A = standardize(MedianAgeMarriage),\n        S = South\n    )\n\nm_s_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bS * S,\n            a ~ dnorm(0, 0.2),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_m_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_a_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_all &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A + bS * S,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_all)\n\n             mean         sd         5.5%      94.5%\na     -0.07590505 0.10600330 -0.245318792  0.0935087\nbM    -0.04224048 0.14778943 -0.278436533  0.1939556\nbA    -0.56163739 0.15090185 -0.802807694 -0.3204671\nbS     0.34998019 0.21572095  0.005216455  0.6947439\nsigma  0.76290395 0.07580295  0.641756198  0.8840517\n\ncoeftab_plot(\n    coeftab(\n        m_a_only, m_s_only, m_m_only, m_all\n    ),\n    pars = c(\"bA\", \"bS\", \"bM\")\n)\n\n\n\n\n\n\n\n\nSo, interestingly, from the estimated coefficients, we see that the coefficient for being in the South does not change that much when we control for both of the other variables. Of course, the CI crosses zero in the new model, so if we were doing bad statistics we would say that the effect has disappeared, but it was significant in the S-only model, which means that there is no direct effect of S. Since we are not doing bad statistics though, it seems unreasonable to claim that – there appears to be an effect of both \\(A\\) and \\(S\\) in the final model.\nRecall our previous model which was consistent with the idea that marriage rate only impacts divorce rate through the effect of age of marriage. So another potential DAG is like this.\n\ndag_med &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A -&gt; D; M -&gt; A -&gt;; S -&gt; D}\")\nplot(dag_med)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_med)\n\nD _||_ M | A, S\n\n\nSo the only casual independency of this DAG is whether D is independent of M after controlling for A and S, and after fitting our previous model we see that the data are consistent with this DAG. Arguably the data are not consistent with the other DAGs since those DAGs do not have any conditional independencies involving M, but the best model should still be chosen by a domain expert, not based on what the data are consistent with after observing the data."
  },
  {
    "objectID": "sr/cp7.html",
    "href": "sr/cp7.html",
    "title": "Chapter 7: Ulysses’ Compass",
    "section": "",
    "text": "Predictive accuracy is the main focus of this chapter. The titular metaphor “Ulysses’ compass” refers to the mythological hero Ulysses navigating the path between the two monsters Charybdis and Scylla, who each lived on either side of a narrow strait. Sailors attempting to avoid Charybdis would sail too close to Scylla, and vice versa. McElreath likens this to the scientist navigating between underfitting and overfitting. Major topics of this chapter include out-of-sample predictive accuracy estimation via LOOCV, PSIS, and WAIC; regularizing priors; and the prediction/inference trade-off."
  },
  {
    "objectID": "sr/cp7.html#chapter-notes",
    "href": "sr/cp7.html#chapter-notes",
    "title": "Chapter 7: Ulysses’ Compass",
    "section": "Chapter notes",
    "text": "Chapter notes\n\nWhen we think about Copernicus and the heliocentric model, we have to remember that the geocentric model makes very good predictions, despite being wrong. Models that are completely causally wrong can make great predictions.\nThe principle of parsimony is often used to distinguish between models which make good predictions, but even this is not always useful for us in science.\n\\(R^2\\) is a commonly used method for choosing the “best” regression model, but even this is not correct. In general, overfitting and underfitting are both dangerous and we must be wary of methods which can lead us to overfit. The notion of overfitting and underfitting is related to the bias-variance tradeoff.\nTo construct useful measures in a Bayesian framework, we need to consider the entropy of our models – that is, how much is our uncertainty reduced if we learn an outcome? The information entropy is the function \\[h(p) = -\\sum_{i=1}^n p_i \\cdot \\log(p_i).\\]\nWe can relate the entropy of the model to the accuracy of our predictions using the Kullback-Leibler divergence: the additional uncertainty induced by using probabilities from one distribution to describe another. The divergence is given by \\[D_{KL}(p, q) = \\sum_i p_i \\log \\left( \\frac{p_i}{q_i}\\right).\\]\nUsing the divergence, or the related deviance, we cannot estimate how close a model is to the truth. However, we can tell which of a set of models is closest to the truth, and how much better it is from the others.\nWe estimate the deviance as \\(-2 \\times \\texttt{lppd}\\) where \\(\\texttt{lppd}\\) is the log pointwise predictive density. The formula is omitted here but is on page 210 of the book.\nImportantly, we cannot simply score models on the data used to fit the models, because this leads to overfitting.\nWe can often use regularizing priors, which are skeptical and try to prevent the model from taking on extreme values, to improve our out-of-sample performance.\nThe traditional way to approximate out-of-sample error is by using Cross-Validation, specifically Leave-One-Out CV (LOOCV).\nSince LOOCV is computationally very expensive, we want to approximate it. One method is called Pareto-smoothed importance sampling (PSIS), and another is called the Widely Applicable Information Criterion (WAIC). See the text, section 7.4 for details on both methods.\nAs a sidenote, this chapter discusses robust regression using a \\(t\\) likelihood instead of a Gaussian likelihood on page 233.\nI really enjoyed the metaphor that “if we search hard enough, we are bound to found a Curse of Tippicanoe” – if we torture the data enough, it will confess."
  },
  {
    "objectID": "sr/cp7.html#exercises",
    "href": "sr/cp7.html#exercises",
    "title": "Chapter 7: Ulysses’ Compass",
    "section": "Exercises",
    "text": "Exercises\n\n7E1\nThe three motivating criteria which define information entropy are\n\nEntropy should be a continuous-valued function;\nAs the size of the sample space increases, entropy should increase for events that are equally likely; and\nIf the entropy associated with the event \\(E_1\\) is \\(h_1\\) and the entropy associated with the event \\(E_2\\) is \\(h_2\\), then the entropy associated with the event \\(E_1 \\cup E_2\\) should be \\(h_1 + h_2\\).\n\n\n\n7E2\nIf a coin is weighted such that when the coin is flipped, the probability of heads is \\(70\\%\\) is given by \\[h = -\\left( 0.7 \\cdot \\log(0.7) + 0.3 \\cdot \\log(0.3) \\right) \\approx 0.61,\\] because the only other possibility is that the coin lands on tails, which occurs with probability \\(0.3\\).\n\n\n7E3\nSuppose that a four-sided die is weighted so that each possible outcome occurs with the frequency given in the following table.\n\n\n\nroll\np\n\n\n\n\n1\n0.20\n\n\n2\n0.25\n\n\n3\n0.25\n\n\n4\n0.30\n\n\n\nThe entropy is then \\[h = -\\sum_{i = 1}^4 p_i \\cdot \\log p_i \\approx 1.38.\\]\n\n\n7E4\nSuppose we have another 4-sided die when the sides 1, 2, and 3 occur equally often but the side 4 never occurs. If \\(X\\) is the random variable representing the result of the die roll, we calculate the entropy over the support of \\(X\\), which is \\(S(X) = \\{1, 2, 3\\}\\) and we leave the value of 4 out of the calculation entirely. The entropy is then\n\\[h = -3\\left( \\frac{1}{3}\\cdot\\log\\frac{1}{3}\\right) \\approx 1.10.\\]\n\n\n7M1\nThe definition of the AIC is \\[\\mathrm{AIC} = -2(\\mathrm{lppd} - p),\\] while the definition of the WAIC is \\[\\mathrm{WAIC} = -2 \\left(\\mathrm{lppd} - \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right)\\right).\\]\nBoth of these formulas involve comparing the lppd to some penalty term, which is more general for the WAIC than for the AIC. In order for the AIC to be similar to the WAIC, we need to make assumptions which lead to the equivalence \\[ p = \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right), \\] on average.\nAccording to the text, this will occur if we assume that the priors are flat (or are overwhelmed by the likelihood), the posterior distribution is approximately multivariate Gaussian, and the sample size is much greater than the number of parameters. So for models with complicated, hierarchical likelihoods, which are common in actual research questions, the AIC will likely not be a good approximation to the WAIC, and unfortunately the AIC gives us no diagnostic criteria to determine when it fails.\n\n\n7M2\nModel selection concerns selecting one model out of a group and discarding the others. Typically one would use the remaining model to make inferences or predictions. However, model comparison involves investigating the differences between multiple models, including comparing the criterion values and comparing the estimates and predictions. When we choose to do selection rather than comparison, we lose all of the information encoded in the differences between the model – knowing which differences in models change criterion values and estimates is crucial information which can inform our understanding of the system.\n\n\n7M3\nWe need to fit models to the exact same set of data points in order to compare them based on WAIC because changing the data points will change the WAIC even if nothing about the model changes. If we were to use the same number of data points, but different sets of data, the WAIC will fluctuate based on properties of the data, and we could get different results when we compare the same models.\nThe penalty parameter of the WAIC also depends on the value of \\(N\\), the number of data points. If we were to drop data points (for example, due to missing data in some models, but not others), we would expect the WAIC to increase (become worse) because we have less information relative to the complexity of the models. Conversely, if we increased the number of data points the WAIC could be better just because of that. We could then make an incorrect decision by comparing the WAICs from models fit on different data.\n\n\n7M4\nAs the width of the priors decreases (i.e. the priors become more concentrated), the WAIC penalty term shrinks. The WAIC penalty term is based on the variances of individual probability estimates across samples. As the width of a prior is narrowed, the model will tend to produce samples for each individual that are closer together on average and thus the penalty term will decrease. However, since the lppd also changes when we change the priors we cannot say for sure whether this increases or decreases the overall WAIC.\n\n\n7M5\nWhen we use informative priors, we make the model more skeptical of extreme values in the data, and less trustworthy of values that would pull the model away from the priors. The data thus need to contain a large amount of evidence to make extreme values more likely in the posterior distribution. Under these conditions, the model is “excited” less by the training sample – and thus, the model fitting process is more robust to variations in the training sample due to sample error. Ideally, the model will capture less of the noise in the data while still capturing strong underlying trends, improving the performance of the model at explaining novel data from the same data generating process.\n\n\n7M6\nOverly informative priors, or in the worst case, degenerate priors, will dominate the model and prevent the model from learning from the data. If a prior is too narrow, the data cannot provide enough evidence to move the model away from the priors. Such a model is so skeptical of the data that it does not pick up the noise from sampling variability in the data, nor does it pick up any signal from the underlying trends either. Because the model has learned nothing from the data, we could make predictions just as good by making up random numbers.\n\n\n7H1\nFor this exercise, we want to fit a curve to the Laffer data. First let’s load and plot the data. Note that I’ve gone ahead and standardized the independent variable, tax rate.\n\ndata(Laffer)\nd &lt;- list(\n    x = standardize(Laffer$tax_rate),\n    y = Laffer$tax_revenue\n)\n\nplot(\n    d,\n    xlab = \"Standardized tax rate\",\n    ylab = \"Tax revenue\"\n)\n\n\n\n\n\n\n\n\nFor this exercise, I’ll fit three models to the data: a regular linear model, a quadratic polynomial model, and a cubic polynomial model. For a real research question, I would also probably consider a spline model, but that is too much work for no payoff here. I messed around for a bit and tried to get a model like the curve from the image in the book, but that is laughably wrong and impossible to fit, so it didn’t work out.\n\nset.seed(123123)\nm1 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x,\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\nm2 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x + b2 * I(x ^ 2),\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        b2 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\nm3 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x + b2 * I(x ^ 2) + b3 * I(x ^ 3),\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        b2 ~ dnorm(0, 2),\n        b3 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\n\nThe first thing we probably want to do is plot the predictions of these models.\n\nx_m &lt;- attr(d$x, \"scaled:center\")\nx_s &lt;- attr(d$x, \"scaled:scale\")\nx_vec &lt;- list(x = seq(min(d$x), max(d$x), length.out = 500))\nx_tf &lt;- x_vec$x * x_s + x_m\n\nplot(\n    x = d$x * x_s + x_m,\n    y = d$y,\n    xlab = \"Tax rate\",\n    ylab = \"Tax revenue\"\n)\n\nm1_post &lt;- rethinking::link(m1, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m1_post,\n    lty = 1, col = \"black\",\n    lwd = 2\n)\n\nm2_post &lt;- rethinking::link(m2, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m2_post,\n    lty = 2, col = \"blue\",\n    lwd = 2\n)\n\nm3_post &lt;- rethinking::link(m3, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m3_post,\n    lty = 3, col = \"red\",\n    lwd = 2\n)\n\nlegend(\n    x = \"topleft\",\n    legend = c(\"Linear\", \"Quadratic\", \"Cubic\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = c(1, 2, 3),\n    lwd = c(2, 2, 2)\n)\n\n\n\n\n\n\n\n\nOK, so we can first of all see that none of those models are even anywhere close to what that editorial showed. None of them really get pulled towards the point with the high tax revenue, but we’ll check that better in the next question. The next thing that we need to do is compare the models. First let’s check the WAIC.\n\nrethinking::compare(m1, m2, m3)\n\n       WAIC       SE      dWAIC      dSE    pWAIC    weight\nm2 124.6255 24.81799 0.00000000       NA 7.640306 0.4111828\nm1 124.6665 22.90424 0.04095811 2.739979 6.513822 0.4028478\nm3 126.2125 24.78288 1.58691188 1.341284 8.774990 0.1859694\n\n\nAll of the models appear to perform about the same. So even though the linear model had technically the lowest WAIC, it’s probably preferable to use that model since the gains in performance are smaller than the standard errors of the WAIC estimates. Let’s check PSIS as well.\n\nrethinking::compare(m1, m2, m3, func = \"PSIS\")\n\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\n\n\n       PSIS       SE     dPSIS      dSE     pPSIS       weight\nm2 128.4209 28.46723  0.000000       NA  9.518310 0.6014412426\nm1 129.2479 27.64550  0.826972  2.08552  8.797377 0.3977576351\nm3 141.6631 38.33021 13.242141 10.10142 16.458178 0.0008011224\n\n\nWe’ll address the issues with the high Pareto \\(k\\) values in the next question. For now we can see that model 1, the linear model, has the lowest PSIS, although again we can see that the standard errors are much higher than the differences in performance. But since we have few data points, adopting the simpler model will likely be a better choice and it seems that there is some linear relationship between tax revenue and tax rate. Let’s look at the model summary for m1.\n\nsummary(m1)\n\n          mean        sd       5.5%    94.5%\nb0    3.228658 0.3069237 2.73813453 3.719181\nb1    0.566714 0.3116142 0.06869426 1.064734\nsigma 1.669183 0.2150402 1.32550725 2.012859\n\n\nWe see that, on average, we would expect for a 1 standard deviation increase in the tax rate, we would expect the tax revenue to increase by 1 unit, whatever those units are (I didn’t look them up). Almost all of the probability mass is above zero, so even if the quantitative effect value is somewhat larger or stronger, we can be reasonably confident that increasing the tax rate will increase the tax revenue by some amount.\n\n\n7H2\nThis question tells us that there is an outlier, but based on the previous Pareto \\(k\\) values that I already glanced at (not all listed out here), that seems to be correct. Point #12 is identified as incredibly influential in all three models (along with point 1 for the quadratic model, giving us another reason to trust that model less than the linear model regardless of what the WAIC tells us). Because of the outlier, we’ll refit the linear model using a Student’s \\(t\\) regression and see what happens.\n\nm4 &lt;- rethinking::quap(\n    alist(\n        y ~ dstudent(2, mu, sigma),\n        mu &lt;- b0 + b1 * x,\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\n\nplot(\n    x = d$x * x_s + x_m,\n    y = d$y,\n    xlab = \"Tax rate\",\n    ylab = \"Tax revenue\"\n)\n\nm4_post &lt;- rethinking::link(m4, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m1_post,\n    lty = 1, col = \"black\",\n    lwd = 2\n)\nlines(\n    x = x_tf, y = m4_post,\n    lty = 2, col = \"firebrick3\",\n    lwd = 2\n)\nlegend(\n    x = \"topleft\",\n    legend = c(\"Gaussian\", \"T(2 d.f.)\"),\n    lwd = c(2, 2),\n    lty = c(1, 2),\n    col = c(\"black\", \"firebrick3\")\n)\n\n\n\n\n\n\n\n\nInterestingly, we can see that if we use a Student’s \\(t\\) likelihood which is less susceptible to the influence of outliers, the line is flatter, reflecting the fact that the outlier was dragging the slope upwards. Excluding this outlier, which has an unusually high tax revenue compared to its tax rate, gives us a line that matches the rest of the points better.\nOf course, this change is not too dramatic. Let’s look at the summary.\n\nsummary(m4)\n\n           mean        sd       5.5%    94.5%\nb0    2.8746595 0.1931870 2.56590941 3.183410\nb1    0.3869709 0.2346150 0.01201094 0.761931\nsigma 0.8011000 0.1689198 0.53113354 1.071067\n\n\nThe change in the coefficient is within the standard error of that coefficient from the first model, so as long as we don’t fall victim to pointeffectism ( AKA point-estimate-is-the-effect syndrome), this doesn’t change our conclusions really at all. This is because while that point is an outlier, it is not incredibly influential. I.e., in standard frequentist linear regression theory, the leverage of that point would not be exceptionally high, indicating that it does not have that much power to influence the coefficient estimates on its own. An outlier with a more extreme \\(x\\) value would influence the estimates more.\n\n\n7H3\nThe first thing we need to do for this problem is type in the data.\n\nbirds &lt;- matrix(\n    c(0.2, 0.8, 0.05, 0.2, 0.1, 0.15, 0.2, 0.05, 0.7, 0.2, 0.025, 0.05,\n        0.2, 0.025, 0.05),\n    ncol = 5,\n    nrow = 3,\n    dimnames = list(\n        paste(\"Island\", 1:3),\n        paste(\"Species\", LETTERS[1:5])\n    )\n)\nbirds\n\n         Species A Species B Species C Species D Species E\nIsland 1      0.20      0.20      0.20     0.200     0.200\nIsland 2      0.80      0.10      0.05     0.025     0.025\nIsland 3      0.05      0.15      0.70     0.050     0.050\n\n\nNext, we want to compute the entropy of each island’s bird distribution.\n\nh &lt;- apply(birds, 1, \\(p) -sum(p * log(p)))\nround(h, 2)\n\nIsland 1 Island 2 Island 3 \n    1.61     0.74     0.98 \n\n\nIsland 1 has the highest entropy, followed by Island 3, and finally Island 2. This is because Island 1 has an even distribution of birds, which gives us the maximum possible entropy of an island with five bird species. We can think of this intuitively as a measurement of our uncertainty in which type of bird we will see. If we see a random bird on Island 1, there is an equally likely chance for it to be any of the species, so any guess we make should be quite uncertain.\nHowever, on Island 2 one scecies represents 80% of birds on the island, and similar on Island 3, one species represents 70% of birds on the island. So if we saw a random bird on Island 3, we could make a decent guess about the species, and an even stronger guess on Island 2. So the decreasing entropies reflect our lowered uncertainty about the type of bird we guess we might see.\nNext we want to compute the KL divergence between each pair of islands.\n\nkl &lt;- function(p, q) {return(sum(p * (log(p) - log(q))))}\n\npairs &lt;-\n    tidyr::expand_grid(p = 1:3, q = 1:3) |&gt;\n    dplyr::filter(p != q) |&gt;\n    dplyr::mutate(\n        div = purrr::map2_dbl(p, q, \\(x, y) kl(birds[x, ], birds[y, ]))\n    )\n\npairs\n\n# A tibble: 6 × 3\n      p     q   div\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2 0.970\n2     1     3 0.639\n3     2     1 0.866\n4     2     3 2.01 \n5     3     1 0.626\n6     3     2 1.84 \n\n\nThese are the KL divergence values if we used the distribution of island \\(p\\) to predict the distribution of island \\(q\\). We see that it’s very difficult to predict island 2 from island 3 and vice versa. Predictions using Island 1 are always much better, regardless of the direction. However, it is much easier to predict Island 3 from Island 1 and vice versa, then trying to predict Island 1 using Island 2 (or vice versa). This is because Island 1 has the maximum entropy, so reflects the maximum amount of uncertainty in our predictions – therefore using Island 1 is the safest way to make a prediction in any other island, because we are expressing the highest amount of uncertainty that we can in this situation.\nIsland 3 and 1 are more compatible in predictions than Island 2 and 1 because the difference in entropy between Islands 1 and 3 is smaller than between Islands 1 and 2. Islands 2 and 3 are almost completely dominated by 2 bird species each, but the dominant species are different between the islands, so using one to predict the other is very wrong. That is, we are using one distribution with extreme values to predict another distribution with different extreme values, so our predictions are more wrong on average.\n\n\n7H4\nOK, the first thing we need to do is type in all that code from the book to recreate the models, so I’ll do that without comments.\n\nd &lt;- sim_happiness(seed = 1977, N_years = 1000)\nd2 &lt;- d[d$age &gt; 17, ]\nd2$A &lt;- (d2$age - 18) / (65 - 18)\nd2$mid &lt;- d2$married + 1\n\nm6.9 &lt;- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu &lt;- a[mid] + bA * A,\n        a[mid] ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = d2\n)\n\nm6.10 &lt;- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu &lt;- a + bA * A,\n        a ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = d2\n)\n\nSince McElreath says that WAIC and PSIS produce identical results here, we’ll go ahead and use WAIC since it’s like one second faster.\n\nrethinking::compare(m6.9, m6.10)\n\n          WAIC       SE    dWAIC      dSE    pWAIC       weight\nm6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00\nm6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85\n\n\nWe can see that the WAIC for m6.9 is much lower than the WAIC for m6.10, even if we consider the magnitude of the standard errors – the WAICs are multiple standard errors apart, so m6.9 should do a better job at generating predictions than m6.10. Let’s look at the model parameters.\n\nprecis(m6.9, depth = 2)\n\n            mean         sd       5.5%      94.5%\na[1]  -0.2350877 0.06348986 -0.3365568 -0.1336186\na[2]   1.2585517 0.08495989  1.1227694  1.3943340\nbA    -0.7490274 0.11320112 -0.9299447 -0.5681102\nsigma  0.9897080 0.02255800  0.9536559  1.0257600\n\nprecis(m6.10)\n\n               mean         sd       5.5%     94.5%\na      1.649248e-07 0.07675015 -0.1226614 0.1226617\nbA    -2.728620e-07 0.13225976 -0.2113769 0.2113764\nsigma  1.213188e+00 0.02766080  1.1689803 1.2573949\n\n\nOf course, we know from the previous chapter than m6.9 is conditioning on a collider! And m6.10 produces the correct causal inference. However, we have to remember the crucial fact that colliders and confounders contain information. As McElreath says earlier in this chapter, “highly confounded models can still make good predictions, at least in the short term.” So the model that makes the wrong causal conclusion has better predictive accuracy than the correct model, but this should not surprise us too much – this is why building a causal model is so important.\n\n\n7H5\nFor this exercise, we’ll go back to the foxes data. We have five models that we need to fit, using the fox weight as the outcome. I’ll go ahead and fit those models in the order indicated in the question. I did the same data processing as in the previous chapter, taking the log of the outcome and then standardizing all of the variables.\n\nset.seed(987865)\ndata(foxes)\nf2 &lt;-\n    foxes |&gt;\n    dplyr::transmute(\n        A = area,\n        F = avgfood,\n        G = groupsize,\n        W = log(weight)\n    ) |&gt;\n    as.list() |&gt;\n    lapply(FUN = rethinking::standardize)\n\nmf.1 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F + bG * G + bA * A,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.2 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F + bG * G,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.3 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bG * G + bA * A,\n        b0 ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.4 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.5 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bA * A,\n        b0 ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nrethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5)\n\n         WAIC       SE    dWAIC      dSE    pWAIC      weight\nmf.3 324.7503 17.91089 0.000000       NA 4.320544 0.481904455\nmf.1 325.8046 19.33267 1.054305 3.831498 6.108581 0.284460254\nmf.2 326.2658 19.75153 1.515478 7.641027 4.583343 0.225880692\nmf.4 334.3019 16.42091 9.551600 7.090994 2.946806 0.004063099\nmf.5 334.4937 16.14821 9.743426 6.923563 3.212804 0.003691501\n\n\nOk, so the main thing that I can see is that Model 1, 2, and 3 are all similar, and model 4 and 5 are similar. But following McElreath’s notion to exam the standard error of the differences, we can see that all the models are actually not too different. However, we can see that model 1 and model 3 are quite similar, whereas the others have larger standard errors.\nIf we look at the plot we can definitely see two groups of models.\n\nplot(rethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5))\n\n\n\n\n\n\n\n\nIf we look back at the DAG, maybe we can understand why there are two groups of models.\n\nfox_dag &lt;-\n    dagitty::dagitty(\n        \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n        }\"\n    )\ndagitty::coordinates(fox_dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\nplot(fox_dag)\n\n\n\n\n\n\n\n\nSo the groups that are similar are: 1. (avgfood, groupsize, area) and (avgfood, groupsize) and (groupsize, area); 2. (avgfood only) and (area only).\nSo the main difference between the models is the inclusion of the groupsize variable, \\(G\\) in the DAG. We can see that \\(G\\) is a mediator of the relatioship between \\(F\\) and \\(W\\) here.\nWhen we fit only avgfood or only area, it makes sense that the two models are the same – the effect of area is entirely through its effect on avgfood, so those two models are essentially finding the same effect.\nHowever, when we include the mediator we get slightly different results. Interestingly, we’re still only capturing one causal effect, because the effect of everything else comes from area. But it seems that the predictive model is better when we control for groupsize – why would this be, if we are still only capturing descendants of area? I think that this is because, as we also know, conditioning on a variable which is the ancestor of the outcome can lead to increased precision in our estimates. So even though we aren’t getting any “new” signal here, and all of the models with groupsize in them perform similarly, we get slightly more efficient estimates, which can increase our predictive accuracy on average. It may also be the case that estimating the effect of the groupsize is a statistical issue – this variable is integer valued, and we’re modeling it like a continuous value, but because we only have a discrete set of actual observations, this may reduce the precision of our estimate, since there’s a lot of “space” in the groupsize-axis that isn’t covered by any measurements.\nWe can also see that model 3 had the best WAIC overall, and it specifically includes the two variables which are direct ancestors of the treatment, so maybe that supports my idea or maybe it is a coincidence, I’m not too sure.\nAnyways, I think this is mostly a statistical phenomenon. There are no confounders or colliders here, everything is just direct or indirect effects of the effect of area on weight."
  },
  {
    "objectID": "sr/week2.html",
    "href": "sr/week2.html",
    "title": "2023 Homework, week 2",
    "section": "",
    "text": "This homework covers the material from Lectures 3 and 4, and the content from book Chapter 4. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\n\n\n\n\n\n\n1. From the Howell1 dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume that age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions.\nDraw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.\n\n\n\nOK, I will assume that the first paragraph is just an introduction to the homework set and the actual task for this question is in the second paragraph. Here is the DAG for this problem.\n\n# Specify the relationships in the DAG\ndag &lt;-\n    dagitty::dagitty(\n        \"dag {\n            age -&gt; height -&gt; weight\n            age -&gt; weight\n        }\"\n    )\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(dag) &lt;-\n    list(\n        x = c(age = 1, height = 2, weight = 2),\n        y = c(age = 2, height = 3, weight = 1)\n    )\nplot(dag)\n\n\n\n\n\n\n\n\nNow we can write a generative simulation. First, let’s look at the pairwise correlations so we can get sort of an idea of the data distributions and the effects we should simulate.\n\nlibrary(rethinking)\ndata(Howell1)\nh1 &lt;-\n    Howell1 |&gt;\n    dplyr::filter(age &lt; 13)\n\npairs(h1[1:3])\n\n\n\n\n\n\n\n\nOk, so with that plot in mind we see that ages are discrete from 0 to 13, height ranges from about 50 units to 150 units, and weight ranges from about 5 units to 35 units. So our generative simulation should stay within those ranges.\n\n# Set the seed so the simulation makes the same numbers every time\nset.seed(101)\n\n# This part does the simulation and puts it into a tibble for storage\nsim &lt;- tibble::tibble(\n    # Just randomly draw an age. In the original data the ages are not\n    # 100% even but I think this is fine.\n    age = sample(0:13, nrow(h1), replace = TRUE),\n    # Height and weight simulations using dag relationships and made up numbers.\n    height = rnorm(nrow(h1), 60 + 5 * age, 10),\n    weight = rnorm(nrow(h1), 3 + 0.1 * age + 0.2 * height, 1)\n)\n\n# Put the columns into the same order for easier comparisons and plot\nsim &lt;- sim[, c(\"height\", \"weight\", \"age\")]\npairs(sim)\n\n\n\n\n\n\n\n\nI just randomly picked these numbers and fiddled with it a bit until the two plots looked similar, and I think I was able to get them pretty close for such a simple simulation using linear effects and normal errors.\n\n\n\n\n\n\n2. Use a linear regression to estimate the total causal effect of each year of growth on weight.\n\n\n\nBased on the DAG, to obtain the total causal effect of a year of growth (the interpretation of the parameter associated with the independent variable age) we want to use age as the only independent variable in the model. If we controlled for height, the parameter would estimate the direct causal effect of age, but we want the total effect. So the basic structure of our model will look like this. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Prior}() \\\\\n\\sigma &\\sim \\text{Prior}()\n\\end{align*}\\]\nWe will need to assign some priors to our data. In general, I tend to prefer weakly informative priors, whereas I think McElreath tends to prefer less broad priors. I’ll base my priors off the default recommended priors from the Stan devs’ prior choice recommendations. Of course they also recommend rescaling all variables before modeling, which I think is a good idea, but I won’t do it here because I’m lazy and I don’t think it’s going to be particularly useful here.\nOne additional constraint that we have is that the \\(\\alpha\\) and \\(\\beta\\) parameters should both be positive! It doesn’t make sense for someone to shrink as they get older (at least not for ages 0 to 13, maybe for seniors but not here). And it certainly doesn’t make sense for someone to ever have a negative weight, even at age zero. So we’ll use a distribution that has to be positive. I’ll choose a half-normal distribution, which is easy to sample by just taking the absolute value of a random normal sample.\n\\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\beta &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\nPerhaps we should do a prior predictive check to visualize them before doing anything else.\n\nset.seed(101)\npps &lt;-\n    tibble::tibble(\n        a = abs(rnorm(1000, 0, 1)),\n        b = abs(rnorm(1000, 0, 1)),\n        s = rexp(1000, 1)\n    )\n\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 40),\n    xlab = \"Age\", ylab = \"Simulated mu\",\n    main = \"Prior predictive simulation of E[weight | age]\"\n)\n\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.1)\n    )\n}\n\n\n\n\n\n\n\n\nWell, some of those are way too flat, and some of them are way too steep, but overall I think this encompasses a good range of possibilities. Let’s also look at the prior predictive distribution of the actual outcomes. Here, I’ll take random samples of age from a discrete uniform distribution. That’s probably not the best way to do it but it seems easiest.\n\n# Do the simulation\nset.seed(102)\nsim_age &lt;- sample(0:13, 1000, replace = TRUE)\nsim_y &lt;- rnorm(\n    1000,\n    mean = pps$a + pps$b * sim_age,\n    sd = pps$s\n)\nlayout(matrix(c(1, 2), nrow = 1))\n\n# Histogram of all y values\nhist(\n    sim_y,\n    xlab = \"Simulated outcome\",\n    main = \"Distribution of simulated weights\",\n    breaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 40),\n    xlab = \"Age\", ylab = \"Simulated weight\",\n    main = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\npoints(sim_age, sim_y)\n\n\n\n\n\n\n\n\nWell, we ended up with a few negative and a few ridiculously large weights, but since we’re just doing a linear regression here I think we can live with that and let the data inform the golem that no one has negative heights. Probably we would want to either change the likelihood function or transform something (e.g. use a log link) to prevent any negative responses, but this will probably wash out in the fitting. So let’s do that.\n\nfit &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a + b * age,\n            a ~ dnorm(0, 1),\n            b ~ dnorm(0, 1),\n            sigma ~ dexp(1)\n        ),\n        constraints = alist(\n            a = \"lower=0\",\n            b = \"lower=0\"\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age\n        )\n    )\n\nHmm. Looks like quap does not take a constraints argument the way I thought it did. So I guess we will just have to settle for exponential priors, which like I mentioned, is probably the easiest way (not the best way) to get a strictly positive prior. Let’s redo the prior predictive simulation using this model. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Exponential}(1) \\\\\n\\beta &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\n\nlayout(matrix(c(1, 2), nrow = 1))\nset.seed(101)\npps &lt;-\n    tibble::tibble(\n        a = rexp(1000, 1),\n        b = rexp(1000, 1),\n        s = rexp(1000, 1)\n    )\n\nset.seed(102)\nsim_age &lt;- sample(0:13, 1000, replace = TRUE)\nsim_y &lt;- rnorm(\n    1000,\n    mean = pps$a + pps$b * sim_age,\n    sd = pps$s\n)\n# Histogram of all y values\nhist(\n    sim_y,\n    xlab = \"Simulated outcome\",\n    main = \"Distribution of simulated weights\",\n    breaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n    NULL,\n    xlim = c(0, 13), ylim = c(-10, 150),\n    xlab = \"Age\", ylab = \"Simulated weight\",\n    main = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n    curve(\n        pps$a[i] + pps$b[i] * x,\n        from = 0, to = 13, n = 1000,\n        add = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\npoints(sim_age, sim_y)\n\n\n\n\n\n\n\n\nYes, this definitely results in a more left-skewed distribution, but I think that is actually good, since we don’t expect a lot of kids to weight 50+ kilograms. So I’m not too pressed about it. Now let’s finally fit the model, for real.\n\nset.seed(101)\nfit &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a + b * age,\n            a ~ dexp(1),\n            b ~ dexp(1),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age\n        )\n    )\n\nrethinking::precis(fit)\n\n          mean         sd     5.5%    94.5%\na     7.332870 0.35961024 6.758143 7.907596\nb     1.354753 0.05438219 1.267840 1.441667\nsigma 2.503612 0.14476058 2.272256 2.734967\n\n\nSo our estimate for the total causal effect of age on weight is 1.35. In other words, we would expect that the average individual is born at weight 7.33 units, and increases in weight by 1.35 units each year.\n\n\n\n\n\n\n3. Now suppose the causal association between age and weight might be different for boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do girls and boys differ? Provide one or more posterior contrasts as a summary.\n\n\n\nSo what we are assuming here is that the effect of age is different for males and females – we’ll also allow the intercept to vary by sex, meaning that males and females can also have a different weight at birth on average. We’ll use a similar model from before other than these changes. \\[\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Sex}_i} + \\beta \\cdot \\text{age}_{\\text{Sex}_i} \\\\\n\\alpha_{j} &\\sim \\text{Exponential}(1) \\\\\n\\beta_{j} &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\\]\nSo now we will fit the model.\n\nset.seed(103)\nfit2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            weight ~ dnorm(mu, sigma),\n            mu &lt;- a[sex] + b[sex] * age,\n            a[sex] ~ dexp(1),\n            b[sex] ~ dexp(1),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            weight = h1$weight,\n            age = h1$age,\n            # We have to add 1 for the index coding to work right\n            sex = h1$male + 1\n        ),\n        start = list(a = c(1, 1), b = c(1, 1), sigma = 0.5)\n    )\n\nrethinking::precis(fit2, depth = 2)\n\n          mean         sd     5.5%    94.5%\na[1]  6.861204 0.47573974 6.100880 7.621528\na[2]  7.664618 0.50386642 6.859343 8.469894\nb[1]  1.312828 0.07258895 1.196816 1.428839\nb[2]  1.414252 0.07545861 1.293655 1.534850\nsigma 2.406891 0.13948954 2.183960 2.629822\n\n\nOk, so I had some issues with the start values here. Probably because the priors are very diffuse, quap sometimes cannot get to the MAP from randomly sampled starting locations. However, I just had to find a seed that works, because it seems to be ignoring the start value for the vector-valued parameters (or I am specifying it incorrectly) in the error messages I get. But this one fit, so let’s go. There appears to be a slight difference between the two slope parameters, but we cannot allow ourselves to be mislead by the table of coefficients. We must compute the contrast distribution to truly understand what is happening here.\nNow, in order to understand the differences, we have to construct the contrast distribution.\n\nN &lt;- 2500\n\nage_vals &lt;- 0:12\nmean_f &lt;- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n                                                     n = N)\nmean_m &lt;- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n                                                     n = N)\n\nmean_contrast &lt;- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(0, 30),\n    xlab = \"Simulated age\",\n    ylab = \"Weight (kg)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_m[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"dodgerblue2\", 0.01)\n    )\n    lines(\n        age_vals, mean_f[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"hotpink1\", 0.01)\n    )\n}\n\nlegend(\"topleft\", c(\"Males\", \"Females\"), col = c(\"dodgerblue2\", \"hotpink1\"),\n             lty = c(1, 1))\n\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(-3, 5),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_contrast[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nHere, we can observe that the regression lines for males tend to be steeper than the regression lines for females. From the contrast, we can see that most of the time, the effect is positive and the line lies above zero. Let’s compute a posterior interval for the mean.\n\nlayout(1)\nplot(\n    NULL,\n    xlim = c(0, 12),\n    ylim = c(-3, 5),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    shade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nFrom the contrast, we can see that men tend to be heavier at nearly all ages. The weight of males and females tends to be closer at birth than on average at older ages.\nThis contrast is between the estimates of the conditional mean response. To incorporate individual variance into the uncertainty around the estimate, we can also use samples from the posterior of the conditional mean, along with samples from the posterior of the scale parameter to simulate individual responses.\n\nN &lt;- 2500\n\nage_vals &lt;- 0:12\nmean_f &lt;- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n                                                     n = N)\nmean_m &lt;- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n                                                     n = N)\n\nmean_contrast &lt;- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(0, 12), ylim = c(-12, 12),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n    lines(\n        age_vals, mean_contrast[i, ],\n        lwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n    )\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\nplot(\n    NULL,\n    xlim = c(0, 12),\n    ylim = c(-12, 12),\n    xlab = \"Simulated age\",\n    ylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n    shade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\n\n\n\n\n\n\n\nInterestingly, the variance tends to be quite large. However, as we see from the plot on the left, we could probably use a better variance structure for this model. If each line represents an individual, the spikes over time are unlikely to be this drastic, especially for children. We still see (from the right plot) that a given male is more likely to be heavier than a given female of the same age, but there is a lot of variation between individuals so this will not always be true.\n\n\n\n\n\n\n4. The data in data(Oxboys) are growth records for 26 boys measured over 9 periods. I want you to model their growth. Specifically, model the increments in growth from one Occassion to the next. Each increment is simply the difference between height in one occasion and height in the previous occasion. Since none of these boys shrunk during the study, all of the growth increments are greater than zero. Estimate the posterior distribution of these increments. Constrain the distribution so it is always positive – it should not be possible for the model to think that boys can shrink from year to year. Finally computer the posterior distribution of the total growth over all 9 occasions.\n\n\n\n\ndata(Oxboys)\ndplyr::glimpse(Oxboys)\n\nRows: 234\nColumns: 4\n$ Subject  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3…\n$ age      &lt;dbl&gt; -1.0000, -0.7479, -0.4630, -0.1643, -0.0027, 0.2466, 0.5562, …\n$ height   &lt;dbl&gt; 140.5, 143.4, 144.8, 147.1, 147.7, 150.2, 151.7, 153.3, 155.8…\n$ Occasion &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3…\n\n\nFrom the data documentation, we see that age and Occasion basically represent the same thing, so I’ll ignore age for now. I think for this model, we don’t even need to draw a DAG as there are only two variables we really care about.\nThe first thing I’ll do is the necessary data processing. We want to get the increments in growth by subtracting the previous measurement from each height value. This will give us the amount of growth in between each measurement. Note that this will reduce us from 9 total measurements per boy to 8 total differences per boy, because there is nothing to subtract from the first measurement.\n\no2 &lt;- Oxboys |&gt;\n    dplyr::group_by(Subject) |&gt;\n    dplyr::mutate(prev = dplyr::lag(height)) |&gt;\n    dplyr::mutate(\n        increment = height - prev,\n        period = Occasion - 1\n    ) |&gt;\n    dplyr::filter(\n        Occasion != 1\n    )\n\no2_l &lt;- list(\n    increment = o2$increment,\n    period = o2$period\n)\n\nSo let’s plot the distribution of each increment.\n\nset.seed(100)\nlayout(cbind(\n    matrix(c(1:8), nrow = 2, ncol = 4, byrow = TRUE)\n))\n\n# Histograms of each increment\nfor (i in 1:8) {\n    dat &lt;- subset(o2, period == i)\n    hist(\n        dat$increment,\n        breaks = seq(0, 5, 0.5),\n        xlab = NULL,\n        main = paste(\"Increment\", i),\n        cex.lab = 1.25,\n        cex.axis = 1.25\n    )\n}\n\n\n\n\n\n\n\nlayout(1)\n# Boxplot for each increment\nboxplot(\n    increment ~ period,\n    data = o2_l,\n    cex.lab = 1.75,\n    cex.axis = 1.75,\n    xlab = \"Period\", ylab = \"Increment\"\n)\nstripchart(\n    increment ~ period,\n    data = o2_l,\n    method = \"jitter\",\n    vertical = TRUE,\n    add = TRUE,\n    pch = 21,\n    col = rethinking::col.alpha(\"red\", 0.5),\n    cex = 1.25\n)\n\n\n\n\n\n\n\n\nThere is definitely some variance in each increment, so I think it makes sense to use a model that looks like this \\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\ninstead of a model that looks like this\n\\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta_i \\cdot \\text{Period}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\n\nMcElreath also chose to constrain the alpha parameters to all be the same, which I think is fine and makes the model much simpler. But since we were interested in the posterior distribution of their sum anyways, I prefer to leave the model like this anyways. Besides, from the histogram we saw that some of them were different – I’m not sure if this is a good biological assumption or not though since I don’t really know enough about this problem.\n\nwhere increment is the difference, and period is the same as occasion minus 1. We still need to choose \\(k\\). Since we have a lognormal distribution, the variance parameter should be biased to be small (untuitively we do this by making the exponential parameter larger) or else when we do the log part, the means will get really big. This is also true for \\(k\\), which I decided to simulate instead of just picking to be big.\n\nNote that I originally used a normal regression with a lognormal prior, but I read McElreath’s solution and decided to update this to use a lognormal regression with a normal prior, which I think is actually much better. If you use a Gaussian model for this, you run the risk of randomly getting growth increments from the model that are negative, which we specifically wanted to avoid in this problem.\n\nSo the increment over the first period is the difference between the second height measurement and the first height measurement. (I also think that this is what the problem is asking for.)\nIf we use period as an index variable, we could also include an “overall” intercept, but we don’t really need to (the models are computationally equivalent parametrizations). Sometimes adding the overall intercept makes the model run better, or so I’ve heard, but I’ll ignore it for now.\nI choose a standard exponential prior on sigma, but for each alpha I choose a lognormal prior. This also constrains the effect of each increment to be strictly positive, but prefers a moderate effect rather than having most of the weight near zero. In order to choose the variance constant \\(k\\) that I wrote in above as a placeholder, I think it might be best to do a small prior predictive simulation. I’ll just simulate one increment, since the model will allow them to be different regardless.\nI probably should have done this before looking at the data, but I think for now it will be OK. We want to choose a diffuse prior, but not one that produces insane results.\n\nset.seed(2323)\nk &lt;- c(0.05, 0.1, 0.25, 0.5, 1, 1.5) # Values to try\nN &lt;- 1000 # Number of simulations\n\n# Create a container for the output\npps &lt;- matrix(nrow = N, ncol = length(k))\n\n# Do the simulation\nfor (i in 1:length(k)) {\n    pps[, i] &lt;- rlnorm(N, rnorm(N, 0, k[i]), rexp(N, 10))\n    \n}\n\n# Make a plot\nlayout(matrix(1:length(k), nrow = 2, byrow = TRUE))\nfor (i in 1:length(k)) {\n    hist(pps[, i], breaks = \"FD\", main = paste(\"k =\", k[i]), xlab = NULL)\n} \n\n\n\n\n\n\n\n\nWow, I forgot how hard it is to choose parameters for a lognormal prior. That scaling parameter really starts to act up quickly. So for this problem, I’ll choose \\(k=0.5\\) since it creates a prior that, in general, constrains our values to what seems “reasonable” but also allows the parameter to get large if it needs to, due to the tail of the distribution.\nSo to recap before we fit, our final model will be this. \\[\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Lognormal}(0, 0.5) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\\]\n\nset.seed(105)\nfit_ob &lt;-\n    rethinking::quap(\n        flist = alist(\n            increment ~ dlnorm(mu, sigma),\n            mu &lt;- a[period],\n            a[period] ~ dnorm(0, 0.5),\n            sigma ~ dexp(10)\n        ),\n        data = o2_l,\n        control = list(maxit = 500)\n    )\nrethinking::precis(fit_ob, depth = 2)\n\n            mean         sd        5.5%     94.5%\na[1]   0.3978845 0.10740207  0.22623527 0.5695338\na[2]   0.2322873 0.10739249  0.06065337 0.4039213\na[3]   0.5005341 0.10741057  0.32887130 0.6721970\na[4]  -0.1303368 0.10738912 -0.30196531 0.0412918\na[5]   0.1171602 0.10738882 -0.05446786 0.2887883\na[6]   0.6545573 0.10742686  0.48286841 0.8262461\na[7]   0.5295969 0.10741329  0.35792970 0.7012641\na[8]   0.3111911 0.10739646  0.13955079 0.4828314\nsigma  0.5606550 0.02697832  0.51753842 0.6037716\n\n\nOkay, so we can see that all of the parameters are in the same sort of neighborhood, but they definitely appear to be somewhat different. However! Recall that we cannot allow ourselves to stare into the void and be mislead. We shall first visualize the posterior distributions of these parameters.\n\nset.seed(1344134)\npost &lt;- extract.samples(fit_ob)\nmn &lt;- apply(post$a, 2, mean)\npi &lt;- apply(post$a, 2, rethinking::PI)\n\nlayout(matrix(1:8, nrow = 2, byrow = TRUE))\nfor (i in 1:ncol(post$a)) {\n    # Blank plot\n    plot(\n        NULL,\n        xlim = c(-1.5, 1.5), ylim = c(0, 4.5),\n        xlab = paste(\"Increment\", i, \"effect (cm)\"),\n        ylab = \"Density\",\n        xaxs = \"i\", yaxs = \"i\",\n        cex.lab = 1.5,\n        cex.axis = 1.5\n    )\n    \n    # Mean and 89% PI\n    abline(v = mn[i], col = \"red\", lty = 2, lwd = 2)\n    abline(v = pi[1, i], col = \"red\", lty = 2)\n    abline(v = pi[2, i], col = \"red\", lty = 2)\n    \n    # Density curve\n    rethinking::dens(post$a[, i], add = TRUE, lwd = 3)\n}\n\n# Common title\nmtext(\n    paste(\n        \"Posterior distributions of individual increment effects\",\n        \"(mean and 89% posterior interval)\",\n        sep = \"\\n\"\n    ),\n    side = 3,\n    line = -3,\n    outer = TRUE,\n    cex = 1.1\n    )\n\n\n\n\n\n\n\n\nAgain, we must be careful not to make any undue comparisons here. These summaries are fine for thinking about individual effects, but it does not make sense to compare them across parameters. So now we shall compute the contrast of interest. We can get the posterior of the total growth by creating a contrast across each of the growth increments. Of course this contrast will look like \\(c^{\\mkern-1.5mu\\mathsf{T}} = \\langle 1, 1, \\ldots, 1 \\rangle\\) because we will just be adding up the samples.\n\nlayout(1)\n\n# Sample the growth increments from the posterior\n# and then add them up\ncontrast &lt;- sapply(\n    1:1000,\n    \\(i) sum(rlnorm(8, post$a[i, ], post$sigma[i]))\n)\ncontrast_mean &lt;- mean(contrast)\n\nplot(\n    NULL,\n    xlim = c(0, 30),\n    ylim = c(0, 0.18),\n    xlab = \"Sum of growth increments (cm)\",\n    ylab = \"Density\",\n    xaxs = \"i\", yaxs = \"i\",\n    cex.axis = 1.25,\n    cex.lab = 1.1,\n    tcl = -0.25\n)\n\nplot_dens &lt;- density(contrast, adjust = 0.5)\n\nfor (i in c(67, 89, 95, 99) / 100) {\n    shade(plot_dens, PI(contrast, prob = i))\n}\n\nlines(\n    x = rep(contrast_mean, 2),\n    y = c(0, plot_dens$y[which.min(abs(plot_dens$x - contrast_mean))]),\n    lty = 2, lwd = 2\n)\nrethinking::dens(contrast, lwd = 3, add = TRUE)\n\n\n\n\n\n\n\n\nHere the dashed line shows the posterior mean. The dashed line shows the posterior mean, and the shaded regions show equal-tailed credible intervals. From darkest to lightest, the coverage for the CIs are, respectively: 67%, 89%, 95%, 99%.\nI think we should also probably do this and include the individual-level variance (the estimated sigma parameter), but for right now I’ll call it a day. We can see that the mean total growth was most likely to be 13 cm over the nine occasions, although a range of values from around 12 to 14 were also quite plausible.\n\nThis looks quite similar to McElreath’s solution where the alpha for each increment is required to be the same, with one major difference. The slope of the density is much more gradual to the left of the mean in my estimated distribution. This could be sampling variation, or it could be that one (or more) of the increments is typically smaller than the others in this population, and drags down the overall average parameter value when they are constrained to be the same."
  },
  {
    "objectID": "sr/week4.html",
    "href": "sr/week4.html",
    "title": "2023 Homework, week 4",
    "section": "",
    "text": "This homework covers the material from Lectures 7 and 8, and the content from book Chapters 7, 8, and 9. The questions are reproduced almost identically from Richard McElreath’s original assignment, I did not write them. I only wrote these solutions.\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1.9000\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\n\n\n\n\n\n\n1. Revisit the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again (pages 178-179). Compare these two models using both PSIS and WAIC. Which model is expected to make better predictions, according to these criteria, and which model yields the correct causal inference?\n\n\n\nOK, first we will fit the models. Since these are exactly the same as in the book, the results will be similar and I won’t spend a lot of time on them.\n\n# Data setup\nd &lt;- sim_happiness(seed = 1977, N_years = 1000)\nd2 &lt;- d[d$age &gt; 17, ]\nd2$A &lt;- (d2$age - 18) / (65 - 18)\nd2$M &lt;- d2$married + 1\n\nset.seed(134123)\n\n# First model: \nm6.9 &lt;-\n    quap(\n        alist(\n            happiness ~ dnorm(mu, sigma),\n            mu &lt;- a[M] + bA * A,\n            a[M] ~ dnorm(0, 1),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = d2\n    )\n\n# Second model\nm6.10 &lt;-\n    quap(\n        alist(\n            happiness ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 1),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = d2\n    )\n\nOK, now we want to score the models by PSIS and WAIC. Let’s do PSIS first.\n\nPSIS(m6.9)\n\n      PSIS      lppd  penalty  std_err\n1 2713.834 -1356.917 3.687653 37.43357\n\nPSIS(m6.10)\n\n      PSIS      lppd  penalty  std_err\n1 3101.878 -1550.939 2.321511 27.82064\n\n\nAnd now the WAIC.\n\nWAIC(m6.9)\n\n      WAIC      lppd  penalty  std_err\n1 2713.726 -1353.256 3.606974 37.38137\n\nWAIC(m6.10)\n\n     WAIC      lppd penalty  std_err\n1 3102.34 -1548.622 2.54812 27.77344\n\n\nOK, so for this model they are basically the same. But either way, we see that model m6.9, which stratifies by marriage, is better at prediction! However, we know that m6.10 actually makes the correct causal inference. This should not surprised us, because colliders contain information – even if they distort causal estimates, including them will often work better for prediction.\n\n\n\n\n\n\n2. Reconsider the urban fox analysis from last week’s homework. On the basis of PSIS and WAIC scores, which combination of variables best predicts body weight? What causal interpretation can you assign each coefficient from the best scoring model?\n\n\n\nFirst I’ll fit the two models that we were using at the end of last week’s homework – the model for the direct effect of \\(F\\) and the model for the total effect of \\(F\\) (which we recall that we could not accurately estimate due to an unmeasured confounder). We’ll also add an additional “kitchen sink” model that includes age (even though it is a precision parasite), just because we also have that. There are seven different models that we could choose, but I think these three will probably be sufficient.\n\n# Set up data\ndata(foxes)\nD &lt;-\n    foxes |&gt;\n    dplyr::select(\n        F = avgfood,\n        A = area,\n        W = weight,\n        G = groupsize\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(dplyr::everything(), standardize)\n    ) |&gt;\n    as.list()\n\n# Fit the two models\nset.seed(193482)\nkitchen_sink &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F + bG * G + bA * A,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            bG ~ dnorm(0, 2),\n            bA ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\nf_direct &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F + bG * G,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            bG ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\nf_total &lt;-\n    rethinking::quap(\n        flist = alist(\n            W ~ dnorm(mu, sigma),\n            mu &lt;- a + bF * F,\n            a ~ dnorm(0, 2),\n            bF ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = D,\n        control = list(maxit = 500)\n    )\n\ncoeftab(kitchen_sink, f_direct, f_total) |&gt;\n    coeftab_plot(pars = c(\"bA\", \"bF\", \"bG\"))\n\n\n\n\n\n\n\n\nOK, now we can calculate the PSIS and WAIC for all of these.\n\nsapply(list(kitchen_sink, f_direct, f_total), PSIS) |&gt;\n    `colnames&lt;-`(c(\"A, G, F\", \"F, G\", \"F\"))\n\n        A, G, F   F, G      F        \nPSIS    324.0635  324.3235  333.7011 \nlppd    -162.0317 -162.1617 -166.8506\npenalty 5.523674  4.297541  2.555102 \nstd_err 17.02429  16.84845  13.91888 \n\nsapply(list(kitchen_sink, f_direct, f_total), WAIC) |&gt;\n    `colnames&lt;-`(c(\"A, G, F\", \"F, G\", \"F\"))\n\n        A, G, F   F, G      F       \nWAIC    323.9801  324.5721  334.1936\nlppd    -156.4935 -157.8467 -164.32 \npenalty 5.496565  4.43933   2.776743\nstd_err 16.89324  17.05316  13.91089\n\n\nHere we get a little bit of a competition, but not much of one. We can see that the “kitchen sink” model wins by less than a point for PSIS, but the direct causal model wins by less than a point for WAIC. That’s obviously due to approximation error, it doesn’t matter which one wins by just a few points. Clearly we can see that the model for the direct causal effect beats out the model for the total causal effect, which makes sense because both of these variables encode unique information about the outcome (from our causal model, \\(A\\) does not).\nFor the best model, we can infer the direct causal effects of \\(F\\) and \\(G\\), but not the total causal effect of \\(F\\).\n\n\n\n\n\n\n3. Build a predictive model of the relationship shown on the cover of the book, the relationship between the timing of cherry blossoms and March temperature in the same year. The data are found in data(cherry_blossoms). Consider at least two different models (functional relationships) to predict doy with temp. Compare them with PSIS or WAIC.\nSuppose March temperatures reach 9 degrees by the year 2050. What does your best model predict for the predictive distribution of the day-in-year that the cherry trees will blossom?\n\n\n\nOK, I feel like I kind of already did this one in the textbook work I’ve done, but let’s go through it as a fun exercise anyways. First we need to process the data, which I’ll do the same way he did in the book.\n\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy, d$temp), ]\n\n# Also standardize the data\nd3 &lt;-\n    list(\n        doy = standardize(d2$doy),\n        temp = standardize(d2$temp)\n    )\n\nI’ll compare the null model, a linear model, a quadratic model, and a spline-based model similar to the one from the book. I’ll just use kind of random priors on this without a lot of explanation, just fiddling until the models seems to work correctly. Then I’ll compare the models.\n\n# Null model (intercept only)\nq3_m1 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a,\n            a ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Linear model\nq3_m2 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b * temp,\n            a ~ dnorm(0, 3),\n            b ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Quadratic model\nq3_m3 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b1 * temp + b2 * (temp ^ 2),\n            a ~ dnorm(0, 3),\n            b1 ~ dnorm(0, 3),\n            b2 ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = d3\n    )\n\n# Power law model\nq3_m4 &lt;-\n    quap(\n        alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + b * temp,\n            a ~ dnorm(0, 3),\n            b ~ dnorm(0, 3),\n            sigma ~ dexp(1)\n        ),\n        data = list(\n            doy = standardize(log(d2$doy)),\n            temp = standardize(log(d2$temp))\n        )\n    )\n\n# Spline model\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d3$temp, probs = seq(0, 1, length.out = nk))\nB &lt;-\n    splines::bs(\n        x = d3$temp,\n        knots = knot_list[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\nq3_m5 &lt;-\n    quap(\n        flist = alist(\n            doy ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(doy = d3$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\nAll the models fit fine so I assume my randomly guessed priors were OK. First we’ll compare with WAIC.\n\ncompare(q3_m1, q3_m2, q3_m3, q3_m4, q3_m5)\n\n          WAIC       SE     dWAIC        dSE     pWAIC       weight\nq3_m2 2149.026 40.95329  0.000000         NA  2.721421 6.209027e-01\nq3_m3 2151.255 41.03282  2.228806  0.2597071  3.724725 2.037251e-01\nq3_m4 2151.557 42.18830  2.531068  5.5127294  3.057807 1.751496e-01\nq3_m5 2164.893 41.43704 15.866859  8.0200012 17.965816 2.226276e-04\nq3_m1 2236.671 39.77412 87.644915 16.8970472  2.121330 5.769951e-20\n\n\nAnd now with PSIS.\n\ncompare(\n    q3_m1, q3_m2, q3_m3, q3_m4, q3_m5,\n    sort = \"PSIS\",\n    func = PSIS\n)\n\nSome Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points.\n\n\n          PSIS       SE     dPSIS        dSE     pPSIS       weight\nq3_m2 2149.271 40.84579  0.000000         NA  2.841805 5.643948e-01\nq3_m3 2150.928 41.03927  1.657495  0.2972671  3.559165 2.464124e-01\nq3_m4 2151.458 42.14878  2.187364  5.5039158  3.004142 1.890615e-01\nq3_m5 2166.003 41.54788 16.731654  8.1282313 18.531424 1.313259e-04\nq3_m1 2236.743 39.70714 87.471828 16.8454321  2.157284 5.718959e-20\n\n\nOK, so both the WAIC the the PSIS comparison give us the same result. The linear model is actually the best fit, jujst by a hair. If we take the SE into account, the linear, quadratic, and power law models all seem to fit pretty decent. The spline model is a bit worse, within one SE, and all of the “real” models are better than the null model. My guess is that while the spline model is flexible, it doesn’t gain enough predictive power from this flexibility in order to be “worth” the large number of parameters needed.\nNow let’s plot the results to get a better idea of what we’re looking at.\n\npost &lt;- sim(q3_m2)\nout &lt;- colMeans(post * sd(d2$doy) + mean(d2$doy))\nplot(\n    d2$doy, out,\n    xlab = \"Actual DoY\", ylab = \"Predicted DoY\"\n)\n\n\n\n\n\n\n\nplot(\n    d2$year, d2$doy, type = \"l\"\n)\nshade(\n    apply(post, 2, \\(x) rethinking::PI(x* sd(d2$doy) + mean(d2$doy)) ),\n    d2$year,\n    col = rethinking::col.alpha(\"gray\", alpha = 0.75)\n)\nlines(d2$year, out, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\nplot(\n    NULL, xlim = range(d2$year), ylim = c(-20, 20)\n)\nshade(\n    apply(post, 2, \\(x) rethinking::PI(x  * sd(d2$doy) + mean(d2$doy) - d2$doy)),\n    d2$year,\n    col = rethinking::col.alpha(\"gray\", alpha = 0.75)\n)\n\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\nWarning in x * sd(d2$doy) + mean(d2$doy) - d2$doy: longer object length is not\na multiple of shorter object length\n\nlines(d2$year, out - d2$doy, type = \"l\", col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFinally, let’s look at the distribution of predictions for if we set the temperature to 9 degrees.\n\n\n\n\n\n\n4. The data in data(Dinosaurs) are body mass estimates at different estimated ages for six different dinosaur species. Choose one or more of these species and make a predictive model of body mass using age as a predictor. Consider two or more model types for the function relating age to body mass and score each using PSIS and WAIC.\nWhich model do you think is best, on predictive grounds? On scientific grounds? If your answers to these questions differ, why?"
  },
  {
    "objectID": "arm/cp3/index.html",
    "href": "arm/cp3/index.html",
    "title": "Chapter 3: Linear regression: the basics",
    "section": "",
    "text": "Code\nbox::use(\n    arm\n)"
  },
  {
    "objectID": "arm/cp3/index.html#q1",
    "href": "arm/cp3/index.html#q1",
    "title": "Chapter 3: Linear regression: the basics",
    "section": "Q1",
    "text": "Q1\n\n\n\n\n\n\nThe folder pyth contains outcome \\(y\\) and inputs \\(x_1, x_2\\) for \\(40\\) data points, with a further \\(20\\) points with the inputs but no observed outcome. Save the file to your working directory and read it into R using the read.table() function.\n\n\n\nWell, my first problem here was that I downloaded the data from the textbook website and the pyth folder wasn’t in there. It’s on the website, just not in the zip folder. Anyways.\n\n\nCode\npyth &lt;- read.table(\n    here::here(\"arm\", \"data\", \"pyth\", \"exercise2.1.dat\"),\n    header = TRUE\n)\n\n# Get only data with observed outcomes\npyth_obs &lt;- pyth[1:40, ]\n\n# Get only data with unobserved outcomes\npyth_unobs &lt;- pyth[41:60, ]\n\n\n\n\nUse R to fit a linear regression model predicting \\(y\\) from \\(x_1, x_2,\\) using the first \\(40\\) data points in the file. Summarize the inferences and check the fit of your model.\n\n\n\n\nCode\nfit_q1a &lt;- lm(y ~ x1 + x2, data = pyth_obs)\narm::display(fit_q1a)\n\n\nlm(formula = y ~ x1 + x2, data = pyth_obs)\n            coef.est coef.se\n(Intercept) 1.32     0.39   \nx1          0.51     0.05   \nx2          0.81     0.02   \n---\nn = 40, k = 3\nresidual sd = 0.90, R-Squared = 0.97\n\n\nFrom the model output, it looks like all of the coefficients should be statistically significant (at a \\(0.05\\) significance level). The \\(R^2\\) indicates that the model fits quite well. The mean of the observed \\(y\\) values is \\(\\bar{y} = 13.59\\), which is quite large compared to the residual SD of \\(0.9\\), so the model fits the data quite well.\n\n\nDisplay the estimated model graphically as in Figure 3.2.\n\n\n\n\nCode\nset.seed(370)\nbeta_hat &lt;- coef(fit_q1a)\nbeta_sim &lt;- arm::sim(fit_q1a)@coef\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    pyth_obs$x1, pyth_obs$y,\n    xlab = \"x1\", ylab = \"y\"\n)\nfor (i in 1:100) {\n    curve(\n        cbind(1, x, mean(pyth_obs$x2)) %*% beta_sim[i, ],\n        lwd = 0.5, col = adjustcolor(\"gray\", alpha.f = 0.5),\n        add = TRUE, n = 1000L\n    )\n}\ncurve(\n    cbind(1, x, mean(pyth_obs$x2)) %*% beta_hat,\n    lwd = 1, col = \"black\",\n    add = TRUE, n = 1000L\n)\n\nplot(\n    pyth_obs$x2, pyth_obs$y,\n    xlab = \"x2\", ylab = \"y\"\n)\nfor (i in 1:100) {\n    curve(\n        cbind(1, mean(pyth_obs$x1), x) %*% beta_sim[i, ],\n        lwd = 0.5, col = adjustcolor(\"gray\", alpha.f = 0.5),\n        add = TRUE, n = 1000L\n    )\n}\ncurve(\n    cbind(1, mean(pyth_obs$x1), x) %*% beta_hat,\n    lwd = 1, col = \"black\",\n    add = TRUE, n = 1000L\n)\n\n\n\n\n\n\n\n\n\nCode\nlayout(1)\n\n\n\n\nMake a residual plot for this model. Do the assumptions appear to be met?\n\n\n\n\nCode\nplot(\n    fitted.values(fit_q1a), residuals(fit_q1a),\n    xlab = \"Fitted values\", ylab = \"Residuals\",\n    ylim = c(-3, 3)\n)\nabline(h = 0, lty = 2)\nabline(h = arm::sigma.hat(fit_q1a), col = \"darkgray\", lty = 3)\nabline(h = -arm::sigma.hat(fit_q1a), col = \"darkgray\", lty = 3)\n\n\n\n\n\n\n\n\n\nThe residuals are clearly not evenly distributed above and below 0, as they should be. It appears that underpredictions are typically less bad than overpredictions are, which indicates that perhaps either additivity, linearity, or equal variance are violated.\n\n\nMake predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?\n\n\n\n\nCode\nlayout(matrix(c(1, 2), nrow = 1))\npyth_pred &lt;- predict(fit_q1a, newdata = pyth_unobs, interval = \"prediction\")\n# X1 plot\nplot(\n    NULL, NULL,\n    xlim = c(0, 10), ylim = c(3, 22),\n    xlab = \"x1\", ylab = \"y\"\n)\npoints(\n    pyth_obs$x1, pyth_obs$y,\n    pch = 1, col = \"black\"\n)\nsegments(\n    pyth_unobs$x1,\n    pyth_pred[, 2],\n    pyth_unobs$x1,\n    pyth_pred[, 3],\n    col = \"firebrick4\"\n)\npoints(\n    pyth_unobs$x1, pyth_pred[, 1],\n    pch = 21, col = \"firebrick4\", bg = \"white\"\n)\nfor (i in 1:100) {\n    curve(\n        cbind(1, x, mean(pyth_obs$x2)) %*% beta_sim[i, ],\n        lwd = 0.5, col = adjustcolor(\"gray\", alpha.f = 0.5),\n        add = TRUE, n = 1000L\n    )\n}\ncurve(\n    cbind(1, x, mean(pyth_obs$x2)) %*% beta_hat,\n    lwd = 1, col = \"black\",\n    add = TRUE, n = 1000L\n)\n\n# x2 plot\nplot(\n    NULL, NULL,\n    xlim = c(0, 20), ylim = c(3, 22),\n    xlab = \"x2\", ylab = \"y\"\n)\npoints(\n    pyth_obs$x2, pyth_obs$y,\n    pch = 1, col = \"black\"\n)\nsegments(\n    pyth_unobs$x2,\n    pyth_pred[, 2],\n    pyth_unobs$x2,\n    pyth_pred[, 3],\n    col = \"firebrick4\"\n)\npoints(\n    pyth_unobs$x2, pyth_pred[, 1],\n    pch = 21, col = \"firebrick4\", bg = \"white\"\n)\nfor (i in 1:100) {\n    curve(\n        cbind(1, mean(pyth_obs$x1), x) %*% beta_sim[i, ],\n        lwd = 0.5, col = adjustcolor(\"gray\", alpha.f = 0.5),\n        add = TRUE, n = 1000L\n    )\n}\ncurve(\n    cbind(1, mean(pyth_obs$x1), x) %*% beta_hat,\n    lwd = 1, col = \"black\",\n    add = TRUE, n = 1000L\n)\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see that the new predictions are fairly similar to the original data, and none of them look crazy. So given the good diagnostics, these predictions are probably OK – as long as we assume that they came from the same data-generating process as the observed outcomes. Fixing the issue that causes the somewhat strange residuals might give us some more peace of mind though."
  },
  {
    "objectID": "arm/cp3/index.html#q2",
    "href": "arm/cp3/index.html#q2",
    "title": "Chapter 3: Linear regression: the basics",
    "section": "Q2",
    "text": "Q2\n\n\n\n\n\n\nSuppose that, for a certain population, we can predict log earnings from log height as follows:\n\nA person who is \\(66\\) inches tall is predicted to have earnings of \\(\\$30,000\\).\nEvery increase of \\(1\\%\\) in height corresponds to a predicted increase of \\(0.8\\%\\) in earnings.\nThe earnings of approximately \\(95\\%\\) of people fall within a factor of 1.1 of predicted values.\n\n\n\n\n\n\nGive the equation of the regression line and the residual standard deviation of the regression.\n\n\nFrom the second bullet point, we can get that\n\\[\\hat{\\beta} = \\frac{0.8\\%}{0.1\\%} = 8.\\]\nThen, using the information from the first bullet point we get that\n\\[\n\\$30,000 = \\hat{\\alpha} + 8 \\times 66 \\implies \\hat{\\alpha} = \\$30,000 - 8 \\times 66 = \\$29,472.\n\\]"
  }
]