{
  "hash": "a5174a2f2b4eaf653bb61e45fc33a9f9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 15: From Parameter Estimation to Hypothesis Testing\"\ndraft: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(\"scipen\" = 9999, \"digits\" = 4)\n```\n:::\n\n\n\n## Q1\n\n::: {.callout-note appearance=\"simple\"}\n\nSuppose a director of marketing with many years of experience tells you he\nbelieves very strongly that the variant without images (B) won't perform\nany differently than the original variant. How could you account for this in\nour model? Implement this change and see how your final conclusions change\nas well.\n\n:::\n\nWe can account for the director of marketing's experience by adjusting our\npriors. To make our prior belief that the conversion rates are the same more\nstringent, we could use, say, a $\\text{Beta}(300, 700)$ prior.\n\nThat means the posterior distribution for variant A is $\\text{Beta}(336, 814)$\nand the posterior distribution for variant B is $\\text{Beta}(350, 800)$. We\ncan repeat the Monte Carlo simulation for these priors and draw $100,000$\nsamples for each variant.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(370)\nn_sims <- 1e5\na_sims <- rbeta(n_sims, 336, 814)\nb_sims <- rbeta(n_sims, 350, 800)\n\na_density <- density(a_sims, kernel = \"epanechnikov\")\nb_density <- density(b_sims, kernel = \"epanechnikov\")\n\nplot(\n\tNULL, NULL,\n\txlim = c(0.1, 0.5),\n\tylim = c(0, 30),\n\txlab = \"Conversion rate\",\n\tylab = \"Density\",\n\tmain = \"Simulated conversion rate distributions for variants A and B\",\n\taxes = FALSE\n)\nlines(a_density, lwd = 3, col = \"dodgerblue2\")\nlines(b_density, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0.1, 0.5, 0.1))\naxis(2, seq(0, 30, 10))\nlegend(\n\t0.1, 30, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n\tlty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/q1 sim-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that the posterior distributions for variants A and B are much\ncloser together. In total, $73.81\\%$ of the\nsimulated conversation rates for variant B were greater than the simulated\nconversation rate for variant A. The empirical CDF for relative improvements\nis shown below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nimp <- b_sims / a_sims\nimp_ecdf <- ecdf(imp)\nxs <- seq(min(imp), max(imp), length.out = 1e3)\nys <- imp_ecdf(xs)\nplot(\n\tNULL, NULL,\n\tmain = \"Emperical cdf of simulated relative improvements of B over A\",\n\txlab = \"Relative improvement of B over A\",\n\tylab = \"Empirical cumulative probability\",\n\taxes = FALSE,\n\txlim = c(0.68, 1.42),\n\tylim = c(-0.02, 1.02),\n\txaxs = \"i\", yaxs = \"i\"\n)\nabline(h = 0.25, lty = 2, col = \"gray\")\nabline(h = 0.50, lty = 2, col = \"gray\")\nabline(h = 0.75, lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.25))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.50))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.75))], lty = 2, col = \"gray\")\nlines(\n\txs, ys,\n\tlwd = 3, col = \"firebrick4\", type = \"s\",\n)\naxis(1, seq(0.7, 1.4, 0.1))\naxis(2, seq(0, 1, 0.25))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/q1 ecdf-1.png){width=672}\n:::\n:::\n\n\n\nUnder the new more stringent prior, we can see that nearly a quarter of all\nsamples showed a higher conversion rate for variant A than variant B, and\nvery few samples showed a relative improvement of variant B more than 1.2.\nSo while our simulation suggests that B is most likely better than A, we are\nless confident in that assertion, and the magnitude of improvement is\nestimated to be smaller on average.\n\n## Q2\n\n::: {.callout-note appearance=\"simple\"}\n\nThe lead designer sees your results and insists there's no way that variant B\nshould perform better with no images. She feels that you should assume the\nconversion rate for variant B is closer to $20\\%$ than $30\\%$. Implement a\nsolution for this and again review the results of our analysis.\n\n:::\n\nTo accomodate the lead designer's prior beliefs, we'll return to our original\nprior of $\\text{Beta}(3, 7)$. However, we'll only use this prior for variant\nA. For variant B, we want a prior that has a mean conversion rate closer to\n$20\\%$, so we'll use $\\text{Beta}(2, 8)$. Let's take a look at the priors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxs <- seq(1e-4, 1 - 1e-4, 1e-4)\na_prior_q2 <- dbeta(xs, 3, 7)\nb_prior_q2 <- dbeta(xs, 2, 8)\n\nplot(\n\tNULL, NULL,\n\txlim = c(0, 1),\n\tylim = c(0, 3.6),\n\txlab = \"Conversion rate\",\n\tylab = \"Density\",\n\tmain = \"Prior conversion rate distributions for variants A and B\",\n\taxes = FALSE\n)\nlines(xs, a_prior_q2, lwd = 3, col = \"dodgerblue2\")\nlines(xs, b_prior_q2, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 3.6, 0.5))\nlegend(\n\t0.6, 3.2, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n\tlty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/q2 priors-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that our priors now assume the average conversion rate for variant\nB is around $20\\%$ as desired, although both priors are not too strict. So\nnow our posterior distributions given the observed data are\n$\\text{Beta}(39, 121)$ for variant A and $\\text{Beta}(52, 108)$ for variant B.\nLet's take a look at the posterior distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxs <- seq(1e-4, 1 - 1e-4, 1e-4)\na_post_q2 <- dbeta(xs, 39, 121)\nb_post_q2 <- dbeta(xs, 52, 108)\n\nplot(\n\tNULL, NULL,\n\txlim = c(0, 1),\n\tylim = c(0, 12),\n\txlab = \"Conversion rate\",\n\tylab = \"Density\",\n\tmain = \"Posterior conversion rate distributions for variants A and B\",\n\taxes = FALSE\n)\nlines(xs, a_post_q2, lwd = 3, col = \"dodgerblue2\")\nlines(xs, b_post_q2, lwd = 3, col = \"springgreen3\")\naxis(1, seq(0, 1, 0.25))\naxis(2, seq(0, 12, 2))\nlegend(\n\t0.6, 10, c(\"A\", \"B\"), col = c(\"dodgerblue2\", \"springgreen3\"),\n\tlty = 1, lwd = 3, box.lwd = 0, cex = 1.5\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/q2 posteriors-1.png){width=672}\n:::\n:::\n\n\n\nOur posterior distributions again seem to show that the conversion rate for\nB is, on average, higher than the conversion rate for A. However, the spread\nof the two posterior distributions is now different and we are less certain in\nthe magnitude of the conversion rate for B than we are for B. Let's repeat\nthe Monte Carlo simulation with these new priors and look at the ECDF of\nimprovements.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\na_sims_q2 <- rbeta(n_sims, 39, 121)\nb_sims_q2 <- rbeta(n_sims, 52, 108)\nimp_q2 <- b_sims_q2 / a_sims_q2\nimp_ecdf_q2 <- ecdf(imp_q2)\nxs <- seq(min(imp_q2), max(imp_q2), length.out = 1e3)\nys <- imp_ecdf(xs)\nplot(\n\tNULL, NULL,\n\tmain = \"Emperical cdf of simulated relative improvements of B over A\",\n\txlab = \"Relative improvement of B over A\",\n\tylab = \"Empirical cumulative probability\",\n\taxes = FALSE,\n\txlim = c(0.48, 3),\n\tylim = c(-0.02, 1.02),\n\txaxs = \"i\", yaxs = \"i\"\n)\nabline(h = 0.25, lty = 2, col = \"gray\")\nabline(h = 0.50, lty = 2, col = \"gray\")\nabline(h = 0.75, lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.25))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.50))], lty = 2, col = \"gray\")\nabline(v = xs[which.min(abs(ys - 0.75))], lty = 2, col = \"gray\")\nlines(\n\txs, ys,\n\tlwd = 3, col = \"firebrick4\", type = \"s\",\n)\naxis(1, seq(0.5, 3, 0.5))\naxis(2, seq(0, 1, 0.25))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/q2 ecdf-1.png){width=672}\n:::\n:::\n\n\n\nUnder these priors, $94.78\\%$ of\nsampled conversation rates for variant B were higher than the sampled conversion\nrates for variant A. So despite the lowered probability for variant B, we are\nstill very confident that B outperforms A, although we are slightly less\nconfident than the example in the book using the same priors for B and A.\n\n## Q3\n\n::: {.callout-note appearance=\"simple\"}\n\nAssume that being 95% certain means that you're more or less \"convinced\" of\na hypothesis. Also assume that there's no longer any limit to the number of\nemails you can send in your test. If the true conversion rate for A is 0.25\nand the true conversion rate for B is 0.3, explore how many samples it would\ntake to convince the director of marketing that B was in fact superior.\nExplore the same for the lead designer.\n\n:::\n\nI think this question, like many others in this book, is quite poorly worded.\nBut I'll assume that what we actually want to know is how many samples we would\nneed to convince either person that the $P(B > A) = 0.95$.\n\nSo, for a number of different sample sizes, we'll simulate conversions. Then,\nfrom those simulated conversion trials (pretend they are real data), we'll\nrun our original Monte Carlo simulation using the updated prior based on the\nnumber of conversion events we observed for variants A and B. We'll repeat\nthis entire scheme 100 times for both the director and the lead designer in\norder to make sure our results aren't due to variance. Then we can plot the\nresults.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# This function runs a single round of our simulation procedure. That is,\n# we generate n_samples number of trials, where a trial is defined as one\n# variant A email being sent and one variant B email being sent. We simulate\n# from the true conversion rate whether those are conversions, and then create\n# the posterior and repeat our MC simulation as before.\nsimulate_probability <- function (\n\t\tn_samples, true_a, true_b,\n\t\tprior_alpha_a, prior_alpha_b,\n\t\tprior_beta_a, prior_beta_b\n\t) {\n\t\t# Generate the number of conversions in this simulation\n\t\tevents_a <- runif(n_samples) <= true_a\n\t\tevents_b <- runif(n_samples) <= true_b\n\t\t\n\t\t# Calculate the parameters for the posterior distributions\n\t\t\tpost_alpha_a <- prior_alpha_a + sum(events_a)\n\t\tpost_beta_a <- prior_beta_a + sum(!events_a)\n\t\tpost_alpha_b <- prior_alpha_b + sum(events_b)\n\t\tpost_beta_b <- prior_beta_b + sum(!events_b)\n\t\t\n\t\t# Now run the Monte Carlo simulation from the implied posterior\n\t\tsamples_a <- rbeta(1e5, post_alpha_a, post_beta_a)\n\t\tsamples_b <- rbeta(1e5, post_alpha_b, post_beta_b)\n\t\t\n\t\t# Get the probability b > a\n\t\tprob_bga <- mean(samples_b > samples_a)\n\t\treturn(prob_bga)\n\t}\n\n# This function runs the simulate_probability() function multiple times, each\n# time using a different number of samples defined by the samples_seq argument.\n# The result is given as a matrix where each column is the output of one\n# run of simulate_probability().\nrun_sample_seq <- function(\n\t\tsamples_seq = seq(25, 1000, 25),\n\t\t...\n\t) {\n\tsapply(\n\t\tsamples_seq,\n\t\t\\(n) simulate_probability(n, ...)\n\t)\n}\n\n# This function will take in our results and return a data frame.\nsummarize_replicate_results <- function(res, alpha = 0.1) {\n\tres_mean <- rowMeans(res)\n\tres_PI <- apply(res, 1, quantile, probs = c(alpha / 2, 1 - alpha / 2))\n\tout <- data.frame(\n\t\test = res_mean,\n\t\tlwr = res_PI[1, ],\n\t\tupr = res_PI[2, ]\n\t)\n\treturn(out)\n}\n\nsamples_vector <- seq(25, 1000, 25)\n\n# Run the simulation for the director\nsims_director <- replicate(\n\t100,\n\trun_sample_seq(\n\t\tsamples_seq = samples_vector,\n\t\ttrue_a = 0.25,\n\t\ttrue_b = 0.35,\n\t\tprior_alpha_a = 300,\n\t\tprior_alpha_b = 300,\n\t\tprior_beta_a = 700,\n\t\tprior_beta_b = 700\n\t)\n)\nres_director <- cbind(\n\tn = samples_vector,\n\tperson = 1,\n\tsummarize_replicate_results(sims_director)\n)\n\n# And for the designer\nsims_designer <- replicate(\n\t100,\n\trun_sample_seq(\n\t\tsamples_seq = samples_vector,\n\t\ttrue_a = 0.25,\n\t\ttrue_b = 0.35,\n\t\tprior_alpha_a = 3,\n\t\tprior_alpha_b = 7,\n\t\tprior_beta_a = 2,\n\t\tprior_beta_b = 8\n\t)\n)\nres_designer <- cbind(\n\tn = samples_vector,\n\tperson = 2,\n\tsummarize_replicate_results(sims_designer)\n)\n\n# Data cleaning\nres_combined <- rbind(res_director, res_designer)\nres_combined$person <- factor(\n\tres_combined$person,\n\tlevels = c(1, 2),\n\tlabels = c(\"Director of Marketing\", \"Lead Designer\")\n)\n\n# Get the intercepts for the number of samples closest to 95% probability\ndirector_n <- res_director$n[which.min(abs(res_director$est - 0.95))]\ndesigner_n <- res_director$n[which.min(abs(res_designer$est - 0.95))]\n\n# Plot the results\nlibrary(ggplot2)\nres_combined |>\n\tggplot() +\n\taes(\n\t\tx = n, y = est, ymin = lwr, ymax = upr, group = person,\n\t\tcolor = person, fill = person, shape = person\n\t) +\n\tgeom_ribbon(alpha = 0.3, color = \"transparent\") +\n\tgeom_hline(yintercept = 0.95, linetype = 2, linewidth = 1) +\n\tgeom_vline(\n\t\txintercept = c(director_n, designer_n),\n\t\tlinetype = 2, linewidth = 1\n\t) +\n\tgeom_line(linewidth = 1.5, alpha = 0.8) +\n\tgeom_point(size = 3, stroke = 2, fill = \"white\", alpha = 0.8) +\n\tscale_color_manual(values = c(\"dodgerblue2\", \"firebrick3\")) +\n\tscale_fill_manual(values = c(\"dodgerblue2\", \"firebrick3\")) +\n\tscale_shape_manual(values = c(21, 22)) +\n\thgp::theme_ms() +\n\tlabs(\n\t\tx = \"Number of samples\",\n\t\ty = \"Probability of B > A\",\n\t\tcolor = NULL,\n\t\tfill = NULL,\n\t\tshape = NULL\n\t)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Q3-1.png){width=672}\n:::\n:::\n\n\n\nFrom the plot, we can see that we would need about 250 samples to convince the\nlead designer, and around 475 samples to convince the director of marketing.\nHowever, if we wanted to make sure we take the sampling variability into\naccount, (instead assuming that our lower bound for the probability over all of\nour sampling replications is 95% or higher), we would actually need around 475\nsamples to convince the lead designer and 750 samples to convince the director\nof marketing.\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}